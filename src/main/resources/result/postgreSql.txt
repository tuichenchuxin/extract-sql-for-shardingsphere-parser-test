
--
-- CREATE_OPERATOR
--

CREATE OPERATOR ## (
   leftarg = path,
   rightarg = path,
   function = path_inter,
   commutator = ##
)
================================


CREATE OPERATOR <% (
   leftarg = point,
   rightarg = widget,
   procedure = pt_in_widget,
   commutator = >% ,
   negator = >=%
)
================================


CREATE OPERATOR @#@ (
   rightarg = int8,		-- prefix
   procedure = factorial
)
================================


CREATE OPERATOR #%# (
   leftarg = int8,		-- fail, postfix is no longer supported
   procedure = factorial
)
================================


-- Test comments
COMMENT ON OPERATOR ###### (NONE, int4) IS 'bad prefix'
================================

COMMENT ON OPERATOR ###### (int4, NONE) IS 'bad postfix'
================================

COMMENT ON OPERATOR ###### (int4, int8) IS 'bad infix'
================================


-- Check that DROP on a nonexistent op behaves sanely, too
DROP OPERATOR ###### (NONE, int4)
================================

DROP OPERATOR ###### (int4, NONE)
================================

DROP OPERATOR ###### (int4, int8)
================================


-- => is disallowed as an operator name now
CREATE OPERATOR => (
   rightarg = int8,
   procedure = factorial
)
================================


-- lexing of <=, >=, <>, != has a number of edge cases
-- (=> is tested elsewhere)

-- this is legal because ! is not allowed in sql ops
CREATE OPERATOR !=- (
   rightarg = int8,
   procedure = factorial
)
================================

-- postfix operators don't work anymore
SELECT 10 !=-
================================

DO $$ -- use DO to protect -- from psql
  declare r boolean
================================

    raise info 'r = %', r
================================

$$
================================

CREATE SCHEMA schema_op1
================================

CREATE OPERATOR schema_op1.#*# (
   rightarg = int8,
   procedure = factorial
)
================================

CREATE OPERATOR #*# (
   leftarg = SETOF int8,
   procedure = factorial
)
================================

CREATE OPERATOR #*# (
   rightarg = SETOF int8,
   procedure = factorial
)
================================

$$ LANGUAGE sql IMMUTABLE
================================

CREATE OPERATOR === (
    LEFTARG = boolean,
    RIGHTARG = boolean,
    PROCEDURE = fn_op2,
    COMMUTATOR = ===,
    NEGATOR = !==,
    RESTRICT = contsel,
    JOIN = contjoinsel,
    SORT1, SORT2, LTCMP, GTCMP, HASHES, MERGES
)
================================


-- Should fail. Invalid attribute
CREATE OPERATOR #@%# (
   rightarg = int8,
   procedure = factorial,
   invalid_att = int8
)
================================


-- Should fail. At least rightarg should be mandatorily specified
CREATE OPERATOR #@%# (
   procedure = factorial
)
================================


-- Should fail. Procedure should be mandatorily specified
CREATE OPERATOR #@%# (
   rightarg = int8
)
================================

CREATE TYPE type_op3 AS ENUM ('new', 'open', 'closed')
================================

$$ LANGUAGE sql IMMUTABLE
================================

CREATE OPERATOR #*# (
   leftarg = type_op3,
   rightarg = int8,
   procedure = fn_op3
)
================================

CREATE TYPE type_op4 AS ENUM ('new', 'open', 'closed')
================================

$$ LANGUAGE sql IMMUTABLE
================================

CREATE OPERATOR #*# (
   leftarg = int8,
   rightarg = type_op4,
   procedure = fn_op4
)
================================

CREATE TYPE type_op5 AS ENUM ('new', 'open', 'closed')
================================

$$ LANGUAGE sql IMMUTABLE
================================

CREATE OPERATOR #*# (
   leftarg = int8,
   rightarg = int8,
   procedure = fn_op5
)
================================

CREATE TYPE type_op6 AS ENUM ('new', 'open', 'closed')
================================

$$ LANGUAGE sql IMMUTABLE
================================

CREATE OPERATOR #*# (
   leftarg = int8,
   rightarg = int8,
   procedure = fn_op6
)
================================


-- invalid: non-lowercase quoted identifiers
CREATE OPERATOR ===
(
	"Leftarg" = box,
	"Rightarg" = box,
	"Procedure" = area_equal_function,
	"Commutator" = ===,
	"Negator" = !==,
	"Restrict" = area_restriction_function,
	"Join" = area_join_function,
	"Hashes",
	"Merges"
)
================================


================================
--
-- ROWTYPES
--

-- Make both a standalone composite type and a table rowtype

create type complex as (r float8, i float8)
================================


-- Nested composite

create type quad as (c1 complex, c2 complex)
================================


-- test insertion/updating of subfields
update people set fn.suffix = 'Jr'
================================


update quadtable set q.c1.r = 12 where f1 = 2
================================


update quadtable set q.c1 = 12
================================


insert into people select ('Jim', f1, null)::fullname, current_date from pp
================================


-- Test row comparison semantics.  Prior to PG 8.2 we did this in a totally
-- non-spec-compliant way.

select ROW(1,2) < ROW(1,3) as true
================================

select ROW(1,2) < ROW(1,1) as false
================================

select ROW(1,2) < ROW(1,NULL) as null
================================

select ROW(1,2,3) < ROW(1,3,NULL) as true
================================
 -- the NULL is not examined
select ROW(11,'ABC') < ROW(11,'DEF') as true
================================

select ROW(11,'ABC') > ROW(11,'DEF') as false
================================

select ROW(12,'ABC') > ROW(11,'DEF') as true
================================


-- = and <> have different NULL-behavior than < etc
select ROW(1,2,3) < ROW(1,NULL,4) as null
================================

select ROW(1,2,3) = ROW(1,NULL,4) as false
================================

select ROW(1,2,3) <> ROW(1,NULL,4) as true
================================


-- We allow operators beyond the six standard ones, if they have btree
-- operator classes.
select ROW('ABC','DEF') ~<=~ ROW('DEF','ABC') as true
================================

select ROW('ABC','DEF') ~>=~ ROW('DEF','ABC') as false
================================

insert into test_table select 'a', null from generate_series(1,1000)
================================
  -- fail
select (row(1, 2.0)).*
================================
  -- fail
select (r).* from (select row(1, 2.0) as r) ss
================================


-- Check behavior with a non-comparable rowtype
create type cantcompare as (p point, r float8)
================================
 -- fail, but should complain about cantcompare

--
-- Tests for record_{eq,cmp}
--

create type testtype1 as (a int, b int)
================================


-- mismatches
create type testtype3 as (a int, b text)
================================

create type testtype5 as (a int)
================================


-- non-comparable types
create type testtype6 as (a int, b point)
================================


drop type testtype1, testtype3, testtype5, testtype6
================================


--
-- Tests for record_image_{eq,cmp}
--

create type testtype1 as (a int, b int)
================================


-- all true
select row(1, 2)::testtype1 *< row(1, 3)::testtype1
================================

select row(1, 2)::testtype1 *<= row(1, 3)::testtype1
================================

select row(1, 2)::testtype1 *= row(1, 2)::testtype1
================================

select row(1, 2)::testtype1 *<> row(1, 3)::testtype1
================================

select row(1, 3)::testtype1 *>= row(1, 2)::testtype1
================================

select row(1, 3)::testtype1 *> row(1, 2)::testtype1
================================


-- all false
select row(1, -2)::testtype1 *< row(1, -3)::testtype1
================================

select row(1, -2)::testtype1 *<= row(1, -3)::testtype1
================================

select row(1, -2)::testtype1 *= row(1, -3)::testtype1
================================

select row(1, -2)::testtype1 *<> row(1, -2)::testtype1
================================

select row(1, -3)::testtype1 *>= row(1, -2)::testtype1
================================

select row(1, -3)::testtype1 *> row(1, -2)::testtype1
================================


-- This returns the "wrong" order because record_image_cmp works on
-- unsigned datums without knowing about the actual data type.
select row(1, -2)::testtype1 *< row(1, 3)::testtype1
================================


-- other types
create type testtype2 as (a smallint, b bool)
================================
  -- byval different sizes
select row(1, true)::testtype2 *< row(2, true)::testtype2
================================

select row(-2, true)::testtype2 *< row(-1, true)::testtype2
================================

select row(0, false)::testtype2 *< row(0, true)::testtype2
================================

select row(0, false)::testtype2 *<> row(0, true)::testtype2
================================


create type testtype3 as (a int, b text)
================================
  -- variable length
select row(1, 'abc')::testtype3 *< row(1, 'abd')::testtype3
================================

select row(1, 'abc')::testtype3 *< row(1, 'abcd')::testtype3
================================

select row(1, 'abc')::testtype3 *> row(1, 'abd')::testtype3
================================

select row(1, 'abc')::testtype3 *<> row(1, 'abd')::testtype3
================================


create type testtype4 as (a int, b point)
================================
  -- by ref, fixed length
select row(1, '(1,2)')::testtype4 *< row(1, '(1,3)')::testtype4
================================

select row(1, '(1,2)')::testtype4 *<> row(1, '(1,3)')::testtype4
================================


-- mismatches
select row(1, 2)::testtype1 *< row(1, 'abc')::testtype3
================================

select row(1, 2)::testtype1 *<> row(1, 'abc')::testtype3
================================

create type testtype5 as (a int)
================================

select row(1, 2)::testtype1 *< row(1)::testtype5
================================

select row(1, 2)::testtype1 *<> row(1)::testtype5
================================


-- non-comparable types
create type testtype6 as (a int, b point)
================================

select row(1, '(1,2)')::testtype6 *< row(1, '(1,3)')::testtype6
================================

select row(1, '(1,2)')::testtype6 *>= row(1, '(1,3)')::testtype6
================================

select row(1, '(1,2)')::testtype6 *<> row(1, '(1,3)')::testtype6
================================


drop type testtype1, testtype2, testtype3, testtype4, testtype5, testtype6
================================


CREATE TYPE price_input AS (
    id INTEGER,
    price NUMERIC
)
================================


CREATE TYPE price_key AS (
    id INTEGER
)
================================


UPDATE price
    SET active = true, price = input_prices.price
    FROM unnest(ARRAY[(10, 123.00), (11, 99.99)]::price_input[]) input_prices
    WHERE price_key_from_table(price.*) = price_key_from_input(input_prices.*)
================================
  -- fail
$$ language sql
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================


create temp table tt1 as select * from int8_tbl limit 2
================================


--
-- IS [NOT] NULL should not recurse into nested composites (bug #14235)
--

explain (verbose, costs off)
select r, r is null as isnull, r is not null as isnotnull
from (values (1,row(1,2)), (1,row(null,null)), (1,null),
             (null,row(1,2)), (null,row(null,null)), (null,null) ) r(a,b)
================================


select r, r is null as isnull, r is not null as isnotnull
from (values (1,row(1,2)), (1,row(null,null)), (1,null),
             (null,row(1,2)), (null,row(null,null)), (null,null) ) r(a,b)
================================


explain (verbose, costs off)
with r(a,b) as materialized
  (values (1,row(1,2)), (1,row(null,null)), (1,null),
          (null,row(1,2)), (null,row(null,null)), (null,null) )
select r, r is null as isnull, r is not null as isnotnull from r
================================


with r(a,b) as materialized
  (values (1,row(1,2)), (1,row(null,null)), (1,null),
          (null,row(1,2)), (null,row(null,null)), (null,null) )
select r, r is null as isnull, r is not null as isnotnull from r
================================


================================

\! cat /tmp/copy_test

-- Access sequence directly
select is_called from hsseq
================================


discard temp
================================

discard all
================================


DECLARE hsc CURSOR FOR select * from hs3
================================


FETCH next from hsc
================================

fetch first from hsc
================================

fetch last from hsc
================================

fetch 1 from hsc
================================


CLOSE hsc
================================


EXECUTE hsp
================================

LOCK hs1 IN ACCESS SHARE MODE
================================

LOCK hs1 IN ROW SHARE MODE
================================

LOCK hs1 IN ROW EXCLUSIVE MODE
================================


-- UNLISTEN
UNLISTEN a
================================

UNLISTEN *
================================


-- LOAD
-- should work, easier if there is no test for that...


-- ALLOWED COMMANDS

CHECKPOINT
================================


discard all
================================


================================
--
-- CREATE_MISC
--

-- CLASS POPULATION
--	(any resemblance to real life is purely coincidental)
--

INSERT INTO tenk2 SELECT * FROM tenk1
================================


CREATE TABLE onek2 AS SELECT * FROM onek
================================


INSERT INTO fast_emp4000 SELECT * FROM slow_emp4000
================================


INSERT INTO hobbies_r (name, person)
   SELECT 'posthacking', p.name
   FROM person* p
   WHERE p.name = 'mike' or p.name = 'jeff'
================================


INSERT INTO hobbies_r (name, person)
   SELECT 'basketball', p.name
   FROM person p
   WHERE p.name = 'joe' or p.name = 'sally'
================================


INSERT INTO ihighway
   SELECT *
   FROM road
   WHERE name ~ 'I- .*'
================================


INSERT INTO shighway
   SELECT *
   FROM road
   WHERE name ~ 'State Hwy.*'
================================


INSERT INTO f_star (class, a, c, e, f)
   VALUES ('f', 19, 'hi claire'::name, '-5'::int2, '(1,3),(2,4)'::polygon)
================================


INSERT INTO f_star (class, a, c, f)
   VALUES ('f', 21, 'hi marcel'::name, '(11,44),(22,55),(33,66)'::polygon)
================================


INSERT INTO f_star (class, a, e, f)
   VALUES ('f', 22, '-7'::int2, '(111,555),(222,666),(333,777),(444,888)'::polygon)
================================


INSERT INTO f_star (class, c, e, f)
   VALUES ('f', 'hi keith'::name, '-8'::int2,
	   '(1111,3333),(2222,4444)'::polygon)
================================


INSERT INTO f_star (class, a, f)
   VALUES ('f', 26, '(11111,33333),(22222,44444)'::polygon)
================================


INSERT INTO f_star (class, c, f)
   VALUES ('f', 'hi jeff'::name,
           '(111111,333333),(222222,444444)'::polygon)
================================


INSERT INTO f_star (class, e, f)
   VALUES ('f', '-11'::int2, '(1111111,3333333),(2222222,4444444)'::polygon)
================================


INSERT INTO f_star (class, f)
   VALUES ('f', '(11111111,33333333),(22222222,44444444)'::polygon)
================================


INSERT INTO iportaltest (i, d, p)
   VALUES (1, 3.567, '(3.0,1.0),(4.0,2.0)'::polygon)
================================


INSERT INTO iportaltest (i, d, p)
   VALUES (2, 89.05, '(4.0,2.0),(3.0,1.0)'::polygon)
================================


================================


-- a few dummy ops to push up the CommandId counter
INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================


DECLARE c CURSOR FOR SELECT ctid,cmin,* FROM combocidtest
================================


FETCH ALL FROM c
================================


-- a few dummy ops to push up the CommandId counter
INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================

INSERT INTO combocidtest SELECT 1 LIMIT 0
================================


================================
--
-- macaddr8
--

-- test various cases of valid and invalid input
-- valid
SELECT '08:00:2b:01:02:03     '::macaddr8
================================

SELECT '    08:00:2b:01:02:03     '::macaddr8
================================

SELECT '    08:00:2b:01:02:03'::macaddr8
================================

SELECT '08:00:2b:01:02:03:04:05     '::macaddr8
================================

SELECT '    08:00:2b:01:02:03:04:05     '::macaddr8
================================

SELECT '    08:00:2b:01:02:03:04:05'::macaddr8
================================


SELECT '123    08:00:2b:01:02:03'::macaddr8
================================
 -- invalid
SELECT '08:00:2b:01:02:03  123'::macaddr8
================================
 -- invalid
SELECT '123    08:00:2b:01:02:03:04:05'::macaddr8
================================
 -- invalid
SELECT '08:00:2b:01:02:03:04:05  123'::macaddr8
================================
 -- invalid
SELECT '08:00:2b:01:02:03:04:05:06:07'::macaddr8
================================
 -- invalid
SELECT '08-00-2b-01-02-03-04-05-06-07'::macaddr8
================================
 -- invalid
SELECT '08002b:01020304050607'::macaddr8
================================
 -- invalid
SELECT '08002b01020304050607'::macaddr8
================================
 -- invalid
SELECT '0z002b0102030405'::macaddr8
================================
 -- invalid
SELECT '08002b010203xyza'::macaddr8
================================
 -- invalid

SELECT '08:00-2b:01:02:03:04:05'::macaddr8
================================
 -- invalid
SELECT '08:00-2b:01:02:03:04:05'::macaddr8
================================
 -- invalid
SELECT '08:00:2b:01.02:03:04:05'::macaddr8
================================
 -- invalid
SELECT '08:00:2b:01.02:03:04:05'::macaddr8
================================
 -- invalid

-- test converting a MAC address to modified EUI-64 for inclusion
-- in an ipv6 address
SELECT macaddr8_set7bit('00:08:2b:01:02:03'::macaddr8)
================================
 -- false
SELECT b::macaddr <= '08:00:2b:01:02:04' FROM macaddr8_data WHERE a = 1
================================
 -- true
SELECT b::macaddr >= '08:00:2b:01:02:04' FROM macaddr8_data WHERE a = 1
================================
 -- true
SELECT b::macaddr <> '08:00:2b:01:02:04'::macaddr FROM macaddr8_data WHERE a = 1
================================
 -- true
SELECT b::macaddr <> '08:00:2b:01:02:03'::macaddr FROM macaddr8_data WHERE a = 1
================================


================================


DECLARE foo1 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo2 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo3 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo4 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo5 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo6 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo7 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo8 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo9 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo10 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo11 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo12 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo13 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo14 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo15 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo16 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo17 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo18 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo19 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo20 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo21 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


DECLARE foo22 SCROLL CURSOR FOR SELECT * FROM tenk2
================================


DECLARE foo23 SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


FETCH 1 in foo1
================================


FETCH 2 in foo2
================================


FETCH 3 in foo3
================================


FETCH 4 in foo4
================================


FETCH 5 in foo5
================================


FETCH 6 in foo6
================================


FETCH 7 in foo7
================================


FETCH 8 in foo8
================================


FETCH 9 in foo9
================================


FETCH 10 in foo10
================================


FETCH 11 in foo11
================================


FETCH 12 in foo12
================================


FETCH 13 in foo13
================================


FETCH 14 in foo14
================================


FETCH 15 in foo15
================================


FETCH 16 in foo16
================================


FETCH 17 in foo17
================================


FETCH 18 in foo18
================================


FETCH 19 in foo19
================================


FETCH 20 in foo20
================================


FETCH 21 in foo21
================================


FETCH 22 in foo22
================================


FETCH 23 in foo23
================================


FETCH backward 1 in foo23
================================


FETCH backward 2 in foo22
================================


FETCH backward 3 in foo21
================================


FETCH backward 4 in foo20
================================


FETCH backward 5 in foo19
================================


FETCH backward 6 in foo18
================================


FETCH backward 7 in foo17
================================


FETCH backward 8 in foo16
================================


FETCH backward 9 in foo15
================================


FETCH backward 10 in foo14
================================


FETCH backward 11 in foo13
================================


FETCH backward 12 in foo12
================================


FETCH backward 13 in foo11
================================


FETCH backward 14 in foo10
================================


FETCH backward 15 in foo9
================================


FETCH backward 16 in foo8
================================


FETCH backward 17 in foo7
================================


FETCH backward 18 in foo6
================================


FETCH backward 19 in foo5
================================


FETCH backward 20 in foo4
================================


FETCH backward 21 in foo3
================================


FETCH backward 22 in foo2
================================


FETCH backward 23 in foo1
================================


CLOSE foo1
================================


CLOSE foo2
================================


CLOSE foo3
================================


CLOSE foo4
================================


CLOSE foo5
================================


CLOSE foo6
================================


CLOSE foo7
================================


CLOSE foo8
================================


CLOSE foo9
================================


CLOSE foo10
================================


CLOSE foo11
================================


CLOSE foo12
================================


DECLARE foo24 NO SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


FETCH 1 FROM foo24
================================


FETCH BACKWARD 1 FROM foo24
================================


DECLARE foo24 NO SCROLL CURSOR FOR SELECT * FROM tenk1 ORDER BY unique2
================================


FETCH 1 FROM foo24
================================


FETCH ABSOLUTE 2 FROM foo24
================================
 -- allowed

FETCH ABSOLUTE 1 FROM foo24
================================


DECLARE foo25 SCROLL CURSOR WITH HOLD FOR SELECT * FROM tenk2
================================


FETCH FROM foo25
================================


FETCH FROM foo25
================================


FETCH FROM foo25
================================


FETCH BACKWARD FROM foo25
================================


FETCH ABSOLUTE -1 FROM foo25
================================


CLOSE foo25
================================


DECLARE foo25ns NO SCROLL CURSOR WITH HOLD FOR SELECT * FROM tenk2
================================


FETCH FROM foo25ns
================================


FETCH FROM foo25ns
================================


FETCH FROM foo25ns
================================


FETCH ABSOLUTE 4 FROM foo25ns
================================


FETCH ABSOLUTE 4 FROM foo25ns
================================


CLOSE foo25ns
================================


DECLARE foo26 CURSOR WITH HOLD FOR SELECT * FROM tenk1 ORDER BY unique2
================================


-- should fail
FETCH FROM foo26
================================


CREATE FUNCTION declares_cursor(text)
   RETURNS void
   AS 'DECLARE c CURSOR FOR SELECT stringu1 FROM tenk1 WHERE stringu1 LIKE $1
================================
'
   LANGUAGE SQL
================================


FETCH ALL FROM c
================================


--
-- Test behavior of both volatile and stable functions inside a cursor
================================


declare c1 cursor for select count_tt1_v(), count_tt1_s()
================================


fetch all from c1
================================


declare c2 cursor with hold for select count_tt1_v(), count_tt1_s()
================================


fetch all from c2
================================

DECLARE bc BINARY CURSOR FOR SELECT * FROM tenk1
================================

EXECUTE cprep
================================


-- test CLOSE ALL
================================

CLOSE ALL
================================

DECLARE foo1 CURSOR WITH HOLD FOR SELECT 1
================================

DECLARE foo2 CURSOR WITHOUT HOLD FOR SELECT 1
================================

CLOSE ALL
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest
================================

FETCH 2 FROM c1
================================

DELETE FROM uctest WHERE CURRENT OF c1
================================

-- cursor did not move
FETCH ALL FROM c1
================================

-- cursor is insensitive
MOVE BACKWARD ALL IN c1
================================

FETCH ALL FROM c1
================================


-- Check UPDATE WHERE CURRENT
================================
 this time use FOR UPDATE
BEGIN
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest FOR UPDATE
================================

FETCH c1
================================

UPDATE uctest SET f1 = 8 WHERE CURRENT OF c1
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest
================================

FETCH c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

-- insensitive cursor should not show effects of updates or deletes
FETCH RELATIVE 0 FROM c1
================================

DELETE FROM uctest WHERE CURRENT OF c1
================================

DELETE FROM uctest WHERE CURRENT OF c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

FETCH RELATIVE 0 FROM c1
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest FOR UPDATE
================================

FETCH c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

DELETE FROM uctest WHERE CURRENT OF c1
================================

DELETE FROM uctest WHERE CURRENT OF c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

--- FOR UPDATE cursors can't currently scroll back, so this is an error:
FETCH RELATIVE 0 FROM c1
================================

DECLARE c1 INSENSITIVE CURSOR FOR SELECT * FROM uctest
================================

FETCH NEXT FROM c1
================================

FETCH NEXT FROM c1
================================

FETCH NEXT FROM c1
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest FOR UPDATE
================================

FETCH 1 FROM c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

FETCH 1 FROM c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

FETCH 1 FROM c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

FETCH 1 FROM c1
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest a, uctest b WHERE a.f1 = b.f1 + 5
================================

FETCH 1 FROM c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest a, uctest b WHERE a.f1 = b.f1 + 5 FOR UPDATE
================================

FETCH 1 FROM c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest a, uctest b WHERE a.f1 = b.f1 + 5 FOR SHARE OF a
================================

FETCH 1 FROM c1
================================

UPDATE uctest SET f1 = f1 + 10 WHERE CURRENT OF c1
================================


-- Check various error cases

DELETE FROM uctest WHERE CURRENT OF c1
================================
  -- fail, no such cursor
DECLARE cx CURSOR WITH HOLD FOR SELECT * FROM uctest
================================

DELETE FROM uctest WHERE CURRENT OF cx
================================

DECLARE c CURSOR FOR SELECT * FROM tenk2
================================

DELETE FROM uctest WHERE CURRENT OF c
================================

DECLARE c CURSOR FOR SELECT * FROM tenk2 FOR SHARE
================================

DELETE FROM uctest WHERE CURRENT OF c
================================

DECLARE c CURSOR FOR SELECT * FROM tenk1 JOIN tenk2 USING (unique1)
================================

DELETE FROM tenk1 WHERE CURRENT OF c
================================

DECLARE c CURSOR FOR SELECT f1,count(*) FROM uctest GROUP BY f1
================================

DELETE FROM uctest WHERE CURRENT OF c
================================

DECLARE c1 CURSOR FOR SELECT * FROM uctest
================================

DELETE FROM uctest WHERE CURRENT OF c1
================================

DECLARE c1 CURSOR FOR SELECT MIN(f1) FROM uctest FOR UPDATE
================================

DECLARE c1 CURSOR FOR SELECT * FROM ucview
================================

FETCH FROM c1
================================

DELETE FROM ucview WHERE CURRENT OF c1
================================

DECLARE c1 CURSOR FOR SELECT stringu1 FROM onek WHERE stringu1 = 'DZAAAA'
================================

FETCH FROM c1
================================

DELETE FROM onek WHERE CURRENT OF c1
================================

INSERT INTO current_check_1 SELECT i, 'p' || i FROM generate_series(1,9) i
================================

INSERT INTO current_check_2 SELECT i, 'P' || i FROM generate_series(10,19) i
================================


DECLARE c1 SCROLL CURSOR FOR SELECT * FROM current_check
================================


-- This tests the fetch-backwards code path
FETCH ABSOLUTE 12 FROM c1
================================

FETCH ABSOLUTE 8 FROM c1
================================

DELETE FROM current_check WHERE CURRENT OF c1 RETURNING *
================================


-- This tests the ExecutorRewind code path
FETCH ABSOLUTE 13 FROM c1
================================

FETCH ABSOLUTE 1 FROM c1
================================

DELETE FROM current_check WHERE CURRENT OF c1 RETURNING *
================================

DECLARE c1 NO SCROLL CURSOR FOR SELECT * FROM cursor FOR UPDATE
================================

FETCH ALL FROM c1
================================

declare c cursor for select * from int8_tbl limit nochange(3)
================================

fetch all from c
================================

move backward all in c
================================

fetch all from c
================================

declare c1 scroll cursor for select (select 42) as x
================================

fetch all in c1
================================

fetch backward all in c1
================================

declare c2 scroll cursor for select generate_series(1,3) as g
================================

fetch all in c2
================================

fetch backward all in c2
================================


================================

INSERT INTO inhf DEFAULT VALUES
================================

\d test_like_id_1
INSERT INTO test_like_id_1 (b) VALUES ('b1')
================================

\d test_like_id_2
INSERT INTO test_like_id_2 (b) VALUES ('b2')
================================

\d test_like_id_3
INSERT INTO test_like_id_3 (b) VALUES ('b3')
================================

\d test_like_gen_1
INSERT INTO test_like_gen_1 (a) VALUES (1)
================================

\d test_like_gen_2
INSERT INTO test_like_gen_2 (a) VALUES (1)
================================

CREATE TABLE test_like_gen_3 (LIKE test_like_gen_1 INCLUDING GENERATED)
================================

\d test_like_gen_3
INSERT INTO test_like_gen_3 (a) VALUES (1)
================================

\d test_like_4
CREATE TABLE test_like_4a (LIKE test_like_4)
================================

CREATE TABLE test_like_4c (LIKE test_like_4 INCLUDING GENERATED)
================================

CREATE TABLE test_like_4d (LIKE test_like_4 INCLUDING DEFAULTS INCLUDING GENERATED)
================================

\d test_like_4a
INSERT INTO test_like_4a (a) VALUES(11)
================================

\d test_like_4b
INSERT INTO test_like_4b (a) VALUES(11)
================================

\d test_like_4c
INSERT INTO test_like_4c (a) VALUES(11)
================================

\d test_like_4d
INSERT INTO test_like_4d (a) VALUES(11)
================================

\d test_like_5c

DROP TABLE test_like_4, test_like_4a, test_like_4b, test_like_4c, test_like_4d
================================

\d inhz
DROP TABLE inhz
================================

CREATE STATISTICS ctlt1_a_b_stat ON a,b FROM ctlt1
================================

CREATE STATISTICS ctlt1_expr_stat ON (a || b) FROM ctlt1
================================

COMMENT ON STATISTICS ctlt1_a_b_stat IS 'ab stats'
================================

COMMENT ON STATISTICS ctlt1_expr_stat IS 'ab expr stats'
================================

COMMENT ON COLUMN ctlt1.a IS 'A'
================================

COMMENT ON COLUMN ctlt1.b IS 'B'
================================

COMMENT ON CONSTRAINT ctlt1_a_check ON ctlt1 IS 't1_a_check'
================================

COMMENT ON INDEX ctlt1_pkey IS 'index pkey'
================================

COMMENT ON INDEX ctlt1_b_key IS 'index b_key'
================================

COMMENT ON COLUMN ctlt2.c IS 'C'
================================

COMMENT ON COLUMN ctlt3.a IS 'A3'
================================

COMMENT ON COLUMN ctlt3.c IS 'C'
================================

COMMENT ON CONSTRAINT ctlt3_a_check ON ctlt3 IS 't3_a_check'
================================

\d+ ctlt12_storage
CREATE TABLE ctlt12_comments (LIKE ctlt1 INCLUDING COMMENTS, LIKE ctlt2 INCLUDING COMMENTS)
================================

\d+ ctlt12_comments
CREATE TABLE ctlt1_inh (LIKE ctlt1 INCLUDING CONSTRAINTS INCLUDING COMMENTS) INHERITS (ctlt1)
================================

\d+ ctlt1_inh
SELECT description FROM pg_description, pg_constraint c WHERE classoid = 'pg_constraint'::regclass AND objoid = c.oid AND c.conrelid = 'ctlt1_inh'::regclass
================================

\d+ ctlt13_inh
CREATE TABLE ctlt13_like (LIKE ctlt3 INCLUDING CONSTRAINTS INCLUDING INDEXES INCLUDING COMMENTS INCLUDING STORAGE) INHERITS (ctlt1)
================================

\d+ ctlt13_like
SELECT description FROM pg_description, pg_constraint c WHERE classoid = 'pg_constraint'::regclass AND objoid = c.oid AND c.conrelid = 'ctlt13_like'::regclass
================================

\d+ ctlt_all
SELECT c.relname, objsubid, description FROM pg_description, pg_index i, pg_class c WHERE classoid = 'pg_class'::regclass AND objoid = i.indexrelid AND c.oid = i.indexrelid AND i.indrelid = 'ctlt_all'::regclass ORDER BY c.relname, objsubid
================================

\d+ public.pg_attrdef
DROP TABLE public.pg_attrdef
================================

CREATE SCHEMA ctl_schema
================================

\d+ ctlt1
ROLLBACK
================================

\d noinh_con_copy1

-- fail, as partitioned tables don't allow NO INHERIT constraints
CREATE TABLE noinh_con_copy1_parted (LIKE noinh_con_copy INCLUDING ALL)
  PARTITION BY LIST (a)
================================


CREATE TYPE ctlty1 AS (a int, b text)
================================

DROP TYPE ctlty1
================================


================================


================================
-- Tests for range data types.

create type textrange as range (subtype=text, collation="C")
================================


--
-- test input parser
--

-- negative tests
================================
 should fail
select ''::textrange
================================

INSERT INTO numrange_test VALUES(numrange(1.1, 2.2))
================================

INSERT INTO numrange_test VALUES(numrange(1.7, 1.7, '[]'))
================================


SELECT * FROM numrange_test WHERE range_contains(nr, numrange(1.9,1.91))
================================

SELECT * FROM numrange_test WHERE range_contained_by(numrange(-1e7,-10000.1), nr)
================================

select * from numrange_test where nr < numrange(-1000.0, -1000.0,'[]')
================================

select * from numrange_test where nr < numrange(0.0, 1.0,'[]')
================================

select * from numrange_test where nr < numrange(1000.0, 1001.0,'[]')
================================

select * from numrange_test where nr > numrange(-1001.0, -1000.0,'[]')
================================

select * from numrange_test where nr > numrange(0.0, 1.0,'[]')
================================

select * from numrange_test where nr > numrange(1000.0, 1000.0,'[]')
================================

select range_adjacent(numrange(2.0, 3.0), numrange(3.1, 4.0))
================================

select range_adjacent(numrange(2.0, 3.0), numrange(3.1, null))
================================

select range_adjacent(numrange(2.0, 3.0, '(]'), numrange(1.0, 2.0, '(]'))
================================

select range_minus(numrange(10.1,12.2,'[]'), numrange(110.0,120.2,'(]'))
================================

select range_minus(numrange(10.1,12.2,'[]'), numrange(0.0,120.2,'(]'))
================================
 -- should fail

select range_merge(numrange(1.0, 2.0), numrange(2.0, 3.0))
================================

select range_merge(numrange(1.0, 2.0), numrange(1.5, 3.0))
================================

select range_merge(numrange(1.0, 2.0), numrange(2.5, 3.0))
================================

INSERT INTO numrange_test2 VALUES(numrange(1.1, 2.2))
================================

INSERT INTO numrange_test2 VALUES(numrange(1.1, 2.2))
================================

INSERT INTO numrange_test2 VALUES(numrange(1.1, 2.2,'()'))
================================


select * from numrange_test2 where nr = 'empty'::numrange
================================

select * from numrange_test2 where nr = numrange(1.1, 2.2)
================================

select * from numrange_test2 where nr = numrange(1.1, 2.3)
================================


insert into test_range_gist select int4range(g, g+10) from generate_series(1,2000) g
================================

insert into test_range_gist select 'empty'::int4range from generate_series(1,500) g
================================

insert into test_range_gist select int4range(g, g+10000) from generate_series(1,1000) g
================================

insert into test_range_gist select 'empty'::int4range from generate_series(1,500) g
================================

insert into test_range_gist select int4range(NULL,g*10,'(]') from generate_series(1,100) g
================================

insert into test_range_gist select int4range(g*10,NULL,'(]') from generate_series(1,100) g
================================

insert into test_range_gist select int4range(g, g+10) from generate_series(1,2000) g
================================


select count(*) from test_range_gist where ir @> 'empty'::int4range
================================

select count(*) from test_range_gist where ir = int4range(10,20)
================================

select count(*) from test_range_gist where ir && int4range(10,20)
================================

select count(*) from test_range_gist where ir << int4range(100,500)
================================

select count(*) from test_range_gist where ir >> int4range(100,500)
================================

select count(*) from test_range_gist where ir &< int4range(100,500)
================================

select count(*) from test_range_gist where ir &> int4range(100,500)
================================

select count(*) from test_range_gist where ir -|- int4range(100,500)
================================

select count(*) from test_range_gist where ir @> int4multirange(int4range(10,20), int4range(30,40))
================================

select count(*) from test_range_gist where ir << int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir >> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir &< int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir &> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir -|- int4multirange(int4range(100,200), int4range(400,500))
================================


select count(*) from test_range_gist where ir @> 'empty'::int4range
================================

select count(*) from test_range_gist where ir = int4range(10,20)
================================

select count(*) from test_range_gist where ir && int4range(10,20)
================================

select count(*) from test_range_gist where ir << int4range(100,500)
================================

select count(*) from test_range_gist where ir >> int4range(100,500)
================================

select count(*) from test_range_gist where ir &< int4range(100,500)
================================

select count(*) from test_range_gist where ir &> int4range(100,500)
================================

select count(*) from test_range_gist where ir -|- int4range(100,500)
================================

select count(*) from test_range_gist where ir @> int4multirange(int4range(10,20), int4range(30,40))
================================

select count(*) from test_range_gist where ir << int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir >> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir &< int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir &> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir -|- int4multirange(int4range(100,200), int4range(400,500))
================================


select count(*) from test_range_gist where ir @> 'empty'::int4range
================================

select count(*) from test_range_gist where ir = int4range(10,20)
================================

select count(*) from test_range_gist where ir && int4range(10,20)
================================

select count(*) from test_range_gist where ir << int4range(100,500)
================================

select count(*) from test_range_gist where ir >> int4range(100,500)
================================

select count(*) from test_range_gist where ir &< int4range(100,500)
================================

select count(*) from test_range_gist where ir &> int4range(100,500)
================================

select count(*) from test_range_gist where ir -|- int4range(100,500)
================================

select count(*) from test_range_gist where ir @> int4multirange(int4range(10,20), int4range(30,40))
================================

select count(*) from test_range_gist where ir << int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir >> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir &< int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir &> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_range_gist where ir -|- int4multirange(int4range(100,200), int4range(400,500))
================================


insert into test_range_spgist select int4range(g, g+10) from generate_series(1,2000) g
================================

insert into test_range_spgist select 'empty'::int4range from generate_series(1,500) g
================================

insert into test_range_spgist select int4range(g, g+10000) from generate_series(1,1000) g
================================

insert into test_range_spgist select 'empty'::int4range from generate_series(1,500) g
================================

insert into test_range_spgist select int4range(NULL,g*10,'(]') from generate_series(1,100) g
================================

insert into test_range_spgist select int4range(g*10,NULL,'(]') from generate_series(1,100) g
================================

insert into test_range_spgist select int4range(g, g+10) from generate_series(1,2000) g
================================


select count(*) from test_range_spgist where ir @> 'empty'::int4range
================================

select count(*) from test_range_spgist where ir = int4range(10,20)
================================

select count(*) from test_range_spgist where ir && int4range(10,20)
================================

select count(*) from test_range_spgist where ir << int4range(100,500)
================================

select count(*) from test_range_spgist where ir >> int4range(100,500)
================================

select count(*) from test_range_spgist where ir &< int4range(100,500)
================================

select count(*) from test_range_spgist where ir &> int4range(100,500)
================================

select count(*) from test_range_spgist where ir -|- int4range(100,500)
================================


select count(*) from test_range_spgist where ir @> 'empty'::int4range
================================

select count(*) from test_range_spgist where ir = int4range(10,20)
================================

select count(*) from test_range_spgist where ir && int4range(10,20)
================================

select count(*) from test_range_spgist where ir << int4range(100,500)
================================

select count(*) from test_range_spgist where ir >> int4range(100,500)
================================

select count(*) from test_range_spgist where ir &< int4range(100,500)
================================

select count(*) from test_range_spgist where ir &> int4range(100,500)
================================

select count(*) from test_range_spgist where ir -|- int4range(100,500)
================================


select count(*) from test_range_spgist where ir @> 'empty'::int4range
================================

select count(*) from test_range_spgist where ir = int4range(10,20)
================================

select count(*) from test_range_spgist where ir && int4range(10,20)
================================

select count(*) from test_range_spgist where ir << int4range(100,500)
================================

select count(*) from test_range_spgist where ir >> int4range(100,500)
================================

select count(*) from test_range_spgist where ir &< int4range(100,500)
================================

select count(*) from test_range_spgist where ir &> int4range(100,500)
================================

select count(*) from test_range_spgist where ir -|- int4range(100,500)
================================


-- test index-only scans
explain (costs off)
select ir from test_range_spgist where ir -|- int4range(10,20) order by ir
================================

select ir from test_range_spgist where ir -|- int4range(10,20) order by ir
================================

insert into test_range_elem select i from generate_series(1,100) i
================================


-- also test spgist index on anyrange expression
create index on test_range_elem using spgist(int4range(i,i+10))
================================

explain (costs off)
select count(*) from test_range_elem where int4range(i,i+10) <@ int4range(10,30)
================================

select count(*) from test_range_elem where int4range(i,i+10) <@ int4range(10,30)
================================


insert into test_range_excl
  values(int4range(123, 123, '[]'), int4range(1, 1, '[]'), '[2010-01-02 10:00, 2010-01-02 11:00)')
================================

insert into test_range_excl
  values(int4range(123, 123, '[]'), int4range(2, 2, '[]'), '[2010-01-02 11:00, 2010-01-02 12:00)')
================================

insert into test_range_excl
  values(int4range(123, 123, '[]'), int4range(3, 3, '[]'), '[2010-01-02 10:10, 2010-01-02 11:00)')
================================

insert into test_range_excl
  values(int4range(124, 124, '[]'), int4range(3, 3, '[]'), '[2010-01-02 10:10, 2010-01-02 11:10)')
================================

insert into test_range_excl
  values(int4range(125, 125, '[]'), int4range(1, 1, '[]'), '[2010-01-02 10:10, 2010-01-02 11:00)')
================================

select '[2010-01-01 01:00:00 -05, 2010-01-01 02:00:00 -08)'::tstzrange
================================

-- should fail
select '[2010-01-01 01:00:00 -08, 2010-01-01 02:00:00 -05)'::tstzrange
================================


--
-- Test user-defined range of floats
--

--should fail
create type float8range as range (subtype=float8, subtype_diff=float4mi)
================================


--should succeed
create type float8range as range (subtype=float8, subtype_diff=float8mi)
================================

create type mydomainrange as range(subtype=mydomain)
================================


--
-- Test domains over range types
--

create domain restrictedrange as int4range check (upper(value) < 10)
================================


--
-- Test multiple range types over the same subtype
--

create type textrange1 as range(subtype=text, collation="C")
================================

create type textrange2 as range(subtype=text, collation="C")
================================


drop type textrange1
================================

drop type textrange2
================================


--
-- Test polymorphic type system
--

create function anyarray_anyrange_func(a anyarray, r anyrange)
  returns anyelement as 'select $1[1] + lower($2)
================================
' language sql
================================


select anyarray_anyrange_func(ARRAY[1,2], int4range(10,20))
================================


-- should fail
select anyarray_anyrange_func(ARRAY[1,2], numrange(10,20))
================================


select range_add_bounds(int4range(1, 17))
================================

select range_add_bounds(numrange(1.0001, 123.123))
================================


select rangetypes_sql(int4range(1,10), ARRAY[2,20])
================================

select rangetypes_sql(numrange(1,10), ARRAY[2,20])
================================
  -- match failure

create function anycompatiblearray_anycompatiblerange_func(a anycompatiblearray, r anycompatiblerange)
  returns anycompatible as 'select $1[1] + lower($2)
================================
' language sql
================================


select anycompatiblearray_anycompatiblerange_func(ARRAY[1,2], int4range(10,20))
================================


select anycompatiblearray_anycompatiblerange_func(ARRAY[1,2], numrange(10,20))
================================


-- should fail
select anycompatiblearray_anycompatiblerange_func(ARRAY[1.1,2], int4range(10,20))
================================


--
-- Arrays of ranges
--

select ARRAY[numrange(1.1, 1.2), numrange(12.3, 155.5)]
================================


create table i8r_array (f1 int, f2 int8range[])
================================

insert into i8r_array values (42, array[int8range(1,10), int8range(2,20)])
================================


--
-- Ranges of arrays
--

create type arrayrange as range (subtype=int4[])
================================


--
-- Ranges of composites
--

create type two_ints as (a int, b int)
================================

create type two_ints_range as range (subtype = two_ints)
================================


-- this must be rejected to avoid self-inclusion issues:
alter type two_ints add attribute c two_ints_range
================================


drop type two_ints cascade
================================


--
-- Check behavior when subtype lacks a hash function
--

create type cashrange as range (subtype = money)
================================


select * from outparam_succeed(int4range(1,2))
================================


select * from outparam2_succeed(int4range(1,11))
================================


select * from outparam_succeed2(int4range(int4range(1,2)))
================================


select * from inoutparam_succeed(int4range(1,2))
================================


select * from table_succeed(int4range(1,11))
================================


================================


--
-- SELECT INTO and INSERT permission, if owner is not allowed to insert.
--
CREATE SCHEMA selinto_schema
================================

ALTER DEFAULT PRIVILEGES FOR ROLE regress_selinto_user
	  REVOKE INSERT ON TABLES FROM regress_selinto_user
================================

-- WITH DATA, passes.
CREATE TABLE selinto_schema.tbl_withdata1 (a)
  AS SELECT generate_series(1,3) WITH DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE selinto_schema.tbl_withdata2 (a) AS
  SELECT generate_series(1,3) WITH DATA
================================

-- WITH NO DATA, passes.
CREATE TABLE selinto_schema.tbl_nodata1 (a) AS
  SELECT generate_series(1,3) WITH NO DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE selinto_schema.tbl_nodata2 (a) AS
  SELECT generate_series(1,3) WITH NO DATA
================================

CREATE TABLE selinto_schema.tbl_withdata3 (a) AS
  EXECUTE data_sel WITH DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE selinto_schema.tbl_withdata4 (a) AS
  EXECUTE data_sel WITH DATA
================================

-- EXECUTE and WITH NO DATA, passes.
CREATE TABLE selinto_schema.tbl_nodata3 (a) AS
  EXECUTE data_sel WITH NO DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE selinto_schema.tbl_nodata4 (a) AS
  EXECUTE data_sel WITH NO DATA
================================


ALTER DEFAULT PRIVILEGES FOR ROLE regress_selinto_user
	  GRANT INSERT ON TABLES TO regress_selinto_user
================================

DROP SCHEMA selinto_schema CASCADE
================================

CREATE TABLE ctas_nodata (ii, jj, kk) AS SELECT i, j FROM ctas_base
================================
 -- Error
CREATE TABLE ctas_nodata (ii, jj, kk) AS SELECT i, j FROM ctas_base WITH NO DATA
================================
 -- Error
CREATE TABLE ctas_nodata (ii, jj) AS SELECT i, j FROM ctas_base
================================
 -- OK
CREATE TABLE ctas_nodata_2 (ii, jj) AS SELECT i, j FROM ctas_base WITH NO DATA
================================
 -- OK
CREATE TABLE ctas_nodata_3 (ii) AS SELECT i, j FROM ctas_base
================================
 -- OK
CREATE TABLE ctas_nodata_4 (ii) AS SELECT i, j FROM ctas_base WITH NO DATA
================================

$$ LANGUAGE SQL
================================


-- Try EXPLAIN ANALYZE SELECT INTO and EXPLAIN ANALYZE CREATE TABLE AS
-- WITH NO DATA, but hide the outputs since they won't be stable.
DO $$
BEGIN
	EXECUTE 'EXPLAIN ANALYZE SELECT * INTO TABLE easi FROM int8_tbl'
================================

	EXECUTE 'EXPLAIN ANALYZE CREATE TABLE easi2 AS SELECT * FROM int8_tbl WITH NO DATA'
================================

END$$
================================


--
-- Disallowed uses of SELECT ... INTO.  All should fail
--
DECLARE foo CURSOR FOR SELECT 1 INTO b
================================

INSERT INTO b SELECT 1 INTO f
================================


-- Test CREATE TABLE AS ... IF NOT EXISTS
CREATE TABLE ctas_ine_tbl AS SELECT 1
================================

CREATE TABLE ctas_ine_tbl AS SELECT 1 / 0
================================
 -- error
CREATE TABLE IF NOT EXISTS ctas_ine_tbl AS SELECT 1 / 0
================================
 -- ok
CREATE TABLE ctas_ine_tbl AS SELECT 1 / 0 WITH NO DATA
================================
 -- error
CREATE TABLE IF NOT EXISTS ctas_ine_tbl AS SELECT 1 / 0 WITH NO DATA
================================
 -- ok
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE ctas_ine_tbl AS SELECT 1 / 0
================================
 -- error
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE IF NOT EXISTS ctas_ine_tbl AS SELECT 1 / 0
================================
 -- ok
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE ctas_ine_tbl AS SELECT 1 / 0 WITH NO DATA
================================
 -- error
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE IF NOT EXISTS ctas_ine_tbl AS SELECT 1 / 0 WITH NO DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE ctas_ine_tbl AS EXECUTE ctas_ine_query
================================
 -- error
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE TABLE IF NOT EXISTS ctas_ine_tbl AS EXECUTE ctas_ine_query
================================


================================


-- anyarray column

CREATE TEMP TABLE rows AS
SELECT x, 'txt' || x as y
FROM generate_series(1,3) AS x
================================


-- to_jsonb, timestamps

select to_jsonb(timestamp '2014-05-28 12:22:35.614298')
================================


select to_jsonb(date '2014-05-28')
================================


select to_jsonb(date 'Infinity')
================================

select to_jsonb(date '-Infinity')
================================

select to_jsonb(timestamp 'Infinity')
================================

select to_jsonb(timestamp '-Infinity')
================================


--jsonb_agg

SELECT jsonb_agg(q)
  FROM ( SELECT $$a$$ || x AS b, y AS c,
               ARRAY[ROW(x.*,ARRAY[1,2,3]),
               ROW(y.*,ARRAY[4,5,6])] AS z
         FROM generate_series(1,2) x,
              generate_series(4,5) y) q
================================

SELECT jsonb_typeof('[]') AS array
================================

SELECT jsonb_typeof('["a", 1]') AS array
================================

SELECT jsonb_typeof('true') AS boolean
================================

SELECT jsonb_typeof('false') AS boolean
================================


-- populate_record
CREATE TYPE jbpop AS (a text, b int, c timestamp)
================================


create type jb_unordered_pair as (x int, y int)
================================


CREATE TYPE jsbrec AS (
	i	int,
	ia	_int4,
	ia1	int[],
	ia2	int[][],
	ia3	int[][][],
	ia1d	jsb_int_array_1d,
	ia2d	jsb_int_array_2d,
	t	text,
	ta	text[],
	c	char(10),
	ca	char(10)[],
	ts	timestamp,
	js	json,
	jsb	jsonb,
	jsa	json[],
	rec	jbpop,
	reca	jbpop[]
)
================================


CREATE TYPE jsbrec_i_not_null AS (
	i	jsb_int_not_null
)
================================


select * from jsonb_to_record('{"out": {"key": 1}}') as x(out json)
================================

select * from jsonb_to_record('{"out": [{"key": 1}]}') as x(out json)
================================

select * from jsonb_to_record('{"out": "{\"key\": 1}"}') as x(out json)
================================

select * from jsonb_to_record('{"out": {"key": 1}}') as x(out jsonb)
================================

select * from jsonb_to_record('{"out": [{"key": 1}]}') as x(out jsonb)
================================

select * from jsonb_to_record('{"out": "{\"key\": 1}"}') as x(out jsonb)
================================


INSERT INTO jsbpoptest
SELECT '{
	"jsa": [1, "2", null, 4],
	"rec": {"a": "abc", "c": "01.02.2003", "x": 43.2},
	"reca": [{"a": "abc", "b": 456}, null, {"c": "01.02.2003", "x": 43.2}]
}'::jsonb
FROM generate_series(1, 3)
================================


SELECT (jsonb_populate_record(NULL::jsbrec, js)).* FROM jsbpoptest
================================


DROP TYPE jsbrec
================================

DROP TYPE jsbrec_i_not_null
================================

DROP TYPE jb_unordered_pair
================================


-- jsonb_set_lax

\pset null NULL

-- pass though non nulls to jsonb_set
select jsonb_set_lax('{"a":1,"b":2}','{b}','5') 
================================

-- explicit treatments
select jsonb_set_lax('{"a":1,"b":2}', '{b}', null, null_value_treatment => 'raise_exception') as raise_exception
================================

select jsonb_set_lax('{"a":1,"b":2}', '{b}', null, null_value_treatment => 'return_target') as return_target
================================

select jsonb_set_lax('{"a":1,"b":2}', '{b}', null, null_value_treatment => 'delete_key') as delete_key
================================

select jsonb_set_lax('{"a":1,"b":2}', '{b}', null, null_value_treatment => 'use_json_null') as use_json_null
================================


\pset null ''

-- jsonb_insert
select jsonb_insert('{"a": [0,1,2]}', '{a, 1}', '"new_value"')
================================
 -- jsonb with data

-- update empty jsonb
update test_jsonb_subscript set test_json['a'] = '1' where id = 1
================================


-- update jsonb with some data
update test_jsonb_subscript set test_json['a'] = '1' where id = 2
================================


-- replace jsonb
update test_jsonb_subscript set test_json['a'] = '"test"'
================================


-- replace by object
update test_jsonb_subscript set test_json['a'] = '{"b": 1}'::jsonb
================================


-- replace by array
update test_jsonb_subscript set test_json['a'] = '[1, 2, 3]'::jsonb
================================


-- use jsonb subscription in where clause
select * from test_jsonb_subscript where test_json['key'] = '"value"'
================================

select * from test_jsonb_subscript where test_json['key_doesnt_exists'] = '"value"'
================================

select * from test_jsonb_subscript where test_json['key'] = '"wrong_value"'
================================


-- NULL
update test_jsonb_subscript set test_json[NULL] = '1'
================================

update test_jsonb_subscript set test_json['another_key'] = NULL
================================

update test_jsonb_subscript set test_json['a'] = '1' where id = 3
================================

update test_jsonb_subscript set test_json[0] = '1'
================================


update test_jsonb_subscript set test_json[5] = '1'
================================


update test_jsonb_subscript set test_json[-4] = '1'
================================


update test_jsonb_subscript set test_json[-8] = '1'
================================


update test_jsonb_subscript set test_json[5] = '1'
================================

update test_jsonb_subscript set test_json['a'][0]['b'][0]['c'] = '1'
================================

update test_jsonb_subscript set test_json['a'][2]['b'][2]['c'][2] = '1'
================================

update test_jsonb_subscript set test_json['a'][0] = '2'
================================

update test_jsonb_subscript set test_json[0]['a'] = '1'
================================

update test_jsonb_subscript set test_json[0]['a'] = '1'
================================

update test_jsonb_subscript set test_json[2]['b'] = '2'
================================

update test_jsonb_subscript set test_json['a']['b'][1] = '1'
================================

update test_jsonb_subscript set test_json['a']['b'][10] = '1'
================================

update test_jsonb_subscript set test_json[0][0][0] = '1'
================================

update test_jsonb_subscript set test_json[0][0][1] = '1'
================================

update test_jsonb_subscript set test_json['a']['b'][10] = '1'
================================

update test_jsonb_subscript set test_json['a'][10][10] = '1'
================================

update test_jsonb_subscript set test_json['a']['b']['c'][2] = '1'
================================

update test_jsonb_subscript set test_json['a'][1]['c'][2] = '1'
================================

update test_jsonb_subscript set test_json['a']['b'] = '1'
================================

update test_jsonb_subscript set test_json['a']['b']['c'] = '1'
================================

update test_jsonb_subscript set test_json['a'][0] = '1'
================================

update test_jsonb_subscript set test_json['a'][0]['c'] = '1'
================================

update test_jsonb_subscript set test_json['a'][0][0] = '1'
================================

update test_jsonb_subscript set test_json[0] = '1'
================================

update test_jsonb_subscript set test_json[0][0] = '1'
================================


-- ts_headline for jsonb
select ts_headline('{"a": "aaa bbb", "b": {"c": "ccc ddd fff", "c1": "ccc1 ddd1"}, "d": ["ggg hhh", "iii jjj"]}'::jsonb, tsquery('bbb & ddd & hhh'))
================================

select ts_headline('english', '{"a": "aaa bbb", "b": {"c": "ccc ddd fff"}, "d": ["ggg hhh", "iii jjj"]}'::jsonb, tsquery('bbb & ddd & hhh'))
================================

select ts_headline('{"a": "aaa bbb", "b": {"c": "ccc ddd fff", "c1": "ccc1 ddd1"}, "d": ["ggg hhh", "iii jjj"]}'::jsonb, tsquery('bbb & ddd & hhh'), 'StartSel = <, StopSel = >')
================================

select ts_headline('english', '{"a": "aaa bbb", "b": {"c": "ccc ddd fff", "c1": "ccc1 ddd1"}, "d": ["ggg hhh", "iii jjj"]}'::jsonb, tsquery('bbb & ddd & hhh'), 'StartSel = <, StopSel = >')
================================


-- corner cases for ts_headline with jsonb
select ts_headline('null'::jsonb, tsquery('aaa & bbb'))
================================

select ts_headline('{}'::jsonb, tsquery('aaa & bbb'))
================================

select ts_headline('[]'::jsonb, tsquery('aaa & bbb'))
================================


================================


\d test_tsvector

DROP INDEX wowidx
================================


\d test_tsvector

EXPLAIN (costs off) SELECT count(*) FROM test_tsvector WHERE a @@ 'wr|qh'
================================


-- ts_debug

SELECT * from ts_debug('english', '<myns:foo-bar_baz.blurfl>abc&nm1
================================
def&#xa9
================================
ghi&#245
================================
jkl</myns:foo-bar_baz.blurfl>')
================================

SELECT plainto_tsquery('english', 'foo bar') || !!plainto_tsquery('english', 'asd fg')
================================



SELECT ts_rank_cd(to_tsvector('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
'), to_tsquery('english', 'paint&water'))
================================


SELECT ts_rank_cd(to_tsvector('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
'), to_tsquery('english', 'breath&motion&water'))
================================


SELECT ts_rank_cd(to_tsvector('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
'), to_tsquery('english', 'ocean'))
================================


SELECT ts_rank_cd(to_tsvector('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
'), to_tsquery('english', 'painted <-> Ship'))
================================


--headline tests
SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'paint&water'))
================================


SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'breath&motion&water'))
================================


SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'ocean'))
================================


SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', phraseto_tsquery('english', 'painted Ocean'))
================================


SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', phraseto_tsquery('english', 'idle as a painted Ship'))
================================


SELECT ts_headline('english', '
<html>
<!-- some comment -->
<body>
Sea view wow <u>foo bar</u> <i>qq</i>
<a href="http://www.google.com/foo.bar.html" target="_blank">YES &nbsp
================================
</a>
ff-bg
<script>
       document.write(15)
================================

</script>
</body>
</html>',
to_tsquery('english', 'sea&foo'), 'HighlightAll=true')
================================


--Check if headline fragments work
SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'ocean'), 'MaxFragments=1')
================================


--Check if more than one fragments are displayed
SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'Coleridge & stuck'), 'MaxFragments=2')
================================


--Fragments when there all query words are not in the document
SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'ocean & seahorse'), 'MaxFragments=1')
================================


--FragmentDelimiter option
SELECT ts_headline('english', '
Day after day, day after day,
  We stuck, nor breath nor motion,
As idle as a painted Ship
  Upon a painted Ocean.
Water, water, every where
  And all the boards did shrink
================================

Water, water, every where,
  Nor any drop to drink.
S. T. Coleridge (1772-1834)
', to_tsquery('english', 'Coleridge & stuck'), 'MaxFragments=2,FragmentDelimiter=***')
================================

\set ECHO none
\copy test_tsquery from stdin
'New York'	new <-> york | big <-> apple | nyc
Moscow	moskva | moscow
'Sanct Peter'	Peterburg | peter | 'Sanct Peterburg'
foo & bar & qq	foo & (bar | qq) & city
1 & (2 <-> 3)	2 <-> 4
5 <-> 6	5 <-> 7
\.
\set ECHO all

ALTER TABLE test_tsquery ADD COLUMN keyword tsquery
================================


SELECT ts_rewrite('foo & bar & qq & new & york',  'new & york'::tsquery, 'big & apple | nyc | new & york & city')
================================

select * from pendtest where 'ipsu:*'::tsquery @@ ts
================================

select * from pendtest where 'ipsa:*'::tsquery @@ ts
================================

select * from pendtest where 'ips:*'::tsquery @@ ts
================================

select * from pendtest where 'ipt:*'::tsquery @@ ts
================================

select * from pendtest where 'ipi:*'::tsquery @@ ts
================================

select websearch_to_tsquery('\')
================================


================================


-- save counters
CREATE TABLE prevstats AS
SELECT t.seq_scan, t.seq_tup_read, t.idx_scan, t.idx_tup_fetch,
       (b.heap_blks_read + b.heap_blks_hit) AS heap_blks,
       (b.idx_blks_read + b.idx_blks_hit) AS idx_blks,
       pg_stat_get_snapshot_timestamp() as snap_ts
  FROM pg_catalog.pg_stat_user_tables AS t,
       pg_catalog.pg_statio_user_tables AS b
 WHERE t.relname='tenk2' AND b.relname='tenk2'
================================

  updated1 bool
================================

  updated2 bool
================================

  updated3 bool
================================

  updated4 bool
================================
 loop will exit after 30 seconds
  for i in 1 .. 300 loop

    -- With parallel query, the seqscan and indexscan on tenk2 might be done
    -- in parallel worker processes, which will send their stats counters
    -- asynchronously to what our own session does.  So we must check for
    -- those counts to be registered separately from the update counts.

    -- check to see if seqscan has been sensed
    SELECT (st.seq_scan >= pr.seq_scan + 1) INTO updated1
      FROM pg_stat_user_tables AS st, pg_class AS cl, prevstats AS pr
     WHERE st.relname='tenk2' AND cl.relname='tenk2'
================================


    exit when updated1 and updated2 and updated3 and updated4
================================


    -- wait a little
    perform pg_sleep_for('100 milliseconds')
================================


    -- reset stats snapshot so we can test again
    perform pg_stat_clear_snapshot()
================================


  -- report time waited in postmaster log (where it won't change test output)
  raise log 'wait_for_stats delayed % seconds',
    extract(epoch from clock_timestamp() - start_time)
================================


-- check that n_live_tup is reset to 0 after truncate
INSERT INTO trunc_stats_test DEFAULT VALUES
================================

INSERT INTO trunc_stats_test DEFAULT VALUES
================================

INSERT INTO trunc_stats_test DEFAULT VALUES
================================


-- test involving a truncate in a transaction
================================
 4 ins but only 1 live
INSERT INTO trunc_stats_test1 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test1 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test1 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test1 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test2 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test2 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test2 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test2 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test3 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test3 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test3 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test3 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test3 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test4 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test4 DEFAULT VALUES
================================

INSERT INTO trunc_stats_test4 DEFAULT VALUES
================================


-- We can't just call wait_for_stats() at this point, because we only
-- transmit stats when the session goes idle, and we probably didn't
-- transmit the last couple of counts yet thanks to the rate-limiting logic
-- in pgstat_report_stat().  But instead of waiting for the rate limiter's
-- timeout to elapse, let's just start a new session.  The old one will
-- then send its stats before dying.
\c -

-- wait for stats collector to update
SELECT wait_for_stats()
================================

-- End of Stats Test

================================


\d gtest1

-- duplicate generated
CREATE TABLE gtest_err_1 (a int PRIMARY KEY, b int GENERATED ALWAYS AS (a * 2) STORED GENERATED ALWAYS AS (a * 3) STORED)
================================


-- GENERATED BY DEFAULT not allowed
CREATE TABLE gtest_err_8 (a int PRIMARY KEY, b int GENERATED BY DEFAULT AS (a * 2) STORED)
================================
  -- ok

ALTER VIEW gtest1v ALTER COLUMN b SET DEFAULT 100
================================

\d gtest1_1
INSERT INTO gtest1_1 VALUES (4)
================================

\d gtest_normal_child
INSERT INTO gtest_normal (a) VALUES (1)
================================

3
4
\.

COPY gtest1 (a, b) FROM stdin
================================

3
4
\.

COPY gtest3 (a, b) FROM stdin
================================


-- composite types
CREATE TYPE double_int as (a int, b int)
================================

DROP TYPE double_int
================================


\d gtest10

CREATE TABLE gtest10a (a int PRIMARY KEY, b int GENERATED ALWAYS AS (a * 2) STORED)
================================

\d gtest22c

INSERT INTO gtest22c VALUES (1), (2), (3)
================================

\d gtest23b

INSERT INTO gtest23b VALUES (1)
================================
  -- error

-- typed tables (currently not supported)
CREATE TYPE gtest_type AS (f1 integer, f2 text, f3 bigint)
================================

DROP TYPE gtest_type CASCADE
================================

\d gtest25

-- ALTER TABLE ... ALTER COLUMN
CREATE TABLE gtest27 (
    a int,
    b int,
    x int GENERATED ALWAYS AS ((a + b) * 2) STORED
)
================================

\d gtest27
SELECT * FROM gtest27
================================

\d gtest27
-- Ideally you could just do this, but not today (and should x change type?):
ALTER TABLE gtest27
  ALTER COLUMN a TYPE float8,
  ALTER COLUMN b TYPE float8
================================
  -- error
\d gtest27
SELECT * FROM gtest27
================================

ALTER TABLE gtest29 ALTER COLUMN a DROP EXPRESSION
================================
  -- error
ALTER TABLE gtest29 ALTER COLUMN a DROP EXPRESSION IF EXISTS
================================
  -- notice
ALTER TABLE gtest29 ALTER COLUMN b DROP EXPRESSION
================================

\d gtest29

-- check that dependencies between columns have also been removed
ALTER TABLE gtest29 DROP COLUMN a
================================
  -- should not drop b
\d gtest29

-- with inheritance
CREATE TABLE gtest30 (
    a int,
    b int GENERATED ALWAYS AS (a * 2) STORED
)
================================

ALTER TABLE gtest30 ALTER COLUMN b DROP EXPRESSION
================================

\d gtest30
\d gtest30_1
DROP TABLE gtest30 CASCADE
================================

ALTER TABLE ONLY gtest30 ALTER COLUMN b DROP EXPRESSION
================================
  -- error
\d gtest30
\d gtest30_1
ALTER TABLE gtest30_1 ALTER COLUMN b DROP EXPRESSION
================================

  IF tg_op IN ('INSERT', 'UPDATE') THEN
    RAISE INFO '%: %: new = %', TG_NAME, TG_WHEN, NEW
================================

  IF tg_op = 'DELETE' THEN
    RETURN OLD
================================

  ELSE
    RETURN NEW
================================


CREATE TRIGGER gtest2b BEFORE INSERT OR UPDATE ON gtest26
  FOR EACH ROW
  WHEN (NEW.* IS NOT NULL)  -- error
  EXECUTE PROCEDURE gtest_trigger_func()
================================


DROP TRIGGER gtest1 ON gtest26
================================

DROP TRIGGER gtest2 ON gtest26
================================

DROP TRIGGER gtest3 ON gtest26
================================

  RETURN NEW
================================


DROP TRIGGER gtest11 ON gtest26
================================

  NEW.b = 300
================================

  RETURN NEW
================================

$$
================================


CREATE TABLE gtest28b (LIKE gtest28a INCLUDING GENERATED)
================================


\d gtest28*

================================


================================
-- Tests for multirange data types.

--
-- test input parser
--

-- negative tests
================================
 should fail
select ''::textmultirange
================================


--
-- test casts, both a built-in range type and a user-defined one:
--
select 'empty'::int4range::int4multirange
================================


--
-- test unnest(multirange) function
--
select unnest(int4multirange(int4range('5', '6'), int4range('1', '2')))
================================

INSERT INTO nummultirange_test VALUES(nummultirange(variadic '{}'::numrange[]))
================================

INSERT INTO nummultirange_test VALUES(nummultirange(numrange(1.1, 2.2)))
================================

INSERT INTO nummultirange_test VALUES(nummultirange(numrange(1.7, 1.7, '[]'), numrange(1.7, 1.9)))
================================

INSERT INTO nummultirange_test VALUES(nummultirange(numrange(1.7, 1.7, '[]'), numrange(1.9, 2.1)))
================================


select nummultirange(numrange(2.0, 1.0))
================================

select nummultirange(numrange(5.0, 6.0), numrange(1.0, 2.0))
================================


-- overlaps
SELECT * FROM nummultirange_test WHERE range_overlaps_multirange(numrange(4.0, 4.2), nmr)
================================

SELECT * FROM nummultirange_test WHERE numrange(4.0, 4.2) && nmr
================================

SELECT * FROM nummultirange_test WHERE multirange_overlaps_range(nmr, numrange(4.0, 4.2))
================================

SELECT * FROM nummultirange_test WHERE nmr && numrange(4.0, 4.2)
================================

SELECT * FROM nummultirange_test WHERE multirange_overlaps_multirange(nmr, nummultirange(numrange(4.0, 4.2), numrange(6.0, 7.0)))
================================

SELECT * FROM nummultirange_test WHERE nmr && nummultirange(numrange(4.0, 4.2), numrange(6.0, 7.0))
================================

SELECT * FROM nummultirange_test WHERE nmr && nummultirange(numrange(6.0, 7.0))
================================

SELECT * FROM nummultirange_test WHERE nmr && nummultirange(numrange(6.0, 7.0), numrange(8.0, 9.0))
================================

SELECT * FROM nummultirange_test WHERE multirange_contains_range(nmr, numrange(4.0, 4.2))
================================

SELECT * FROM nummultirange_test WHERE range_contained_by_multirange(numrange(4.0, 4.2), nmr)
================================

SELECT * FROM nummultirange_test WHERE numrange(4.0, 4.2) <@ nmr
================================


-- overlaps
SELECT 'empty'::numrange && nummultirange()
================================

SELECT 'empty'::numrange && nummultirange(numrange(1,2))
================================

SELECT nummultirange() && 'empty'::numrange
================================

SELECT nummultirange(numrange(1,2)) && 'empty'::numrange
================================

SELECT nummultirange() && nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) && nummultirange()
================================

SELECT nummultirange(numrange(3,4)) && nummultirange(numrange(1,2), numrange(7,8))
================================

SELECT nummultirange(numrange(1,2), numrange(7,8)) && nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(3,4)) && nummultirange(numrange(1,2), numrange(3.5,8))
================================

SELECT nummultirange(numrange(1,2), numrange(3.5,8)) && numrange(3,4)
================================

SELECT nummultirange(numrange(1,2), numrange(3.5,8)) && nummultirange(numrange(3,4))
================================

select '{(10,20),(30,40),(50,60)}'::nummultirange && '(42,92)'::numrange
================================

SELECT nummultirange() @> 'empty'::numrange
================================

SELECT nummultirange(numrange(null,null)) @> numrange(1,2)
================================

SELECT nummultirange(numrange(null,null)) @> numrange(null,2)
================================

SELECT nummultirange(numrange(null,null)) @> numrange(2,null)
================================

SELECT nummultirange(numrange(null,5)) @> numrange(null,3)
================================

SELECT nummultirange(numrange(null,5)) @> numrange(null,8)
================================

SELECT nummultirange(numrange(5,null)) @> numrange(8,null)
================================

SELECT nummultirange(numrange(5,null)) @> numrange(3,null)
================================

SELECT nummultirange(numrange(1,5)) @> numrange(8,9)
================================

SELECT nummultirange(numrange(1,5)) @> numrange(3,9)
================================

SELECT nummultirange(numrange(1,5)) @> numrange(1,4)
================================

SELECT nummultirange(numrange(1,5)) @> numrange(1,5)
================================

SELECT nummultirange(numrange(-4,-2), numrange(1,5)) @> numrange(1,5)
================================

SELECT nummultirange(numrange(1,5), numrange(8,9)) @> numrange(1,5)
================================

SELECT nummultirange(numrange(1,5), numrange(8,9)) @> numrange(6,7)
================================

SELECT nummultirange(numrange(1,5), numrange(6,9)) @> numrange(6,7)
================================

select '{(10,20),(30,40),(50,60)}'::nummultirange @> '(52,56)'::numrange
================================

SELECT 'empty'::numrange <@ nummultirange()
================================

SELECT nummultirange(numrange(1,2)) <@ numrange(null,null)
================================

SELECT nummultirange(numrange(null,2)) <@ numrange(null,null)
================================

SELECT nummultirange(numrange(2,null)) <@ numrange(null,null)
================================

SELECT nummultirange(numrange(null,3)) <@ numrange(null,5)
================================

SELECT nummultirange(numrange(null,8)) <@ numrange(null,5)
================================

SELECT nummultirange(numrange(8,null)) <@ numrange(5,null)
================================

SELECT nummultirange(numrange(3,null)) <@ numrange(5,null)
================================

SELECT nummultirange(numrange(8,9)) <@ numrange(1,5)
================================

SELECT nummultirange(numrange(3,9)) <@ numrange(1,5)
================================

SELECT nummultirange(numrange(1,4)) <@ numrange(1,5)
================================

SELECT nummultirange(numrange(1,5)) <@ numrange(1,5)
================================

SELECT nummultirange(numrange(-4,-2), numrange(1,5)) <@ numrange(1,9)
================================

SELECT nummultirange(numrange(1,5), numrange(8,9)) <@ numrange(1,9)
================================

SELECT nummultirange(numrange(1,5), numrange(6,9)) <@ numrange(1,9)
================================

SELECT nummultirange(numrange(1,5), numrange(6,10)) <@ numrange(1,9)
================================


-- overleft
SELECT 'empty'::numrange &< nummultirange()
================================

SELECT 'empty'::numrange &< nummultirange(numrange(1,2))
================================

SELECT nummultirange() &< 'empty'::numrange
================================

SELECT nummultirange(numrange(1,2)) &< 'empty'::numrange
================================

SELECT nummultirange() &< nummultirange()
================================

SELECT nummultirange(numrange(1,2)) &< nummultirange()
================================

SELECT nummultirange() &< nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(6,7)) &< numrange(3,4)
================================

SELECT nummultirange(numrange(1,2)) &< numrange(3,4)
================================

SELECT nummultirange(numrange(1,4)) &< numrange(3,4)
================================

SELECT nummultirange(numrange(1,6)) &< numrange(3,4)
================================

SELECT nummultirange(numrange(3.5,6)) &< numrange(3,4)
================================

SELECT nummultirange(numrange(6,7)) &< nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,2)) &< nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,4)) &< nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,6)) &< nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(3.5,6)) &< nummultirange(numrange(3,4))
================================


-- overright
SELECT nummultirange() &> 'empty'::numrange
================================

SELECT nummultirange(numrange(1,2)) &> 'empty'::numrange
================================

SELECT 'empty'::numrange &> nummultirange()
================================

SELECT 'empty'::numrange &> nummultirange(numrange(1,2))
================================

SELECT nummultirange() &> nummultirange()
================================

SELECT nummultirange() &> nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) &> nummultirange()
================================

SELECT nummultirange(numrange(3,4)) &> numrange(6,7)
================================

SELECT nummultirange(numrange(3,4)) &> numrange(1,2)
================================

SELECT nummultirange(numrange(3,4)) &> numrange(1,4)
================================

SELECT nummultirange(numrange(3,4)) &> numrange(1,6)
================================

SELECT nummultirange(numrange(3,4)) &> numrange(3.5,6)
================================

SELECT nummultirange(numrange(3,4)) &> nummultirange(numrange(6,7))
================================

SELECT nummultirange(numrange(3,4)) &> nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(3,4)) &> nummultirange(numrange(1,4))
================================

SELECT nummultirange(numrange(3,4)) &> nummultirange(numrange(1,6))
================================

SELECT nummultirange(numrange(3,4)) &> nummultirange(numrange(3.5,6))
================================


-- meets
SELECT 'empty'::numrange -|- nummultirange()
================================

SELECT 'empty'::numrange -|- nummultirange(numrange(1,2))
================================

SELECT nummultirange() -|- 'empty'::numrange
================================

SELECT nummultirange(numrange(1,2)) -|- 'empty'::numrange
================================

SELECT nummultirange(numrange(1,2)) -|- nummultirange()
================================

SELECT nummultirange() -|- nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) -|- numrange(2,4)
================================

SELECT nummultirange(numrange(1,2)) -|- numrange(3,4)
================================

SELECT nummultirange(numrange(1,2)) -|- nummultirange(numrange(2,4))
================================

SELECT nummultirange(numrange(1,2)) -|- nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,2), numrange(5,6)) -|- nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,2), numrange(5,6)) -|- nummultirange(numrange(6,7))
================================

SELECT nummultirange(numrange(1,2), numrange(5,6)) -|- nummultirange(numrange(8,9))
================================

SELECT nummultirange(numrange(1,2)) -|- nummultirange(numrange(2,4), numrange(6,7))
================================


-- strictly left
select 'empty'::numrange << nummultirange()
================================

select nummultirange() << 'empty'::numrange
================================

select nummultirange() << numrange(1,2)
================================

select nummultirange(numrange(3,4)) << numrange(3,6)
================================

select nummultirange(numrange(0,2)) << numrange(3,6)
================================

select nummultirange(numrange(0,2), numrange(7,8)) << numrange(3,6)
================================

select nummultirange(numrange(-4,-2), numrange(0,2)) << numrange(3,6)
================================

select nummultirange() << nummultirange()
================================

select nummultirange() << nummultirange(numrange(1,2))
================================

select nummultirange(numrange(1,2)) << nummultirange()
================================

select nummultirange(numrange(1,2)) << nummultirange(numrange(1,2))
================================

select nummultirange(numrange(1,2)) << nummultirange(numrange(3,4))
================================

select nummultirange(numrange(1,2)) << nummultirange(numrange(3,4), numrange(7,8))
================================

select nummultirange(numrange(1,2), numrange(4,5)) << nummultirange(numrange(3,4), numrange(7,8))
================================


-- strictly right
select nummultirange() >> 'empty'::numrange
================================

select nummultirange() >> numrange(1,2)
================================

select nummultirange(numrange(3,4)) >> numrange(1,2)
================================

select nummultirange(numrange(0,4)) >> numrange(1,2)
================================

select nummultirange(numrange(0,4), numrange(7,8)) >> numrange(1,2)
================================

select 'empty'::numrange >> nummultirange()
================================

select nummultirange() >> nummultirange()
================================

select nummultirange(numrange(1,2)) >> nummultirange()
================================

select nummultirange() >> nummultirange(numrange(1,2))
================================

select nummultirange(numrange(1,2)) >> nummultirange(numrange(1,2))
================================

select nummultirange(numrange(3,4)) >> nummultirange(numrange(1,2))
================================

select nummultirange(numrange(3,4), numrange(7,8)) >> nummultirange(numrange(1,2))
================================

select nummultirange(numrange(3,4), numrange(7,8)) >> nummultirange(numrange(1,2), numrange(4,5))
================================

SELECT nummultirange() + nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) + nummultirange()
================================

SELECT nummultirange(numrange(1,2)) + nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) + nummultirange(numrange(2,4))
================================

SELECT nummultirange(numrange(1,2)) + nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) + nummultirange(numrange(2,4))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) + nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) + nummultirange(numrange(0,9))
================================

SELECT range_merge(nummultirange(numrange(1,2)))
================================

SELECT range_merge(nummultirange(numrange(1,2), numrange(7,8)))
================================

SELECT nummultirange() - nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) - nummultirange()
================================

SELECT nummultirange(numrange(1,2), numrange(3,4)) - nummultirange()
================================

SELECT nummultirange(numrange(1,2)) - nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) - nummultirange(numrange(2,4))
================================

SELECT nummultirange(numrange(1,2)) - nummultirange(numrange(3,4))
================================

SELECT nummultirange(numrange(1,4)) - nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,4)) - nummultirange(numrange(2,3))
================================

SELECT nummultirange(numrange(1,4)) - nummultirange(numrange(0,8))
================================

SELECT nummultirange(numrange(1,4)) - nummultirange(numrange(0,2))
================================

SELECT nummultirange(numrange(1,8)) - nummultirange(numrange(0,2), numrange(3,4))
================================

SELECT nummultirange(numrange(1,8)) - nummultirange(numrange(2,3), numrange(5,null))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) - nummultirange(numrange(-2,0))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) - nummultirange(numrange(2,4))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) - nummultirange(numrange(3,5))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) - nummultirange(numrange(0,9))
================================

SELECT nummultirange(numrange(1,3), numrange(4,5)) - nummultirange(numrange(2,9))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) - nummultirange(numrange(8,9))
================================

SELECT nummultirange(numrange(1,2), numrange(4,5)) - nummultirange(numrange(-2,0), numrange(8,9))
================================

SELECT nummultirange() * nummultirange(numrange(1,2))
================================

SELECT nummultirange(numrange(1,2)) * nummultirange()
================================

insert into test_multirange_gist select int4multirange(int4range(g, g+10),int4range(g+20, g+30),int4range(g+40, g+50)) from generate_series(1,2000) g
================================

insert into test_multirange_gist select '{}'::int4multirange from generate_series(1,500) g
================================

insert into test_multirange_gist select int4multirange(int4range(g, g+10000)) from generate_series(1,1000) g
================================

insert into test_multirange_gist select int4multirange(int4range(NULL, g*10, '(]'), int4range(g*10, g*20, '(]')) from generate_series(1,100) g
================================

insert into test_multirange_gist select int4multirange(int4range(g*10, g*20, '(]'), int4range(g*20, NULL, '(]')) from generate_series(1,100) g
================================

select count(*) from test_multirange_gist where mr @> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr && 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr <@ 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr << 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr >> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr &< 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr &> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr -|- 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr << '{}'::int4multirange
================================

select count(*) from test_multirange_gist where mr >> '{}'::int4multirange
================================

select count(*) from test_multirange_gist where mr &< '{}'::int4multirange
================================

select count(*) from test_multirange_gist where mr &> '{}'::int4multirange
================================


select count(*) from test_multirange_gist where mr = int4multirange(int4range(10,20), int4range(30,40), int4range(50,60))
================================

select count(*) from test_multirange_gist where mr && int4range(10,20)
================================

select count(*) from test_multirange_gist where mr << int4range(100,500)
================================

select count(*) from test_multirange_gist where mr >> int4range(100,500)
================================

select count(*) from test_multirange_gist where mr &< int4range(100,500)
================================

select count(*) from test_multirange_gist where mr &> int4range(100,500)
================================

select count(*) from test_multirange_gist where mr -|- int4range(100,500)
================================

select count(*) from test_multirange_gist where mr @> int4multirange(int4range(10,20), int4range(30,40))
================================

select count(*) from test_multirange_gist where mr << int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr >> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr &< int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr &> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr -|- int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr @> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr && 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr <@ 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr << 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr >> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr &< 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr &> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr -|- 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr << '{}'::int4multirange
================================

select count(*) from test_multirange_gist where mr >> '{}'::int4multirange
================================

select count(*) from test_multirange_gist where mr &< '{}'::int4multirange
================================

select count(*) from test_multirange_gist where mr &> '{}'::int4multirange
================================


select count(*) from test_multirange_gist where mr @> 'empty'::int4range
================================

select count(*) from test_multirange_gist where mr = int4multirange(int4range(10,20), int4range(30,40), int4range(50,60))
================================

select count(*) from test_multirange_gist where mr && int4range(10,20)
================================

select count(*) from test_multirange_gist where mr << int4range(100,500)
================================

select count(*) from test_multirange_gist where mr >> int4range(100,500)
================================

select count(*) from test_multirange_gist where mr &< int4range(100,500)
================================

select count(*) from test_multirange_gist where mr &> int4range(100,500)
================================

select count(*) from test_multirange_gist where mr -|- int4range(100,500)
================================

select count(*) from test_multirange_gist where mr @> int4multirange(int4range(10,20), int4range(30,40))
================================

select count(*) from test_multirange_gist where mr << int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr >> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr &< int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr &> int4multirange(int4range(100,200), int4range(400,500))
================================

select count(*) from test_multirange_gist where mr -|- int4multirange(int4range(100,200), int4range(400,500))
================================

insert into reservations values
-- 1: has a meets and a gap
(1, daterange('2018-07-01', '2018-07-07')),
(1, daterange('2018-07-07', '2018-07-14')),
(1, daterange('2018-07-20', '2018-07-22')),
-- 2: just a single row
(2, daterange('2018-07-01', '2018-07-03')),
-- 3: one null range
(3, NULL),
-- 4: two null ranges
(4, NULL),
(4, NULL),
-- 5: a null range and a non-null range
(5, NULL),
(5, daterange('2018-07-01', '2018-07-03')),
-- 6: has overlap
(6, daterange('2018-07-01', '2018-07-07')),
(6, daterange('2018-07-05', '2018-07-10')),
-- 7: two ranges that meet: no gap or overlap
(7, daterange('2018-07-01', '2018-07-07')),
(7, daterange('2018-07-07', '2018-07-14')),
-- 8: an empty range
(8, 'empty'::daterange)

================================

INSERT INTO nummultirange_test2 VALUES(nummultirange(numrange(1.1, 2.2)))
================================

INSERT INTO nummultirange_test2 VALUES(nummultirange(numrange(1.1, 2.2)))
================================

INSERT INTO nummultirange_test2 VALUES(nummultirange(numrange(1.1, 2.2,'()')))
================================

select * from nummultirange_test2 where nmr = nummultirange(numrange(1.1, 2.2))
================================

select * from nummultirange_test2 where nmr = nummultirange(numrange(1.1, 2.3))
================================

create type mydomainrange as range(subtype=mydomain)
================================


---
-- Check automatic naming of multiranges
---

create type intr as range(subtype=int)
================================

drop type intr
================================

create type intmultirange as (x int, y int)
================================

create type intrange as range(subtype=int)
================================
 -- should fail
drop type intmultirange
================================

create type intr_multirange as (x int, y int)
================================

create type intr as range(subtype=int)
================================
 -- should fail
drop type intr_multirange
================================


--
-- Test multiple multirange types over the same subtype and manual naming of
-- the multirange type.
--

-- should fail
create type textrange1 as range(subtype=text, multirange_type_name=int, collation="C")
================================

-- should pass
create type textrange1 as range(subtype=text, multirange_type_name=multirange_of_text, collation="C")
================================

-- should pass, because existing _textrange1 is automatically renamed
create type textrange2 as range(subtype=text, multirange_type_name=_textrange1, collation="C")
================================


drop type textrange1
================================

drop type textrange2
================================


--
-- Test polymorphic type system
--

create function anyarray_anymultirange_func(a anyarray, r anymultirange)
  returns anyelement as 'select $1[1] + lower($2)
================================
' language sql
================================


select anyarray_anymultirange_func(ARRAY[1,2], int4multirange(int4range(10,20)))
================================


-- should fail
select anyarray_anymultirange_func(ARRAY[1,2], nummultirange(numrange(10,20)))
================================


select range_add_bounds(int4multirange(int4range(1, 17)))
================================

select range_add_bounds(nummultirange(numrange(1.0001, 123.123)))
================================


select multirangetypes_sql(int4multirange(int4range(1,10)), ARRAY[2,20])
================================

select multirangetypes_sql(nummultirange(numrange(1,10)), ARRAY[2,20])
================================
  -- match failure

create function anycompatiblearray_anycompatiblemultirange_func(a anycompatiblearray, mr anycompatiblemultirange)
  returns anycompatible as 'select $1[1] + lower($2)
================================
' language sql
================================


select anycompatiblearray_anycompatiblemultirange_func(ARRAY[1,2], multirange(int4range(10,20)))
================================


select anycompatiblearray_anycompatiblemultirange_func(ARRAY[1,2], multirange(numrange(10,20)))
================================


-- should fail
select anycompatiblearray_anycompatiblemultirange_func(ARRAY[1.1,2], multirange(int4range(10,20)))
================================


create function anycompatiblerange_anycompatiblemultirange_func(r anycompatiblerange, mr anycompatiblemultirange)
  returns anycompatible as 'select lower($1) + lower($2)
================================
' language sql
================================


select anycompatiblerange_anycompatiblemultirange_func(int4range(1,2), multirange(int4range(10,20)))
================================


-- should fail
select anycompatiblerange_anycompatiblemultirange_func(numrange(1,2), multirange(int4range(10,20)))
================================


--
-- Arrays of multiranges
--

select ARRAY[nummultirange(numrange(1.1, 1.2)), nummultirange(numrange(12.3, 155.5))]
================================

insert into i8mr_array values (42, array[int8multirange(int8range(1,10)), int8multirange(int8range(2,20))])
================================


--
-- Ranges of composites
--

create type two_ints as (a int, b int)
================================

create type two_ints_range as range (subtype = two_ints)
================================


drop type two_ints cascade
================================


select * from mr_outparam_succeed(int4multirange(int4range(1,2)))
================================


select * from mr_outparam_succeed2(int4multirange(int4range(1,2)))
================================

select * from mr_outparam_succeed3(int4multirange(int4range(1,2)))
================================


select * from mr_outparam_succeed4(int4range(1,2))
================================


select * from mr_inoutparam_succeed(int4multirange(int4range(1,2)))
================================


select * from mr_table_succeed(123, int4multirange(int4range(1,11)))
================================
 $$ language plpgsql
================================

select mr_polymorphic(int4range(1, 4))
================================


================================

    return next
================================

$$
================================

  element jsonb
================================

  matching_nodes jsonb := '[]'::jsonb
================================

  while jsonb_array_length(elements) > 0 loop
    element := elements->0
================================

    elements := elements - 0
================================

    case jsonb_typeof(element)
    when 'array' then
      if jsonb_array_length(element) > 0 then
        elements := elements || element
================================

    when 'object' then
      if element ? 'Plan' then
        elements := elements || jsonb_build_array(element->'Plan')
================================

        element := element - 'Plan'
================================

      else
        if element ? 'Plans' then
          elements := elements || jsonb_build_array(element->'Plans')
================================

          element := element - 'Plans'
================================

        if (element->>'Node Type')::text = 'Incremental Sort' then
          matching_nodes := matching_nodes || element
================================

  return matching_nodes
================================

$$
================================

  node jsonb
================================

  group_key text
================================

  space_key text
================================

        node := jsonb_set(node, array[group_key, space_key, 'Peak Sort Space Used'], '"NN"', false)
================================

    nodes := nodes || node
================================

  return nodes
================================

$$
================================

  group_stats jsonb
================================

  group_key text
================================

  space_key text
================================

      for space_key in select unnest(array['Sort Space Memory', 'Sort Space Disk']::text[]) t loop
        if (group_stats->space_key->'Peak Sort Space Used')::bigint < (group_stats->space_key->'Peak Sort Space Used')::bigint then
          raise exception '% has invalid max space < average space', group_key
================================

  return true
================================

$$
================================


-- A single large group tested around each mode transition point.
insert into t(a, b) select i/100 + 1, i + 1 from generate_series(0, 999) n(i)
================================


-- An initial large group followed by a small group.
insert into t(a, b) select i/50 + 1, i + 1 from generate_series(0, 999) n(i)
================================


-- An initial small group followed by a large group.
insert into t(a, b) select (case when i < 5 then i else 9 end), i from generate_series(1, 1000) n(i)
================================


-- Small groups of 10 tuples each tested around each mode transition point.
insert into t(a, b) select i / 10, i from generate_series(1, 1000) n(i)
================================


-- Small groups of only 1 tuple each tested around each mode transition point.
insert into t(a, b) select i, i from generate_series(1, 1000) n(i)
================================

insert into t select mod(i,10),mod(i,10),i from generate_series(1,10000) s(i)
================================


================================

INSERT INTO idxpart (a, b, c) SELECT i, i, i FROM generate_series(1, 50) i
================================

\d idxpart1
alter table idxpart attach partition idxpart1 for values from (0) to (10)
================================

\d idxpart1
\d+ idxpart1_a_idx
\d+ idxpart1_b_c_idx

-- Forbid ALTER TABLE when attaching or detaching an index to a partition.
create index idxpart_c on only idxpart (c)
================================

\d idxpart1
select relname, relkind, relhassubclass, inhparent::regclass
    from pg_class left join pg_index ix on (indexrelid = oid)
	left join pg_inherits on (ix.indexrelid = inhrelid)
	where relname like 'idxpart%' order by relname
================================

\d idxpart1
drop table idxpart
================================


-- If CREATE INDEX ONLY, don't create indexes on partitions
================================
 and existing
-- indexes on partitions don't change parent.  ALTER INDEX ATTACH can change
-- the parent after the fact.
create table idxpart (a int) partition by range (a)
================================

-- Here we expect that idxpart1 and idxpart2 have a new index, but idxpart21
-- does not
================================
 also, idxpart22 is not attached.
\d idxpart1
\d idxpart2
\d idxpart21
select indexrelid::regclass, indrelid::regclass, inhparent::regclass
  from pg_index idx left join pg_inherits inh on (idx.indexrelid = inh.inhrelid)
where indexrelid::regclass::text like 'idxpart%'
  order by indexrelid::regclass::text collate "C"
================================

\d idxpart2
-- ... but this one is.
create index on idxpart21 (a)
================================

\d idxpart2
drop table idxpart
================================

\d idxpart1
select relname, relkind, inhparent::regclass
    from pg_class left join pg_index ix on (indexrelid = oid)
	left join pg_inherits on (ix.indexrelid = inhrelid)
	where relname like 'idxpart%' order by relname
================================

\d idxpart1
select relname, relkind, inhparent::regclass
    from pg_class left join pg_index ix on (indexrelid = oid)
	left join pg_inherits on (ix.indexrelid = inhrelid)
	where relname like 'idxpart%' order by relname
================================

\d idxpart2
alter table idxpart2 drop column c
================================

\d idxpart2
drop table idxpart, idxpart2
================================

\d idxpart
\d idxpart1
select attrelid::regclass, attname, attnum from pg_attribute
  where attrelid::regclass::text like 'idxpart%' and attnum > 0
  order by attrelid::regclass, attnum
================================

\d idxpart
\d idxpart1
select attrelid::regclass, attname, attnum from pg_attribute
  where attrelid::regclass::text like 'idxpart%' and attnum > 0
  order by attrelid::regclass, attnum
================================

\d idxpart
-- multiple primary key on child should fail
create table failpart partition of idxpart (b primary key) for values from (0) to (100)
================================

create table idxpart1pk partition of idxpart (a primary key) for values from (0) to (100)
================================

\d idxpart1pk
drop table idxpart
================================
	-- this works
\d idxpart
create table idxpart1 partition of idxpart for values from (0, 0) to (1000, 1000)
================================

\d idxpart1
drop table idxpart
================================
		-- this works
\d idxpart
drop table idxpart
================================


-- If a partitioned table has a unique/PK constraint, then it's not possible
-- to drop the corresponding constraint in the children
================================
 nor it's possible
-- to drop the indexes individually.  Dropping the constraint in the parent
-- gets rid of the lot.
create table idxpart (i int) partition by hash (i)
================================


-- if a partition has a unique index without a constraint, does not attach
-- automatically
================================
 creates a new index instead.
create table idxpart (a int, b int) partition by range (a)
================================

insert into idxpart select 2^g, format('two to power of %s', g) from generate_series(15, 17) g
================================

insert into idxpart select a * 2, b || b from idxpart where a between 2^16 and 2^19
================================


-- More objects intentionally left behind, to verify some pg_dump/pg_upgrade
-- behavior
================================
 see https://postgr.es/m/20190321204928.GA17535@alvherre.pgsql
create schema regress_indexing
================================

\d parted_index_col_drop
\d parted_index_col_drop1
\d parted_index_col_drop2
\d parted_index_col_drop11
drop table parted_index_col_drop
================================


================================


-- count roughly 1/10 of the tuples
CREATE TABLE RANDOM_TBL AS
  SELECT count(*) AS random
  FROM onek WHERE random() < 1.0/10
================================


-- select again, the count should be different
INSERT INTO RANDOM_TBL (random)
  SELECT count(*)
  FROM onek WHERE random() < 1.0/10
================================


-- select again, the count should be different
INSERT INTO RANDOM_TBL (random)
  SELECT count(*)
  FROM onek WHERE random() < 1.0/10
================================


-- select again, the count should be different
INSERT INTO RANDOM_TBL (random)
  SELECT count(*)
  FROM onek WHERE random() < 1.0/10
================================


================================


\d+ numeric_view

explain (verbose, costs off) select * from numeric_view
================================

$$ language plpgsql stable
================================

$$ language plpgsql stable
================================


create type myint
================================


create type myint (input = myintin, output = myintout, like = int4)
================================


create cast (int4 as myint) without function
================================

create cast (myint as int4) without function
================================

  else
    return $1::int = $2::int
================================

$$ language plpgsql immutable
================================

$$ language plpgsql immutable
================================


create operator = (
  leftarg    = myint,
  rightarg   = myint,
  commutator = =,
  negator    = <>,
  procedure  = myinteq,
  restrict   = eqsel,
  join       = eqjoinsel,
  merges
)
================================


create operator <> (
  leftarg    = myint,
  rightarg   = myint,
  commutator = <>,
  negator    = =,
  procedure  = myintne,
  restrict   = eqsel,
  join       = eqjoinsel,
  merges
)
================================


create operator class myint_ops
default for type myint using hash as
  operator    1   =  (myint, myint),
  function    1   myinthash(myint)
================================


================================


SELECT U&'d\0061t\+000061' AS U&"d\0061t\+000061"
================================

SELECT U&'d!0061t\+000061' UESCAPE '!' AS U&"d*0061t\+000061" UESCAPE '*'
================================

SELECT U&'a\\b' AS "a\b"
================================


SELECT U&' \' UESCAPE '!' AS "tricky"
================================


SELECT U&'wrong: \061'
================================

SELECT U&'wrong: \+0061'
================================

SELECT U&'wrong: +0061' UESCAPE +
================================

SELECT U&'wrong: +0061' UESCAPE '+'
================================


SELECT U&'wrong: \db99'
================================

SELECT U&'wrong: \db99xy'
================================

SELECT U&'wrong: \db99\\'
================================

SELECT U&'wrong: \db99\0061'
================================

SELECT U&'wrong: \+00db99\+000061'
================================

SELECT U&'wrong: \+2FFFFF'
================================


SELECT U&'d\0061t\+000061' AS U&"d\0061t\+000061"
================================

SELECT U&'d!0061t\+000061' UESCAPE '!' AS U&"d*0061t\+000061" UESCAPE '*'
================================


SELECT U&' \' UESCAPE '!' AS "tricky"
================================


SELECT U&'wrong: \061'
================================

SELECT U&'wrong: \+0061'
================================

SELECT U&'wrong: +0061' UESCAPE '+'
================================

SELECT E'\\xDeAdBeEf'::bytea
================================

SELECT E'\\x De Ad Be Ef '::bytea
================================

SELECT E'\\xDeAdBeE'::bytea
================================

SELECT E'\\xDeAdBeEx'::bytea
================================

SELECT E'\\xDe00BeEf'::bytea
================================

SELECT E'DeAdBeEf'::bytea
================================

SELECT E'De\\000dBeEf'::bytea
================================

SELECT E'De\123dBeEf'::bytea
================================

SELECT E'De\\123dBeEf'::bytea
================================

SELECT E'De\\678dBeEf'::bytea
================================

SELECT E'\\xDeAdBeEf'::bytea
================================

SELECT E'\\x De Ad Be Ef '::bytea
================================

SELECT E'\\xDe00BeEf'::bytea
================================

SELECT E'DeAdBeEf'::bytea
================================

SELECT E'De\\000dBeEf'::bytea
================================

SELECT E'De\\123dBeEf'::bytea
================================


-- T581 regular expression substring (with SQL's bizarre regexp syntax)
SELECT SUBSTRING('abcdefg' SIMILAR 'a#"(b_d)#"%' ESCAPE '#') AS "bcd"
================================


-- No match should return NULL
SELECT SUBSTRING('abcdefg' SIMILAR '#"(b_d)#"%' ESCAPE '#') IS NULL AS "True"
================================


-- Null inputs should return NULL
SELECT SUBSTRING('abcdefg' SIMILAR '%' ESCAPE NULL) IS NULL AS "True"
================================

SELECT SUBSTRING(NULL SIMILAR '%' ESCAPE '#') IS NULL AS "True"
================================

SELECT SUBSTRING('abcdefg' SIMILAR NULL ESCAPE '#') IS NULL AS "True"
================================


-- The first and last parts should act non-greedy
SELECT SUBSTRING('abcdefg' SIMILAR 'a#"%#"g' ESCAPE '#') AS "bcdef"
================================

SELECT SUBSTRING('abcdefg' SIMILAR 'a*#"%#"g*' ESCAPE '#') AS "abcdefg"
================================


-- Vertical bar in any part affects only that part
SELECT SUBSTRING('abcdefg' SIMILAR 'a|b#"%#"g' ESCAPE '#') AS "bcdef"
================================

SELECT SUBSTRING('abcdefg' SIMILAR 'a#"%#"x|g' ESCAPE '#') AS "bcdef"
================================

SELECT SUBSTRING('abcdefg' SIMILAR 'a#"%|ab#"g' ESCAPE '#') AS "bcdef"
================================


-- Can't have more than two part separators
SELECT SUBSTRING('abcdefg' SIMILAR 'a*#"%#"g*#"x' ESCAPE '#') AS "error"
================================


-- Postgres extension: with 0 or 1 separator, assume parts 1 and 3 are empty
SELECT SUBSTRING('abcdefg' SIMILAR 'a#"%g' ESCAPE '#') AS "bcdefg"
================================

SELECT SUBSTRING('abcdefg' SIMILAR 'a%g' ESCAPE '#') AS "abcdefg"
================================


-- substring() with just two arguments is not allowed by SQL spec
================================


-- Check behavior of SIMILAR TO, which uses largely the same regexp variant
SELECT 'abcdefg' SIMILAR TO '_bcd%' AS true
================================

SELECT 'abcdefg' SIMILAR TO 'bcd%' AS false
================================

SELECT 'abcdefg' SIMILAR TO '_bcd#%' ESCAPE '#' AS false
================================

SELECT 'abcd%' SIMILAR TO '_bcd#%' ESCAPE '#' AS true
================================

-- Postgres uses '\' as the default escape character, which is not per spec
SELECT 'abcdefg' SIMILAR TO '_bcd\%' AS false
================================

-- and an empty string to mean "no escape", which is also not per spec
SELECT 'abcd\efg' SIMILAR TO '_bcd\%' ESCAPE '' AS true
================================

-- these behaviors are per spec, though:
SELECT 'abcdefg' SIMILAR TO '_bcd%' ESCAPE NULL AS null
================================


-- set so we can tell NULL from empty string
\pset null '\\N'

-- return all matches from regexp
SELECT regexp_matches('foobarbequebaz', $re$(bar)(beque)$re$)
================================


-- test case insensitive
SELECT regexp_matches('foObARbEqUEbAz', $re$(bar)(beque)$re$, 'i')
================================


-- global option - more than one match
SELECT regexp_matches('foobarbequebazilbarfbonk', $re$(b[^b]+)(b[^b]+)$re$, 'g')
================================


-- empty capture group (matched empty string)
SELECT regexp_matches('foobarbequebaz', $re$(bar)(.*)(beque)$re$)
================================

-- no match
SELECT regexp_matches('foobarbequebaz', $re$(bar)(.+)(beque)$re$)
================================

-- optional capture group did not match, null entry in array
SELECT regexp_matches('foobarbequebaz', $re$(bar)(.+)?(beque)$re$)
================================


-- give me errors
SELECT regexp_matches('foobarbequebaz', $re$(bar)(beque)$re$, 'gz')
================================

SELECT regexp_matches('foobarbequebaz', $re$(barbeque$re$)
================================

SELECT regexp_matches('foobarbequebaz', $re$(bar)(beque){2,1}$re$)
================================


-- split string on regexp
SELECT foo, length(foo) FROM regexp_split_to_table('the quick brown fox jumps over the lazy dog', $re$\s+$re$) AS foo
================================

SELECT regexp_split_to_array('the quick brown fox jumps over the lazy dog', $re$\s+$re$)
================================


SELECT foo, length(foo) FROM regexp_split_to_table('the quick brown fox jumps over the lazy dog', $re$\s*$re$) AS foo
================================

SELECT regexp_split_to_array('the quick brown fox jumps over the lazy dog', $re$\s*$re$)
================================


-- change NULL-display back
\pset null ''

-- E021-11 position expression
SELECT POSITION('4' IN '1234567890') = '4' AS "4"
================================


SELECT 'abc'::bytea LIKE '_b_'::bytea AS "true"
================================

SELECT 'abc'::bytea NOT LIKE '_b_'::bytea AS "false"
================================


SELECT 'a_c'::bytea LIKE 'a$__'::bytea ESCAPE '$'::bytea AS "true"
================================

SELECT 'a_c'::bytea NOT LIKE 'a$__'::bytea ESCAPE '$'::bytea AS "false"
================================


select md5(''::bytea) = 'd41d8cd98f00b204e9800998ecf8427e' AS "TRUE"
================================


select md5('a'::bytea) = '0cc175b9c0f1b6a831c399e269772661' AS "TRUE"
================================


select md5('abc'::bytea) = '900150983cd24fb0d6963f7d28e17f72' AS "TRUE"
================================


select md5('message digest'::bytea) = 'f96b697d7cb7938d525a2f31aaf161d0' AS "TRUE"
================================


select md5('abcdefghijklmnopqrstuvwxyz'::bytea) = 'c3fcd3d76192e4007dfb496cca67e13b' AS "TRUE"
================================


select md5('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'::bytea) = 'd174ab98d277d9f5a5611c2c9f419d9f' AS "TRUE"
================================


select md5('12345678901234567890123456789012345678901234567890123456789012345678901234567890'::bytea) = '57edf4a22be3c955ac49da2e2107b67a' AS "TRUE"
================================

SELECT encode(('\x' || repeat('1234567890abcdef0001', 7))::bytea, 'base64')
================================

SELECT decode(encode(('\x' || repeat('1234567890abcdef0001', 7))::bytea,
                     'base64'), 'base64')
================================


--
-- get_bit/set_bit etc
--
SELECT get_bit('\x1234567890abcdef00'::bytea, 43)
================================

SELECT get_bit('\x1234567890abcdef00'::bytea, 99)
================================
  -- error
SELECT set_bit('\x1234567890abcdef00'::bytea, 43, 0)
================================

SELECT set_bit('\x1234567890abcdef00'::bytea, 99, 0)
================================
  -- error
SELECT get_byte('\x1234567890abcdef00'::bytea, 3)
================================

SELECT get_byte('\x1234567890abcdef00'::bytea, 99)
================================
  -- error
SELECT set_byte('\x1234567890abcdef00'::bytea, 7, 11)
================================

SELECT set_byte('\x1234567890abcdef00'::bytea, 99, 11)
================================


SELECT SUBSTRING('1234567890'::bytea FROM 3) "34567890"
================================

SELECT SUBSTRING('1234567890'::bytea FROM 4 FOR 3) AS "456"
================================

SELECT SUBSTRING('string'::bytea FROM 2 FOR 2147483646) AS "tring"
================================

SELECT SUBSTRING('string'::bytea FROM -10 FOR 2147483646) AS "string"
================================

SELECT SUBSTRING('string'::bytea FROM -10 FOR -2147483646) AS "error"
================================


SELECT trim(E'\\000'::bytea from E'\\000Tom\\000'::bytea)
================================

SELECT trim(leading E'\\000'::bytea from E'\\000Tom\\000'::bytea)
================================

SELECT trim(trailing E'\\000'::bytea from E'\\000Tom\\000'::bytea)
================================

SELECT btrim(E'\\000trim\\000'::bytea, E'\\000'::bytea)
================================

SELECT btrim(''::bytea, E'\\000'::bytea)
================================

SELECT btrim(E'\\000trim\\000'::bytea, ''::bytea)
================================

SELECT encode(overlay(E'Th\\000omas'::bytea placing E'Th\\001omas'::bytea from 2),'escape')
================================

SELECT encode(overlay(E'Th\\000omas'::bytea placing E'\\002\\003'::bytea from 8),'escape')
================================

SELECT encode(overlay(E'Th\\000omas'::bytea placing E'\\002\\003'::bytea from 5 for 3),'escape')
================================


SELECT bit_count('\x1234567890'::bytea)
================================


================================

1	1	1	1	1	B0101
3	3	3	null	2	B0100
7	7	7	3	4	B1100
\.

SELECT
  BIT_AND(i2) AS "1",
  BIT_AND(i4) AS "1",
  BIT_AND(i8) AS "1",
  BIT_AND(i)  AS "?",
  BIT_AND(x)  AS "0",
  BIT_AND(y)  AS "0100",

  BIT_OR(i2)  AS "7",
  BIT_OR(i4)  AS "7",
  BIT_OR(i8)  AS "7",
  BIT_OR(i)   AS "?",
  BIT_OR(x)   AS "7",
  BIT_OR(y)   AS "1101",

  BIT_XOR(i2) AS "5",
  BIT_XOR(i4) AS "5",
  BIT_XOR(i8) AS "5",
  BIT_XOR(i)  AS "?",
  BIT_XOR(x)  AS "7",
  BIT_XOR(y)  AS "1101"
FROM bitwise_test
================================

TRUE	null	FALSE	null
FALSE	TRUE	null	null
null	TRUE	FALSE	null
\.

SELECT
  BOOL_AND(b1)     AS "f",
  BOOL_AND(b2)     AS "t",
  BOOL_AND(b3)     AS "f",
  BOOL_AND(b4)     AS "n",
  BOOL_AND(NOT b2) AS "f",
  BOOL_AND(NOT b3) AS "t"
FROM bool_test
================================


create type avg_state as (total bigint, count bigint)
================================

	if state is null then
		if n is not null then
			new_state.total := n
================================

			new_state.count := 1
================================

			return new_state
================================

		return null
================================

	elsif n is not null then
		state.total := state.total + n
================================

		state.count := state.count + 1
================================

		return state
================================


	return null
================================

	else
		return state.total / state.count
================================

	else
		return state.total
================================


create aggregate my_avg(int4)
(
   stype = avg_state,
   sfunc = avg_transfn,
   finalfunc = avg_finalfn
)
================================


create aggregate my_sum(int4)
(
   stype = avg_state,
   sfunc = avg_transfn,
   finalfunc = sum_finalfn
)
================================


-- test that aggs with the same sfunc and initcond share the same agg state
create aggregate my_sum_init(int4)
(
   stype = avg_state,
   sfunc = avg_transfn,
   finalfunc = sum_finalfn,
   initcond = '(10,0)'
)
================================


create aggregate my_avg_init(int4)
(
   stype = avg_state,
   sfunc = avg_transfn,
   finalfunc = avg_finalfn,
   initcond = '(10,0)'
)
================================


create aggregate my_avg_init2(int4)
(
   stype = avg_state,
   sfunc = avg_transfn,
   finalfunc = avg_finalfn,
   initcond = '(4,0)'
)
================================

	if state is null then
		if n is not null then
			new_state := n
================================

			return new_state
================================

		return null
================================

	elsif n is not null then
		state := state + n
================================

		return state
================================


	return null
================================

	else
		return state / 2
================================


create aggregate my_sum(int4)
(
   stype = int4,
   sfunc = sum_transfn
)
================================


create aggregate my_half_sum(int4)
(
   stype = int4,
   sfunc = sum_transfn,
   finalfunc = halfsum_finalfn
)
================================

    RETURN NULL
================================

END$$
================================


CREATE AGGREGATE balk(int4)
(
    SFUNC = balkifnull(int8, int4),
    STYPE = int8,
    PARALLEL = SAFE,
    INITCOND = '0'
)
================================

    RETURN NULL
================================

END$$
================================


CREATE AGGREGATE balk(int4)
(
    SFUNC = int4_sum(int8, int4),
    STYPE = int8,
    COMBINEFUNC = balkifnull(int8, int8),
    PARALLEL = SAFE,
    INITCOND = '0'
)
================================


create table agg_data_2k as
select g from generate_series(0, 1999) g
================================


create table agg_data_20k as
select g from generate_series(0, 19999) g
================================


create table agg_group_1 as
select g%10000 as c1, sum(g::numeric) as c2, count(*) as c3
  from agg_data_20k group by g%10000
================================


create table agg_group_2 as
select * from
  (values (100), (300), (500)) as r(a),
  lateral (
    select (g/2)::numeric as c1,
           array_agg(g::numeric) as c2,
	   count(*) as c3
    from agg_data_2k
    where g < r.a
    group by g/2) as s
================================


create table agg_group_3 as
select (g/2)::numeric as c1, sum(7::int4) as c2, count(*) as c3
  from agg_data_2k group by g/2
================================


create table agg_group_4 as
select (g/2)::numeric as c1, array_agg(g::numeric) as c2, count(*) as c3
  from agg_data_2k group by g/2
================================


create table agg_hash_1 as
select g%10000 as c1, sum(g::numeric) as c2, count(*) as c3
  from agg_data_20k group by g%10000
================================


create table agg_hash_2 as
select * from
  (values (100), (300), (500)) as r(a),
  lateral (
    select (g/2)::numeric as c1,
           array_agg(g::numeric) as c2,
	   count(*) as c3
    from agg_data_2k
    where g < r.a
    group by g/2) as s
================================


create table agg_hash_3 as
select (g/2)::numeric as c1, sum(7::int4) as c2, count(*) as c3
  from agg_data_2k group by g/2
================================


create table agg_hash_4 as
select (g/2)::numeric as c1, array_agg(g::numeric) as c2, count(*) as c3
  from agg_data_2k group by g/2
================================


================================


--
-- test extract!
--
SELECT f1 as "date",
    date_part('year', f1) AS year,
    date_part('month', f1) AS month,
    date_part('day', f1) AS day,
    date_part('quarter', f1) AS quarter,
    date_part('decade', f1) AS decade,
    date_part('century', f1) AS century,
    date_part('millennium', f1) AS millennium,
    date_part('isoyear', f1) AS isoyear,
    date_part('week', f1) AS week,
    date_part('dow', f1) AS dow,
    date_part('isodow', f1) AS isodow,
    date_part('doy', f1) AS doy,
    date_part('julian', f1) AS julian,
    date_part('epoch', f1) AS epoch
    FROM date_tbl
================================

--
-- epoch
--
SELECT EXTRACT(EPOCH FROM DATE        '1970-01-01')
================================
     --  0
--
-- century
--
SELECT EXTRACT(CENTURY FROM DATE '0101-12-31 BC')
================================
 -- -2
SELECT EXTRACT(CENTURY FROM DATE '0100-12-31 BC')
================================
 -- -1
SELECT EXTRACT(CENTURY FROM DATE '0001-12-31 BC')
================================
 -- -1
SELECT EXTRACT(CENTURY FROM DATE '0001-01-01')
================================
    --  1
SELECT EXTRACT(CENTURY FROM DATE '0001-01-01 AD')
================================
 --  1
SELECT EXTRACT(CENTURY FROM DATE '1900-12-31')
================================
    -- 19
SELECT EXTRACT(CENTURY FROM DATE '1901-01-01')
================================
    -- 20
SELECT EXTRACT(CENTURY FROM DATE '2000-12-31')
================================
    -- 20
SELECT EXTRACT(CENTURY FROM DATE '2001-01-01')
================================
    -- 21
SELECT EXTRACT(CENTURY FROM CURRENT_DATE)>=21 AS True
================================
     -- true
--
-- millennium
--
SELECT EXTRACT(MILLENNIUM FROM DATE '0001-12-31 BC')
================================
 -- -1
SELECT EXTRACT(MILLENNIUM FROM DATE '0001-01-01 AD')
================================
 --  1
SELECT EXTRACT(MILLENNIUM FROM DATE '1000-12-31')
================================
    --  1
SELECT EXTRACT(MILLENNIUM FROM DATE '1001-01-01')
================================
    --  2
SELECT EXTRACT(MILLENNIUM FROM DATE '2000-12-31')
================================
    --  2
SELECT EXTRACT(MILLENNIUM FROM DATE '2001-01-01')
================================
    --  3
-- next test to be fixed on the turn of the next millennium
================================
-)
SELECT EXTRACT(MILLENNIUM FROM CURRENT_DATE)
================================
         --  3
--
-- decade
--
SELECT EXTRACT(DECADE FROM DATE '1994-12-25')
================================
    -- 199
SELECT EXTRACT(DECADE FROM DATE '0010-01-01')
================================
    --   1
SELECT EXTRACT(DECADE FROM DATE '0009-12-31')
================================
    --   0
SELECT EXTRACT(DECADE FROM DATE '0001-01-01 BC')
================================
 --   0
SELECT EXTRACT(DECADE FROM DATE '0002-12-31 BC')
================================
 --  -1
SELECT EXTRACT(DECADE FROM DATE '0011-01-01 BC')
================================
 --  -1
SELECT EXTRACT(DECADE FROM DATE '0012-12-31 BC')
================================
 --  -2
--
-- all possible fields
--
SELECT EXTRACT(MICROSECONDS  FROM DATE '2020-08-11')
================================

SELECT EXTRACT(MILLISECONDS  FROM DATE '2020-08-11')
================================

SELECT EXTRACT(SECOND        FROM DATE '2020-08-11')
================================

SELECT EXTRACT(MINUTE        FROM DATE '2020-08-11')
================================

SELECT EXTRACT(HOUR          FROM DATE '2020-08-11')
================================

SELECT EXTRACT(DAY           FROM DATE '2020-08-11')
================================

SELECT EXTRACT(MONTH         FROM DATE '2020-08-11')
================================

SELECT EXTRACT(YEAR          FROM DATE '2020-08-11')
================================

SELECT EXTRACT(YEAR          FROM DATE '2020-08-11 BC')
================================

SELECT EXTRACT(DECADE        FROM DATE '2020-08-11')
================================

SELECT EXTRACT(CENTURY       FROM DATE '2020-08-11')
================================

SELECT EXTRACT(MILLENNIUM    FROM DATE '2020-08-11')
================================

SELECT EXTRACT(ISOYEAR       FROM DATE '2020-08-11')
================================

SELECT EXTRACT(ISOYEAR       FROM DATE '2020-08-11 BC')
================================

SELECT EXTRACT(QUARTER       FROM DATE '2020-08-11')
================================

SELECT EXTRACT(WEEK          FROM DATE '2020-08-11')
================================

SELECT EXTRACT(DOW           FROM DATE '2020-08-11')
================================

SELECT EXTRACT(DOW           FROM DATE '2020-08-16')
================================

SELECT EXTRACT(ISODOW        FROM DATE '2020-08-11')
================================

SELECT EXTRACT(ISODOW        FROM DATE '2020-08-16')
================================

SELECT EXTRACT(DOY           FROM DATE '2020-08-11')
================================

SELECT EXTRACT(TIMEZONE      FROM DATE '2020-08-11')
================================

SELECT EXTRACT(TIMEZONE_M    FROM DATE '2020-08-11')
================================

SELECT EXTRACT(TIMEZONE_H    FROM DATE '2020-08-11')
================================

SELECT EXTRACT(EPOCH         FROM DATE '2020-08-11')
================================

SELECT EXTRACT(JULIAN        FROM DATE '2020-08-11')
================================

--
-- test trunc function!
--
SELECT DATE_TRUNC('MILLENNIUM', TIMESTAMP '1970-03-20 04:30:00.00000')
================================
 -- 1001
SELECT DATE_TRUNC('MILLENNIUM', DATE '1970-03-20')
================================
 -- 1001-01-01
SELECT DATE_TRUNC('CENTURY', TIMESTAMP '1970-03-20 04:30:00.00000')
================================
 -- 1901
SELECT DATE_TRUNC('CENTURY', DATE '1970-03-20')
================================
 -- 1901
SELECT DATE_TRUNC('CENTURY', DATE '2004-08-10')
================================
 -- 2001-01-01
SELECT DATE_TRUNC('CENTURY', DATE '0002-02-04')
================================
 -- 0001-01-01
SELECT DATE_TRUNC('CENTURY', DATE '0055-08-10 BC')
================================
 -- 0100-01-01 BC
SELECT DATE_TRUNC('DECADE', DATE '1993-12-25')
================================
 -- 1990-01-01
SELECT DATE_TRUNC('DECADE', DATE '0004-12-25')
================================
 -- 0001-01-01 BC
SELECT DATE_TRUNC('DECADE', DATE '0002-12-31 BC')
================================

--
-- oscillating fields from non-finite date:
--
SELECT EXTRACT(DAY FROM DATE 'infinity')
================================
      -- NULL
SELECT EXTRACT(DAY FROM DATE '-infinity')
================================
     -- NULL
-- all supported fields
SELECT EXTRACT(DAY           FROM DATE 'infinity')
================================
    -- NULL
SELECT EXTRACT(MONTH         FROM DATE 'infinity')
================================
    -- NULL
SELECT EXTRACT(QUARTER       FROM DATE 'infinity')
================================
    -- NULL
SELECT EXTRACT(WEEK          FROM DATE 'infinity')
================================
    -- NULL
SELECT EXTRACT(DOW           FROM DATE 'infinity')
================================
    -- NULL
SELECT EXTRACT(ISODOW        FROM DATE 'infinity')
================================
    -- NULL
SELECT EXTRACT(DOY           FROM DATE 'infinity')
================================
    -- NULL
--
-- monotonic fields from non-finite date:
--
SELECT EXTRACT(EPOCH FROM DATE 'infinity')
================================
         --  Infinity
SELECT EXTRACT(EPOCH FROM DATE '-infinity')
================================
        -- -Infinity
-- all supported fields
SELECT EXTRACT(YEAR       FROM DATE 'infinity')
================================
    --  Infinity
SELECT EXTRACT(DECADE     FROM DATE 'infinity')
================================
    --  Infinity
SELECT EXTRACT(CENTURY    FROM DATE 'infinity')
================================
    --  Infinity
SELECT EXTRACT(MILLENNIUM FROM DATE 'infinity')
================================
    --  Infinity
SELECT EXTRACT(JULIAN     FROM DATE 'infinity')
================================
    --  Infinity
SELECT EXTRACT(ISOYEAR    FROM DATE 'infinity')
================================
    --  Infinity
SELECT EXTRACT(EPOCH      FROM DATE 'infinity')
================================
    --  Infinity
--
-- wrong fields from non-finite date:
--
SELECT EXTRACT(MICROSEC  FROM DATE 'infinity')
================================


================================
-- pg_regress should ensure that this default value applies
================================
 however
-- we can't rely on any specific default value of vacuum_cost_delay
SHOW datestyle
================================

DISCARD TEMP
================================


--
-- Test DISCARD ALL
--

-- do changes
DECLARE foo CURSOR WITH HOLD FOR SELECT 1
================================

LISTEN foo_event
================================

-- discard everything
DISCARD ALL
================================

create schema not_there_initially
================================

drop schema not_there_initially
================================

  return current_setting('work_mem')
================================

  return current_setting('work_mem')
================================

  perform 1/$1
================================

  return current_setting('work_mem')
================================


-- Normally, CREATE FUNCTION should complain about invalid values in
-- function SET options
================================
 but not if check_function_bodies is off,
-- because that creates ordering hazards for pg_dump

create function func_with_bad_set() returns int as $$ select 1 $$
language sql
set default_text_search_config = no_such_config
================================


================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 <> int2 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 <> int4 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 = int2 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 = int4 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 < int2 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 < int4 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 <= int2 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 <= int4 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 > int2 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 > int4 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 >= int2 '0'
================================


SELECT i.* FROM INT4_TBL i WHERE i.f1 >= int4 '0'
================================


-- positive odds
SELECT i.* FROM INT4_TBL i WHERE (i.f1 % int2 '2') = int2 '1'
================================


-- any evens
SELECT i.* FROM INT4_TBL i WHERE (i.f1 % int4 '2') = int2 '0'
================================


SELECT i.f1, i.f1 * int2 '2' AS x FROM INT4_TBL i
================================


SELECT i.f1, i.f1 * int2 '2' AS x FROM INT4_TBL i
WHERE abs(f1) < 1073741824
================================


SELECT i.f1, i.f1 * int4 '2' AS x FROM INT4_TBL i
================================


SELECT i.f1, i.f1 * int4 '2' AS x FROM INT4_TBL i
WHERE abs(f1) < 1073741824
================================


SELECT i.f1, i.f1 + int2 '2' AS x FROM INT4_TBL i
================================


SELECT i.f1, i.f1 + int2 '2' AS x FROM INT4_TBL i
WHERE f1 < 2147483646
================================


SELECT i.f1, i.f1 + int4 '2' AS x FROM INT4_TBL i
================================


SELECT i.f1, i.f1 + int4 '2' AS x FROM INT4_TBL i
WHERE f1 < 2147483646
================================


SELECT i.f1, i.f1 / int2 '2' AS x FROM INT4_TBL i
================================


SELECT i.f1, i.f1 / int4 '2' AS x FROM INT4_TBL i
================================


-- corner case
SELECT (-1::int4<<31)::text
================================

SELECT ((-1::int4<<31)+1)::text
================================
 -- overflow

================================

$$
================================

$$
================================


SELECT * FROM rank() OVER (ORDER BY random())
================================


SELECT rank() OVER (PARTITION BY four, ORDER BY ten) FROM tenk1
================================


CREATE AGGREGATE logging_agg_nonstrict (anyelement)
(
	stype = text,
	sfunc = logging_sfunc_nonstrict,
	mstype = text,
	msfunc = logging_msfunc_nonstrict,
	minvfunc = logging_minvfunc_nonstrict
)
================================


CREATE AGGREGATE logging_agg_nonstrict_initcond (anyelement)
(
	stype = text,
	sfunc = logging_sfunc_nonstrict,
	mstype = text,
	msfunc = logging_msfunc_nonstrict,
	minvfunc = logging_minvfunc_nonstrict,
	initcond = 'I',
	minitcond = 'MI'
)
================================


CREATE AGGREGATE logging_agg_strict (text)
(
	stype = text,
	sfunc = logging_sfunc_strict,
	mstype = text,
	msfunc = logging_msfunc_strict,
	minvfunc = logging_minvfunc_strict
)
================================


CREATE AGGREGATE logging_agg_strict_initcond (anyelement)
(
	stype = text,
	sfunc = logging_sfunc_strict,
	mstype = text,
	msfunc = logging_msfunc_strict,
	minvfunc = logging_minvfunc_strict,
	initcond = 'I',
	minitcond = 'MI'
)
================================


-- test strict and non-strict cases
SELECT
	p::text || ',' || i::text || ':' || COALESCE(v::text, 'NULL') AS row,
	logging_agg_nonstrict(v) over wnd as nstrict,
	logging_agg_nonstrict_initcond(v) over wnd as nstrict_init,
	logging_agg_strict(v::text) over wnd as strict,
	logging_agg_strict_initcond(v) over wnd as strict_init
FROM (VALUES
	(1, 1, NULL),
	(1, 2, 'a'),
	(1, 3, 'b'),
	(1, 4, NULL),
	(1, 5, NULL),
	(1, 6, 'c'),
	(2, 1, NULL),
	(2, 2, 'x'),
	(3, 1, 'z')
) AS t(p, i, v)
WINDOW wnd AS (PARTITION BY P ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)
ORDER BY p, i
================================


-- and again, but with filter
SELECT
	p::text || ',' || i::text || ':' ||
		CASE WHEN f THEN COALESCE(v::text, 'NULL') ELSE '-' END as row,
	logging_agg_nonstrict(v) filter(where f) over wnd as nstrict_filt,
	logging_agg_nonstrict_initcond(v) filter(where f) over wnd as nstrict_init_filt,
	logging_agg_strict(v::text) filter(where f) over wnd as strict_filt,
	logging_agg_strict_initcond(v) filter(where f) over wnd as strict_init_filt
FROM (VALUES
	(1, 1, true,  NULL),
	(1, 2, false, 'a'),
	(1, 3, true,  'b'),
	(1, 4, false, NULL),
	(1, 5, false, NULL),
	(1, 6, false, 'c'),
	(2, 1, false, NULL),
	(2, 2, true,  'x'),
	(3, 1, true,  'z')
) AS t(p, i, f, v)
WINDOW wnd AS (PARTITION BY p ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)
ORDER BY p, i
================================


-- test that volatile arguments disable moving-aggregate mode
SELECT
	i::text || ':' || COALESCE(v::text, 'NULL') as row,
	logging_agg_strict(v::text)
		over wnd as inverse,
	logging_agg_strict(v::text || CASE WHEN random() < 0 then '?' ELSE '' END)
		over wnd as noinverse
FROM (VALUES
	(1, 'a'),
	(2, 'b'),
	(3, 'c')
) AS t(i, v)
WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)
ORDER BY i
================================


SELECT
	i::text || ':' || COALESCE(v::text, 'NULL') as row,
	logging_agg_strict(v::text) filter(where true)
		over wnd as inverse,
	logging_agg_strict(v::text) filter(where random() >= 0)
		over wnd as noinverse
FROM (VALUES
	(1, 'a'),
	(2, 'b'),
	(3, 'c')
) AS t(i, v)
WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)
ORDER BY i
================================


CREATE AGGREGATE sum_int_randomrestart (int4)
(
	stype = int4,
	sfunc = int4pl,
	mstype = int4,
	msfunc = int4pl,
	minvfunc = sum_int_randrestart_minvfunc
)
================================


SELECT i,SUM(v::money) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)
  FROM (VALUES(1,'1.10'),(2,'2.20'),(3,NULL),(4,NULL)) t(i,v)
================================


================================


-- test the unary float4abs operator
SELECT f.f1, @f.f1 AS abs_f1 FROM FLOAT4_TBL f
================================


-- test output (and round-trip safety) of various values.
-- To ensure we're testing what we think we're testing, start with
-- float values specified by bit patterns (as a useful side effect,
-- this means we'll fail on non-IEEE platforms).

create type xfloat4
================================

create type xfloat4 (input = xfloat4in, output = xfloat4out, like = float4)
================================

create cast (xfloat4 as float4) without function
================================

create cast (float4 as xfloat4) without function
================================

create cast (xfloat4 as integer) without function
================================

create cast (integer as xfloat4) without function
================================


-- float4: seeeeeee emmmmmmm mmmmmmmm mmmmmmmm

-- we don't care to assume the platform's strtod() handles subnormals
-- correctly
================================
 those are "use at your own risk". However we do test
-- subnormal outputs, since those are under our control.

with testdata(bits) as (values
  -- small subnormals
  (x'00000001'),
  (x'00000002'), (x'00000003'),
  (x'00000010'), (x'00000011'), (x'00000100'), (x'00000101'),
  (x'00004000'), (x'00004001'), (x'00080000'), (x'00080001'),
  -- stress values
  (x'0053c4f4'),  -- 7693e-42
  (x'006c85c4'),  -- 996622e-44
  (x'0041ca76'),  -- 60419369e-46
  (x'004b7678'),  -- 6930161142e-48
  -- taken from upstream testsuite
  (x'00000007'),
  (x'00424fe2'),
  -- borderline between subnormal and normal
  (x'007ffff0'), (x'007ffff1'), (x'007ffffe'), (x'007fffff'))
select float4send(flt) as ibits,
       flt
  from (select bits::integer::xfloat4::float4 as flt
          from testdata
	offset 0) s
================================


-- clean up, lest opr_sanity complain
drop type xfloat4 cascade
================================


================================
\set HIDE_TOAST_COMPRESSION false

-- ensure we get stable results regardless of installation's default
SET default_toast_compression = 'pglz'
================================


-- test creating table with compression method
CREATE TABLE cmdata(f1 text COMPRESSION pglz)
================================

\d+ cmdata
CREATE TABLE cmdata1(f1 TEXT COMPRESSION lz4)
================================

\d+ cmdata1

-- verify stored compression method in the data
SELECT pg_column_compression(f1) FROM cmdata
================================

\d+ cmmove1
SELECT pg_column_compression(f1) FROM cmmove1
================================


-- copy to existing table
CREATE TABLE cmmove3(f1 text COMPRESSION pglz)
================================

INSERT INTO cmmove3 SELECT * FROM cmdata
================================

INSERT INTO cmmove3 SELECT * FROM cmdata1
================================


-- test LIKE INCLUDING COMPRESSION
CREATE TABLE cmdata2 (LIKE cmdata1 INCLUDING COMPRESSION)
================================

\d+ cmdata2
DROP TABLE cmdata2
================================


-- try setting compression for incompressible data type
CREATE TABLE cmdata2 (f1 int COMPRESSION pglz)
================================


-- update using datum from different table
CREATE TABLE cmmove2(f1 text COMPRESSION pglz)
================================

CREATE TABLE cmdata2 (f1 text COMPRESSION pglz)
================================

INSERT INTO cmdata2 SELECT large_val() || repeat('a', 4000)
================================

INSERT INTO cmdata1 SELECT large_val() || repeat('a', 4000)
================================

\d+ cmdata2
ALTER TABLE cmdata2 ALTER COLUMN f1 TYPE varchar
================================

\d+ cmdata2
ALTER TABLE cmdata2 ALTER COLUMN f1 TYPE int USING f1::integer
================================

\d+ cmdata2

--changing column storage should not impact the compression method
--but the data should not be compressed
ALTER TABLE cmdata2 ALTER COLUMN f1 TYPE varchar
================================

ALTER TABLE cmdata2 ALTER COLUMN f1 SET COMPRESSION pglz
================================

\d+ cmdata2
ALTER TABLE cmdata2 ALTER COLUMN f1 SET STORAGE plain
================================

\d+ cmdata2
INSERT INTO cmdata2 VALUES (repeat('123456789', 800))
================================


-- test compression with materialized view
CREATE MATERIALIZED VIEW compressmv(x) AS SELECT * FROM cmdata1
================================

\d+ compressmv
SELECT pg_column_compression(f1) FROM cmdata1
================================


-- test compression with partition
CREATE TABLE cmpart(f1 text COMPRESSION lz4) PARTITION BY HASH(f1)
================================

CREATE TABLE cmpart2(f1 text COMPRESSION pglz)
================================

CREATE TABLE cminh(f1 TEXT COMPRESSION lz4) INHERITS(cmdata)
================================


-- test alter compression method
ALTER TABLE cmdata ALTER COLUMN f1 SET COMPRESSION lz4
================================

\d+ cmdata
SELECT pg_column_compression(f1) FROM cmdata
================================


ALTER TABLE cmdata2 ALTER COLUMN f1 SET COMPRESSION default
================================

\d+ cmdata2

-- test alter compression method for materialized views
ALTER MATERIALIZED VIEW compressmv ALTER COLUMN x SET COMPRESSION lz4
================================

\d+ compressmv

-- test alter compression method for partitioned tables
ALTER TABLE cmpart1 ALTER COLUMN f1 SET COMPRESSION pglz
================================

ALTER TABLE cmpart2 ALTER COLUMN f1 SET COMPRESSION lz4
================================

CREATE TABLE cmdata2 (f1 TEXT COMPRESSION pglz, f2 TEXT COMPRESSION lz4)
================================


CREATE TABLE badcompresstbl (a text COMPRESSION I_Do_Not_Exist_Compression)
================================

ALTER TABLE badcompresstbl ALTER a SET COMPRESSION I_Do_Not_Exist_Compression
================================


\set HIDE_TOAST_COMPRESSION true

================================


CREATE SCHEMA alt_nsp1
================================

CREATE SCHEMA alt_nsp2
================================

CREATE AGGREGATE alt_agg1 (
  sfunc1 = int4pl, basetype = int4, stype1 = int4, initcond = 0
)
================================

CREATE AGGREGATE alt_agg2 (
  sfunc1 = int4mi, basetype = int4, stype1 = int4, initcond = 0
)
================================

ALTER AGGREGATE alt_func1(int) RENAME TO alt_func3
================================
  -- failed (not aggregate)
ALTER AGGREGATE alt_func1(int) OWNER TO regress_alter_generic_user3
================================
  -- failed (not aggregate)
ALTER AGGREGATE alt_func1(int) SET SCHEMA alt_nsp2
================================
  -- OK

ALTER AGGREGATE alt_agg1(int) RENAME TO alt_agg2
================================
   -- failed (name conflict)
ALTER AGGREGATE alt_agg1(int) RENAME TO alt_agg3
================================
   -- OK
ALTER AGGREGATE alt_agg2(int) OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER AGGREGATE alt_agg2(int) OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER AGGREGATE alt_agg2(int) SET SCHEMA alt_nsp2
================================

CREATE AGGREGATE alt_agg1 (
  sfunc1 = int4pl, basetype = int4, stype1 = int4, initcond = 100
)
================================

CREATE AGGREGATE alt_agg2 (
  sfunc1 = int4mi, basetype = int4, stype1 = int4, initcond = -100
)
================================
	-- failed (name conflicts)

ALTER AGGREGATE alt_agg3(int) RENAME TO alt_agg4
================================
   -- failed (not owner)
ALTER AGGREGATE alt_agg1(int) RENAME TO alt_agg4
================================
   -- OK
ALTER AGGREGATE alt_agg3(int) OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER AGGREGATE alt_agg2(int) OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER AGGREGATE alt_agg3(int) SET SCHEMA alt_nsp2
================================
  -- failed (not owner)
ALTER AGGREGATE alt_agg2(int) SET SCHEMA alt_nsp2
================================

CREATE CONVERSION alt_conv1 FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

CREATE CONVERSION alt_conv2 FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================


ALTER CONVERSION alt_conv1 RENAME TO alt_conv2
================================
  -- failed (name conflict)
ALTER CONVERSION alt_conv1 RENAME TO alt_conv3
================================
  -- OK
ALTER CONVERSION alt_conv2 OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER CONVERSION alt_conv2 OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER CONVERSION alt_conv2 SET SCHEMA alt_nsp2
================================

CREATE CONVERSION alt_conv1 FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

CREATE CONVERSION alt_conv2 FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================


ALTER CONVERSION alt_conv3 RENAME TO alt_conv4
================================
  -- failed (not owner)
ALTER CONVERSION alt_conv1 RENAME TO alt_conv4
================================
  -- OK
ALTER CONVERSION alt_conv3 OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER CONVERSION alt_conv2 OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER CONVERSION alt_conv3 SET SCHEMA alt_nsp2
================================
  -- failed (not owner)
ALTER CONVERSION alt_conv2 SET SCHEMA alt_nsp2
================================


--
-- Foreign Data Wrapper and Foreign Server
--
CREATE FOREIGN DATA WRAPPER alt_fdw1
================================

CREATE FOREIGN DATA WRAPPER alt_fdw2
================================


CREATE SERVER alt_fserv1 FOREIGN DATA WRAPPER alt_fdw1
================================

CREATE SERVER alt_fserv2 FOREIGN DATA WRAPPER alt_fdw2
================================


ALTER FOREIGN DATA WRAPPER alt_fdw1 RENAME TO alt_fdw2
================================
  -- failed (name conflict)
ALTER FOREIGN DATA WRAPPER alt_fdw1 RENAME TO alt_fdw3
================================
  -- OK

ALTER SERVER alt_fserv1 RENAME TO alt_fserv2
================================
   -- failed (name conflict)
ALTER SERVER alt_fserv1 RENAME TO alt_fserv3
================================


--
-- Procedural Language
--
CREATE LANGUAGE alt_lang1 HANDLER plpgsql_call_handler
================================

CREATE LANGUAGE alt_lang2 HANDLER plpgsql_call_handler
================================


ALTER LANGUAGE alt_lang1 OWNER TO regress_alter_generic_user1
================================
  -- OK
ALTER LANGUAGE alt_lang2 OWNER TO regress_alter_generic_user2
================================

ALTER LANGUAGE alt_lang1 RENAME TO alt_lang2
================================
   -- failed (name conflict)
ALTER LANGUAGE alt_lang2 RENAME TO alt_lang3
================================
   -- failed (not owner)
ALTER LANGUAGE alt_lang1 RENAME TO alt_lang3
================================
   -- OK

ALTER LANGUAGE alt_lang2 OWNER TO regress_alter_generic_user3
================================
  -- failed (not owner)
ALTER LANGUAGE alt_lang3 OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER LANGUAGE alt_lang3 OWNER TO regress_alter_generic_user3
================================


CREATE OPERATOR @-@ ( leftarg = int4, rightarg = int4, procedure = int4mi )
================================

CREATE OPERATOR @+@ ( leftarg = int4, rightarg = int4, procedure = int4pl )
================================


ALTER OPERATOR @+@(int4, int4) OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER OPERATOR @+@(int4, int4) OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER OPERATOR @-@(int4, int4) SET SCHEMA alt_nsp2
================================


CREATE OPERATOR @-@ ( leftarg = int4, rightarg = int4, procedure = int4mi )
================================


ALTER OPERATOR @+@(int4, int4) OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER OPERATOR @-@(int4, int4) OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER OPERATOR @+@(int4, int4) SET SCHEMA alt_nsp2
================================
   -- failed (not owner)
-- can't test this: the error message includes the raw oid of namespace
-- ALTER OPERATOR @-@(int4, int4) SET SCHEMA alt_nsp2
================================


--
-- OpFamily and OpClass
--
CREATE OPERATOR FAMILY alt_opf1 USING hash
================================

CREATE OPERATOR FAMILY alt_opf2 USING hash
================================

ALTER OPERATOR FAMILY alt_opf1 USING hash OWNER TO regress_alter_generic_user1
================================

ALTER OPERATOR FAMILY alt_opf2 USING hash OWNER TO regress_alter_generic_user1
================================


CREATE OPERATOR CLASS alt_opc1 FOR TYPE uuid USING hash AS STORAGE uuid
================================

CREATE OPERATOR CLASS alt_opc2 FOR TYPE uuid USING hash AS STORAGE uuid
================================

ALTER OPERATOR CLASS alt_opc1 USING hash OWNER TO regress_alter_generic_user1
================================

ALTER OPERATOR CLASS alt_opc2 USING hash OWNER TO regress_alter_generic_user1
================================


ALTER OPERATOR FAMILY alt_opf1 USING hash RENAME TO alt_opf2
================================
  -- failed (name conflict)
ALTER OPERATOR FAMILY alt_opf1 USING hash RENAME TO alt_opf3
================================
  -- OK
ALTER OPERATOR FAMILY alt_opf2 USING hash OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER OPERATOR FAMILY alt_opf2 USING hash OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER OPERATOR FAMILY alt_opf2 USING hash SET SCHEMA alt_nsp2
================================
  -- OK

ALTER OPERATOR CLASS alt_opc1 USING hash RENAME TO alt_opc2
================================
  -- failed (name conflict)
ALTER OPERATOR CLASS alt_opc1 USING hash RENAME TO alt_opc3
================================
  -- OK
ALTER OPERATOR CLASS alt_opc2 USING hash OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER OPERATOR CLASS alt_opc2 USING hash OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER OPERATOR CLASS alt_opc2 USING hash SET SCHEMA alt_nsp2
================================


CREATE OPERATOR FAMILY alt_opf1 USING hash
================================

CREATE OPERATOR FAMILY alt_opf2 USING hash
================================

ALTER OPERATOR FAMILY alt_opf1 USING hash OWNER TO regress_alter_generic_user2
================================

ALTER OPERATOR FAMILY alt_opf2 USING hash OWNER TO regress_alter_generic_user2
================================


CREATE OPERATOR CLASS alt_opc1 FOR TYPE macaddr USING hash AS STORAGE macaddr
================================

CREATE OPERATOR CLASS alt_opc2 FOR TYPE macaddr USING hash AS STORAGE macaddr
================================

ALTER OPERATOR CLASS alt_opc1 USING hash OWNER TO regress_alter_generic_user2
================================

ALTER OPERATOR CLASS alt_opc2 USING hash OWNER TO regress_alter_generic_user2
================================


ALTER OPERATOR FAMILY alt_opf3 USING hash RENAME TO alt_opf4
================================
	-- failed (not owner)
ALTER OPERATOR FAMILY alt_opf1 USING hash RENAME TO alt_opf4
================================
  -- OK
ALTER OPERATOR FAMILY alt_opf3 USING hash OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER OPERATOR FAMILY alt_opf2 USING hash OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER OPERATOR FAMILY alt_opf3 USING hash SET SCHEMA alt_nsp2
================================
  -- failed (not owner)
ALTER OPERATOR FAMILY alt_opf2 USING hash SET SCHEMA alt_nsp2
================================
  -- failed (name conflict)

ALTER OPERATOR CLASS alt_opc3 USING hash RENAME TO alt_opc4
================================
	-- failed (not owner)
ALTER OPERATOR CLASS alt_opc1 USING hash RENAME TO alt_opc4
================================
  -- OK
ALTER OPERATOR CLASS alt_opc3 USING hash OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER OPERATOR CLASS alt_opc2 USING hash OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER OPERATOR CLASS alt_opc3 USING hash SET SCHEMA alt_nsp2
================================
  -- failed (not owner)
ALTER OPERATOR CLASS alt_opc2 USING hash SET SCHEMA alt_nsp2
================================

CREATE OPERATOR FAMILY alt_opf4 USING btree
================================

ALTER OPERATOR FAMILY alt_opf4 USING btree ADD
  -- int4 vs int2
  OPERATOR 1 < (int4, int2) ,
  OPERATOR 2 <= (int4, int2) ,
  OPERATOR 3 = (int4, int2) ,
  OPERATOR 4 >= (int4, int2) ,
  OPERATOR 5 > (int4, int2) ,
  FUNCTION 1 btint42cmp(int4, int2)
================================


ALTER OPERATOR FAMILY alt_opf4 USING btree DROP
  -- int4 vs int2
  OPERATOR 1 (int4, int2) ,
  OPERATOR 2 (int4, int2) ,
  OPERATOR 3 (int4, int2) ,
  OPERATOR 4 (int4, int2) ,
  OPERATOR 5 (int4, int2) ,
  FUNCTION 1 (int4, int2) 
================================

DROP OPERATOR FAMILY alt_opf4 USING btree
================================


-- Should fail. Invalid values for ALTER OPERATOR FAMILY .. ADD / DROP
CREATE OPERATOR FAMILY alt_opf4 USING btree
================================

ALTER OPERATOR FAMILY alt_opf4 USING invalid_index_method ADD  OPERATOR 1 < (int4, int2)
================================
 -- invalid indexing_method
ALTER OPERATOR FAMILY alt_opf4 USING btree ADD OPERATOR 6 < (int4, int2)
================================
 -- operator number should be between 1 and 5
ALTER OPERATOR FAMILY alt_opf4 USING btree ADD OPERATOR 0 < (int4, int2)
================================
 -- operator number should be between 1 and 5
ALTER OPERATOR FAMILY alt_opf4 USING btree ADD OPERATOR 1 < 
================================
 -- operator without argument types
ALTER OPERATOR FAMILY alt_opf4 USING btree ADD FUNCTION 0 btint42cmp(int4, int2)
================================
 -- invalid options parsing function
ALTER OPERATOR FAMILY alt_opf4 USING btree ADD FUNCTION 6 btint42cmp(int4, int2)
================================
 -- function number should be between 1 and 5
ALTER OPERATOR FAMILY alt_opf4 USING btree ADD STORAGE invalid_storage
================================
 -- Ensure STORAGE is not a part of ALTER OPERATOR FAMILY
DROP OPERATOR FAMILY alt_opf4 USING btree
================================

CREATE OPERATOR FAMILY alt_opf5 USING btree
================================

ALTER OPERATOR FAMILY alt_opf5 USING btree ADD OPERATOR 1 < (int4, int2), FUNCTION 1 btint42cmp(int4, int2)
================================

DROP OPERATOR FAMILY alt_opf5 USING btree
================================

CREATE SCHEMA alt_nsp6
================================

CREATE OPERATOR FAMILY alt_nsp6.alt_opf6 USING btree
================================

ALTER OPERATOR FAMILY alt_nsp6.alt_opf6 USING btree ADD OPERATOR 1 < (int4, int2)
================================


-- Should fail. Only two arguments required for ALTER OPERATOR FAMILY ... DROP OPERATOR
CREATE OPERATOR FAMILY alt_opf7 USING btree
================================

ALTER OPERATOR FAMILY alt_opf7 USING btree ADD OPERATOR 1 < (int4, int2)
================================

ALTER OPERATOR FAMILY alt_opf7 USING btree DROP OPERATOR 1 (int4, int2, int8)
================================

DROP OPERATOR FAMILY alt_opf7 USING btree
================================


-- Should work. During ALTER OPERATOR FAMILY ... DROP OPERATOR
-- when left type is the same as right type, a DROP with only one argument type should work
CREATE OPERATOR FAMILY alt_opf8 USING btree
================================

ALTER OPERATOR FAMILY alt_opf8 USING btree ADD OPERATOR 1 < (int4, int4)
================================

DROP OPERATOR FAMILY alt_opf8 USING btree
================================


-- Should work. Textbook case of ALTER OPERATOR FAMILY ... ADD OPERATOR with FOR ORDER BY
CREATE OPERATOR FAMILY alt_opf9 USING gist
================================

ALTER OPERATOR FAMILY alt_opf9 USING gist ADD OPERATOR 1 < (int4, int4) FOR ORDER BY float_ops
================================

DROP OPERATOR FAMILY alt_opf9 USING gist
================================


-- Should fail. Ensure correct ordering methods in ALTER OPERATOR FAMILY ... ADD OPERATOR .. FOR ORDER BY
CREATE OPERATOR FAMILY alt_opf10 USING btree
================================

ALTER OPERATOR FAMILY alt_opf10 USING btree ADD OPERATOR 1 < (int4, int4) FOR ORDER BY float_ops
================================

DROP OPERATOR FAMILY alt_opf10 USING btree
================================


-- Should work. Textbook case of ALTER OPERATOR FAMILY ... ADD OPERATOR with FOR ORDER BY
CREATE OPERATOR FAMILY alt_opf11 USING gist
================================

ALTER OPERATOR FAMILY alt_opf11 USING gist ADD OPERATOR 1 < (int4, int4) FOR ORDER BY float_ops
================================

ALTER OPERATOR FAMILY alt_opf11 USING gist DROP OPERATOR 1 (int4, int4)
================================

DROP OPERATOR FAMILY alt_opf11 USING gist
================================

CREATE OPERATOR FAMILY alt_opf12 USING btree
================================

CREATE FUNCTION fn_opf12  (int4, int2) RETURNS BIGINT AS 'SELECT NULL::BIGINT
================================
' LANGUAGE SQL
================================

ALTER OPERATOR FAMILY alt_opf12 USING btree ADD FUNCTION 1 fn_opf12(int4, int2)
================================

DROP OPERATOR FAMILY alt_opf12 USING btree
================================

CREATE OPERATOR FAMILY alt_opf13 USING hash
================================

CREATE FUNCTION fn_opf13  (int4) RETURNS BIGINT AS 'SELECT NULL::BIGINT
================================
' LANGUAGE SQL
================================

ALTER OPERATOR FAMILY alt_opf13 USING hash ADD FUNCTION 1 fn_opf13(int4)
================================

DROP OPERATOR FAMILY alt_opf13 USING hash
================================

CREATE OPERATOR FAMILY alt_opf14 USING btree
================================

CREATE FUNCTION fn_opf14 (int4) RETURNS BIGINT AS 'SELECT NULL::BIGINT
================================
' LANGUAGE SQL
================================

ALTER OPERATOR FAMILY alt_opf14 USING btree ADD FUNCTION 1 fn_opf14(int4)
================================

DROP OPERATOR FAMILY alt_opf14 USING btree
================================

CREATE OPERATOR FAMILY alt_opf15 USING hash
================================

CREATE FUNCTION fn_opf15 (int4, int2) RETURNS BIGINT AS 'SELECT NULL::BIGINT
================================
' LANGUAGE SQL
================================

ALTER OPERATOR FAMILY alt_opf15 USING hash ADD FUNCTION 1 fn_opf15(int4, int2)
================================

DROP OPERATOR FAMILY alt_opf15 USING hash
================================


-- Should fail. In gist throw an error when giving different data types for function argument
-- without defining left / right type in ALTER OPERATOR FAMILY ... ADD FUNCTION
CREATE OPERATOR FAMILY alt_opf16 USING gist
================================

ALTER OPERATOR FAMILY alt_opf16 USING gist ADD FUNCTION 1 btint42cmp(int4, int2)
================================

DROP OPERATOR FAMILY alt_opf16 USING gist
================================


-- Should fail. duplicate operator number / function number in ALTER OPERATOR FAMILY ... ADD FUNCTION
CREATE OPERATOR FAMILY alt_opf17 USING btree
================================

ALTER OPERATOR FAMILY alt_opf17 USING btree ADD OPERATOR 1 < (int4, int4), OPERATOR 1 < (int4, int4)
================================
 -- operator # appears twice in same statement
ALTER OPERATOR FAMILY alt_opf17 USING btree ADD OPERATOR 1 < (int4, int4)
================================
 -- operator 1 requested first-time
ALTER OPERATOR FAMILY alt_opf17 USING btree ADD OPERATOR 1 < (int4, int4)
================================
 -- operator 1 requested again in separate statement
ALTER OPERATOR FAMILY alt_opf17 USING btree ADD
  OPERATOR 1 < (int4, int2) ,
  OPERATOR 2 <= (int4, int2) ,
  OPERATOR 3 = (int4, int2) ,
  OPERATOR 4 >= (int4, int2) ,
  OPERATOR 5 > (int4, int2) ,
  FUNCTION 1 btint42cmp(int4, int2) ,
  FUNCTION 1 btint42cmp(int4, int2)
================================
    -- procedure 1 appears twice in same statement
ALTER OPERATOR FAMILY alt_opf17 USING btree ADD
  OPERATOR 1 < (int4, int2) ,
  OPERATOR 2 <= (int4, int2) ,
  OPERATOR 3 = (int4, int2) ,
  OPERATOR 4 >= (int4, int2) ,
  OPERATOR 5 > (int4, int2) ,
  FUNCTION 1 btint42cmp(int4, int2)
================================
    -- procedure 1 appears first time
ALTER OPERATOR FAMILY alt_opf17 USING btree ADD
  OPERATOR 1 < (int4, int2) ,
  OPERATOR 2 <= (int4, int2) ,
  OPERATOR 3 = (int4, int2) ,
  OPERATOR 4 >= (int4, int2) ,
  OPERATOR 5 > (int4, int2) ,
  FUNCTION 1 btint42cmp(int4, int2)
================================
    -- procedure 1 requested again in separate statement
DROP OPERATOR FAMILY alt_opf17 USING btree
================================



-- Should fail. Ensure that DROP requests for missing OPERATOR / FUNCTIONS
-- return appropriate message in ALTER OPERATOR FAMILY ... DROP OPERATOR / FUNCTION
CREATE OPERATOR FAMILY alt_opf18 USING btree
================================

ALTER OPERATOR FAMILY alt_opf18 USING btree DROP OPERATOR 1 (int4, int4)
================================

ALTER OPERATOR FAMILY alt_opf18 USING btree ADD
  OPERATOR 1 < (int4, int2) ,
  OPERATOR 2 <= (int4, int2) ,
  OPERATOR 3 = (int4, int2) ,
  OPERATOR 4 >= (int4, int2) ,
  OPERATOR 5 > (int4, int2) ,
  FUNCTION 1 btint42cmp(int4, int2)
================================

-- Should fail. Not allowed to have cross-type equalimage function.
ALTER OPERATOR FAMILY alt_opf18 USING btree
  ADD FUNCTION 4 (int4, int2) btequalimage(oid)
================================

ALTER OPERATOR FAMILY alt_opf18 USING btree DROP FUNCTION 2 (int4, int4)
================================

DROP OPERATOR FAMILY alt_opf18 USING btree
================================


-- Should fail. Invalid opclass options function (#5) specifications.
CREATE OPERATOR FAMILY alt_opf19 USING btree
================================

ALTER OPERATOR FAMILY alt_opf19 USING btree ADD FUNCTION 5 test_opclass_options_func(internal, text[], bool)
================================

ALTER OPERATOR FAMILY alt_opf19 USING btree ADD FUNCTION 5 (int4) btint42cmp(int4, int2)
================================

ALTER OPERATOR FAMILY alt_opf19 USING btree ADD FUNCTION 5 (int4, int2) btint42cmp(int4, int2)
================================

ALTER OPERATOR FAMILY alt_opf19 USING btree ADD FUNCTION 5 (int4) test_opclass_options_func(internal)
================================
 -- Ok
ALTER OPERATOR FAMILY alt_opf19 USING btree DROP FUNCTION 5 (int4, int4)
================================

DROP OPERATOR FAMILY alt_opf19 USING btree
================================

CREATE STATISTICS alt_stat1 ON a, b FROM alt_regress_1
================================

CREATE STATISTICS alt_stat2 ON a, b FROM alt_regress_1
================================


ALTER STATISTICS alt_stat1 RENAME TO alt_stat2
================================
   -- failed (name conflict)
ALTER STATISTICS alt_stat1 RENAME TO alt_stat3
================================
   -- OK
ALTER STATISTICS alt_stat2 OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER STATISTICS alt_stat2 OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER STATISTICS alt_stat2 SET SCHEMA alt_nsp2
================================

CREATE STATISTICS alt_stat1 ON a, b FROM alt_regress_2
================================

CREATE STATISTICS alt_stat2 ON a, b FROM alt_regress_2
================================


ALTER STATISTICS alt_stat3 RENAME TO alt_stat4
================================
    -- failed (not owner)
ALTER STATISTICS alt_stat1 RENAME TO alt_stat4
================================
    -- OK
ALTER STATISTICS alt_stat3 OWNER TO regress_alter_generic_user2
================================
 -- failed (not owner)
ALTER STATISTICS alt_stat2 OWNER TO regress_alter_generic_user3
================================
 -- failed (no role membership)
ALTER STATISTICS alt_stat3 SET SCHEMA alt_nsp2
================================
		-- failed (not owner)
ALTER STATISTICS alt_stat2 SET SCHEMA alt_nsp2
================================

CREATE TEXT SEARCH DICTIONARY alt_ts_dict1 (template=simple)
================================

CREATE TEXT SEARCH DICTIONARY alt_ts_dict2 (template=simple)
================================


ALTER TEXT SEARCH DICTIONARY alt_ts_dict1 RENAME TO alt_ts_dict2
================================
  -- failed (name conflict)
ALTER TEXT SEARCH DICTIONARY alt_ts_dict1 RENAME TO alt_ts_dict3
================================
  -- OK
ALTER TEXT SEARCH DICTIONARY alt_ts_dict2 OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER TEXT SEARCH DICTIONARY alt_ts_dict2 OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER TEXT SEARCH DICTIONARY alt_ts_dict2 SET SCHEMA alt_nsp2
================================

CREATE TEXT SEARCH DICTIONARY alt_ts_dict1 (template=simple)
================================

CREATE TEXT SEARCH DICTIONARY alt_ts_dict2 (template=simple)
================================


ALTER TEXT SEARCH DICTIONARY alt_ts_dict3 RENAME TO alt_ts_dict4
================================
  -- failed (not owner)
ALTER TEXT SEARCH DICTIONARY alt_ts_dict1 RENAME TO alt_ts_dict4
================================
  -- OK
ALTER TEXT SEARCH DICTIONARY alt_ts_dict3 OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER TEXT SEARCH DICTIONARY alt_ts_dict2 OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER TEXT SEARCH DICTIONARY alt_ts_dict3 SET SCHEMA alt_nsp2
================================
  -- failed (not owner)
ALTER TEXT SEARCH DICTIONARY alt_ts_dict2 SET SCHEMA alt_nsp2
================================

CREATE TEXT SEARCH CONFIGURATION alt_ts_conf1 (copy=english)
================================

CREATE TEXT SEARCH CONFIGURATION alt_ts_conf2 (copy=english)
================================


ALTER TEXT SEARCH CONFIGURATION alt_ts_conf1 RENAME TO alt_ts_conf2
================================
  -- failed (name conflict)
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf1 RENAME TO alt_ts_conf3
================================
  -- OK
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf2 OWNER TO regress_alter_generic_user2
================================
  -- failed (no role membership)
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf2 OWNER TO regress_alter_generic_user3
================================
  -- OK
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf2 SET SCHEMA alt_nsp2
================================

CREATE TEXT SEARCH CONFIGURATION alt_ts_conf1 (copy=english)
================================

CREATE TEXT SEARCH CONFIGURATION alt_ts_conf2 (copy=english)
================================


ALTER TEXT SEARCH CONFIGURATION alt_ts_conf3 RENAME TO alt_ts_conf4
================================
  -- failed (not owner)
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf1 RENAME TO alt_ts_conf4
================================
  -- OK
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf3 OWNER TO regress_alter_generic_user2
================================
  -- failed (not owner)
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf2 OWNER TO regress_alter_generic_user3
================================
  -- failed (no role membership)
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf3 SET SCHEMA alt_nsp2
================================
  -- failed (not owner)
ALTER TEXT SEARCH CONFIGURATION alt_ts_conf2 SET SCHEMA alt_nsp2
================================


--
-- Text Search Template
--
CREATE TEXT SEARCH TEMPLATE alt_ts_temp1 (lexize=dsimple_lexize)
================================

CREATE TEXT SEARCH TEMPLATE alt_ts_temp2 (lexize=dsimple_lexize)
================================


ALTER TEXT SEARCH TEMPLATE alt_ts_temp1 RENAME TO alt_ts_temp2
================================
 -- failed (name conflict)
ALTER TEXT SEARCH TEMPLATE alt_ts_temp1 RENAME TO alt_ts_temp3
================================
 -- OK
ALTER TEXT SEARCH TEMPLATE alt_ts_temp2 SET SCHEMA alt_nsp2
================================
    -- OK

CREATE TEXT SEARCH TEMPLATE alt_ts_temp2 (lexize=dsimple_lexize)
================================

ALTER TEXT SEARCH TEMPLATE alt_ts_temp2 SET SCHEMA alt_nsp2
================================
    -- failed (name conflict)

-- invalid: non-lowercase quoted identifiers
CREATE TEXT SEARCH TEMPLATE tstemp_case ("Init" = init_function)
================================


--
-- Text Search Parser
--

CREATE TEXT SEARCH PARSER alt_ts_prs1
    (start = prsd_start, gettoken = prsd_nexttoken, end = prsd_end, lextypes = prsd_lextype)
================================

CREATE TEXT SEARCH PARSER alt_ts_prs2
    (start = prsd_start, gettoken = prsd_nexttoken, end = prsd_end, lextypes = prsd_lextype)
================================


ALTER TEXT SEARCH PARSER alt_ts_prs1 RENAME TO alt_ts_prs2
================================
 -- failed (name conflict)
ALTER TEXT SEARCH PARSER alt_ts_prs1 RENAME TO alt_ts_prs3
================================
 -- OK
ALTER TEXT SEARCH PARSER alt_ts_prs2 SET SCHEMA alt_nsp2
================================
   -- OK

CREATE TEXT SEARCH PARSER alt_ts_prs2
    (start = prsd_start, gettoken = prsd_nexttoken, end = prsd_end, lextypes = prsd_lextype)
================================

ALTER TEXT SEARCH PARSER alt_ts_prs2 SET SCHEMA alt_nsp2
================================
   -- failed (name conflict)

-- invalid: non-lowercase quoted identifiers
CREATE TEXT SEARCH PARSER tspars_case ("Start" = start_function)
================================


---
--- Cleanup resources
---
DROP FOREIGN DATA WRAPPER alt_fdw2 CASCADE
================================

DROP FOREIGN DATA WRAPPER alt_fdw3 CASCADE
================================


DROP LANGUAGE alt_lang2 CASCADE
================================

DROP LANGUAGE alt_lang3 CASCADE
================================


DROP SCHEMA alt_nsp1 CASCADE
================================

DROP SCHEMA alt_nsp2 CASCADE
================================


================================

--
-- This should fail
--
copy (select * from test1) from stdin
================================

--
-- This should fail
--
copy (select * from test1) (t,id) to stdout
================================

--
-- Test psql builtins, plain table
--
\copy test1 to stdout
--
-- This should fail
--
\copy v_test1 to stdout
--
-- Test \copy (select ...)
--
\copy (select "id",'id','id""'||t,(id + 1)*id,t,"test1"."t" from test1 where id=3) to stdout
--
-- Drop everything
--
drop table test2
================================


-- psql handling of COPY in multi-command strings
copy (select 1) to stdout\
================================
	-- row, then error
select 1/0\
================================
 -- error only
copy (select 1) to stdout\
================================
 copy (select 2) to stdout\
================================
 select 0\
================================

select 0\
================================
 copy test3 from stdin\
================================
 copy test3 from stdin\
================================
 -- 1
1
\.
2
\.
select * from test3
================================


================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val + t2.val
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val + t2.val, 10)
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val - t2.val
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val - t2.val, 40)
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val * t2.val
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val * t2.val, 30)
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val / t2.val
    FROM num_data t1, num_data t2
    WHERE t2.val != '0.0'
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val / t2.val, 80)
    FROM num_data t1, num_data t2
    WHERE t2.val != '0.0'
================================

INSERT INTO num_result SELECT id, 0, SQRT(ABS(val))
    FROM num_data
================================

INSERT INTO num_result SELECT id, 0, LN(ABS(val))
    FROM num_data
    WHERE val != '0.0'
================================

INSERT INTO num_result SELECT id, 0, LOG(numeric '10', ABS(val))
    FROM num_data
    WHERE val != '0.0'
================================

INSERT INTO num_result SELECT id, 0, POWER(numeric '10', LN(ABS(round(val,200))))
    FROM num_data
    WHERE val != '0.0'
================================


WITH v(x) AS
  (VALUES('0'::numeric),('1'),('-1'),('4.2'),('inf'),('-inf'),('nan'))
SELECT x1, x2,
  x1 / x2 AS quot,
  x1 % x2 AS mod,
  div(x1, x2) AS div
FROM v AS v1(x1), v AS v2(x2) WHERE x2 != 0
================================

-5.2
-0.0000000001
0.000000000001
1
1.99999999999999
2
2.00000000000001
3
4
4.5
5
5.5
6
7
8
9
9.99999999999999
10
10.0000000000001
\.

UPDATE width_bucket_test SET operand_f8 = operand_num::float8
================================


WITH v(val) AS
  (VALUES('0'::numeric),('-4.2'),('4.2e9'),('1.2e-5'),('inf'),('-inf'),('nan'))
SELECT val,
  to_char(val, '9.999EEEE') as numeric,
  to_char(val::float8, '9.999EEEE') as float8,
  to_char(val::float4, '9.999EEEE') as float4
FROM v
================================


WITH v(exp) AS
  (VALUES(-16379),(-16378),(-1234),(-789),(-45),(-5),(-4),(-3),(-2),(-1),(0),
         (1),(2),(3),(4),(5),(38),(275),(2345),(45678),(131070),(131071))
SELECT exp,
  to_char(('1.2345e'||exp)::numeric, '9.999EEEE') as numeric
FROM v
================================


WITH v(val) AS
  (VALUES('0'::numeric),('-4.2'),('4.2e9'),('1.2e-5'),('inf'),('-inf'),('nan'))
SELECT val,
  to_char(val, 'MI9999999999.99') as numeric,
  to_char(val::float8, 'MI9999999999.99') as float8,
  to_char(val::float4, 'MI9999999999.99') as float4
FROM v
================================


WITH v(val) AS
  (VALUES('0'::numeric),('-4.2'),('4.2e9'),('1.2e-5'),('inf'),('-inf'),('nan'))
SELECT val,
  to_char(val, 'MI99.99') as numeric,
  to_char(val::float8, 'MI99.99') as float8,
  to_char(val::float4, 'MI99.99') as float4
FROM v
================================

\d num_typemod_test

-- rounding of valid inputs
INSERT INTO num_typemod_test VALUES (123456, 123, 0.123, 0.000123, 0.000000123)
================================

select div(-9999999999999999999999::numeric,1000000000000000000000)*1000000000000000000000 + mod(-9999999999999999999999::numeric,1000000000000000000000)
================================


--
-- Tests for scale()
--

select scale(numeric 'NaN')
================================

select scale(numeric 'inf')
================================


--
-- Tests for min_scale()
--

select min_scale(numeric 'NaN') is NULL
================================
 -- should be true
select min_scale(numeric 'inf') is NULL
================================
                 -- very big number

--
-- Tests for trim_scale()
--

select trim_scale(numeric 'NaN')
================================

select trim_scale(numeric 'inf')
================================

INSERT INTO num_variance SELECT 9e131071 + x FROM generate_series(1, 5) x
================================


--
-- Tests for GCD()
--
SELECT a, b, gcd(a, b), gcd(a, -b), gcd(-b, a), gcd(-b, -a)
FROM (VALUES (0::numeric, 0::numeric),
             (0::numeric, numeric 'NaN'),
             (0::numeric, 46375::numeric),
             (433125::numeric, 46375::numeric),
             (43312.5::numeric, 4637.5::numeric),
             (4331.250::numeric, 463.75000::numeric),
             ('inf', '0'),
             ('inf', '42'),
             ('inf', 'inf')
     ) AS v(a, b)
================================


--
-- Tests for LCM()
--
SELECT a,b, lcm(a, b), lcm(a, -b), lcm(-b, a), lcm(-b, -a)
FROM (VALUES (0::numeric, 0::numeric),
             (0::numeric, numeric 'NaN'),
             (0::numeric, 13272::numeric),
             (13272::numeric, 13272::numeric),
             (423282::numeric, 13272::numeric),
             (42328.2::numeric, 1327.2::numeric),
             (4232.820::numeric, 132.72000::numeric),
             ('inf', '0'),
             ('inf', '42'),
             ('inf', 'inf')
     ) AS v(a, b)
================================


================================


CREATE GROUP regress_priv_group1
================================

CREATE GROUP regress_priv_group2 WITH USER regress_priv_user1, regress_priv_user2
================================


ALTER GROUP regress_priv_group1 ADD USER regress_priv_user4
================================


ALTER GROUP regress_priv_group2 ADD USER regress_priv_user2
================================
	-- duplicate
ALTER GROUP regress_priv_group2 DROP USER regress_priv_user2
================================

LOCK atest1 IN ACCESS EXCLUSIVE MODE
================================
 -- fail
INSERT INTO atest1 SELECT 1, b FROM atest1
================================

LOCK atest2 IN ACCESS EXCLUSIVE MODE
================================
 -- fail
INSERT INTO atest1 SELECT 1, b FROM atest1
================================
 -- fails
================================
 requires SELECT on atest2
UPDATE atest2 SET col2 = true FROM atest1 WHERE atest1.a = 5
================================

LOCK atest2 IN ACCESS EXCLUSIVE MODE
================================
 -- ok
bar	true
\.
SELECT * FROM atest1
================================


CREATE TABLE atest12 as
  SELECT x AS a, 10001 - x AS b FROM generate_series(1,10000) x
================================


CREATE OPERATOR <<< (procedure = leak, leftarg = integer, rightarg = integer,
                     restrict = scalarltsel)
================================


-- views with leaky operator
CREATE VIEW atest12v AS
  SELECT * FROM atest12 WHERE b <<< 5
================================

CREATE VIEW atest12sbv WITH (security_barrier=true) AS
  SELECT * FROM atest12 WHERE b <<< 5
================================


-- And this one.
EXPLAIN (COSTS OFF) SELECT * FROM atest12 x, atest12 y
  WHERE x.a = y.b and abs(y.a) <<< 5
================================
 return $1 > $2
================================
 end$$
  LANGUAGE plpgsql immutable
================================

CREATE OPERATOR >>> (procedure = leak2, leftarg = integer, rightarg = integer,
                     restrict = scalargtsel)
================================


-- This should not show any "leak" notices before failing.
EXPLAIN (COSTS OFF) SELECT * FROM atest12 WHERE a >>> 0
================================


-- A non-security barrier view does not guard against information leakage.
EXPLAIN (COSTS OFF) SELECT * FROM atest12v x, atest12v y
  WHERE x.a = y.b and abs(y.a) <<< 5
================================


-- But a security barrier view isolates the leaky operator.
EXPLAIN (COSTS OFF) SELECT * FROM atest12sbv x, atest12sbv y
  WHERE x.a = y.b and abs(y.a) <<< 5
================================


-- But not for this, due to lack of table-wide permissions needed
-- to make use of the expression index's statistics.
EXPLAIN (COSTS OFF) SELECT * FROM atest12 x, atest12 y
  WHERE x.a = y.b and abs(y.a) <<< 5
================================
 -- fail
SELECT (j.*) IS NULL FROM (atest5 a JOIN atest5 b USING (one)) j
================================
 -- fail
SELECT (a.*) IS NULL FROM (atest5 a JOIN atest5 b USING (one))
================================
 -- ok
1
\.
INSERT INTO atest5 (three) VALUES (4)
================================

LOCK atestp1
================================

LOCK atestc
================================


-- privileges on functions, languages

-- switch to superuser
\c -

REVOKE ALL PRIVILEGES ON LANGUAGE sql FROM PUBLIC
================================
 -- fail
CREATE FUNCTION priv_testfunc1(int) RETURNS int AS 'select 2 * $1
================================
' LANGUAGE sql
================================

CREATE FUNCTION priv_testfunc2(int) RETURNS int AS 'select 3 * $1
================================
' LANGUAGE sql
================================

CREATE AGGREGATE priv_testagg1(int) (sfunc = int4pl, stype = int4)
================================

CREATE PROCEDURE priv_testproc1(int) AS 'select $1
================================
' LANGUAGE sql
================================


CREATE FUNCTION priv_testfunc4(boolean) RETURNS text
  AS 'select col1 from atest2 where col2 = $1
================================
'
  LANGUAGE sql SECURITY DEFINER
================================
 -- ok
CREATE FUNCTION priv_testfunc3(int) RETURNS int AS 'select 2 * $1
================================
' LANGUAGE sql
================================
 -- fail
DROP AGGREGATE priv_testagg1(int)
================================
 -- fail

\c -

DROP FUNCTION priv_testfunc1(int)
================================

REVOKE ALL ON FUNCTION int8(integer) FROM PUBLIC
================================


-- privileges on types

-- switch to superuser
\c -

CREATE TYPE priv_testtype1 AS (a int, b text)
================================


-- commands that should fail

CREATE AGGREGATE priv_testagg1a(priv_testdomain1) (sfunc = int4_sum, stype = bigint)
================================

CREATE CAST (priv_testdomain1 AS priv_testdomain3a) WITH FUNCTION castfunc(int)
================================


CREATE OPERATOR !+! (PROCEDURE = int4pl, LEFTARG = priv_testdomain1, RIGHTARG = priv_testdomain1)
================================


CREATE TYPE test7a AS (a int, b priv_testdomain1)
================================


CREATE TYPE test8a AS (a int, b int)
================================

ALTER TYPE test8a ADD ATTRIBUTE c priv_testdomain1
================================

ALTER TYPE test8a ALTER ATTRIBUTE b TYPE priv_testdomain1
================================


CREATE TABLE test11a AS (SELECT 1::priv_testdomain1 AS a)
================================


-- commands that should succeed

CREATE AGGREGATE priv_testagg1b(priv_testdomain1) (sfunc = int4_sum, stype = bigint)
================================

CREATE CAST (priv_testdomain1 AS priv_testdomain3b) WITH FUNCTION castfunc(int)
================================


CREATE OPERATOR !! (PROCEDURE = priv_testfunc5b, RIGHTARG = priv_testdomain1)
================================


CREATE TYPE test7b AS (a int, b priv_testdomain1)
================================


CREATE TYPE test8b AS (a int, b int)
================================

ALTER TYPE test8b ADD ATTRIBUTE c priv_testdomain1
================================

ALTER TYPE test8b ALTER ATTRIBUTE b TYPE priv_testdomain1
================================


CREATE TABLE test11b AS (SELECT 1::priv_testdomain1 AS a)
================================


\c -
DROP AGGREGATE priv_testagg1b(priv_testdomain1)
================================

DROP OPERATOR !! (NONE, priv_testdomain1)
================================

DROP TYPE test7b
================================

DROP TYPE test8b
================================

DROP CAST (priv_testdomain1 AS priv_testdomain3b)
================================


DROP TYPE priv_testtype1
================================


-- superuser
\c -

select has_table_privilege(current_user,'pg_authid','select')
================================
 -- true


-- security-restricted operations
\c -
CREATE ROLE regress_sro_user
================================

CREATE FUNCTION mv_action() RETURNS bool LANGUAGE sql AS
	'DECLARE c CURSOR WITH HOLD FOR SELECT unwanted_grant()
================================
 SELECT true'
================================

-- REFRESH of this MV will queue a GRANT at end of transaction
CREATE MATERIALIZED VIEW sro_mv AS SELECT mv_action() WITH NO DATA
================================

REFRESH MATERIALIZED VIEW sro_mv
================================

\c -
REFRESH MATERIALIZED VIEW sro_mv
================================

CREATE FUNCTION sro_trojan() RETURNS trigger LANGUAGE plpgsql AS
	'BEGIN PERFORM unwanted_grant()
================================
 RETURN NULL
================================

CREATE CONSTRAINT TRIGGER t AFTER INSERT ON sro_trojan_table
    INITIALLY DEFERRED FOR EACH ROW EXECUTE PROCEDURE sro_trojan()
================================

-- Now, REFRESH will issue such an INSERT, queueing the GRANT
CREATE OR REPLACE FUNCTION mv_action() RETURNS bool LANGUAGE sql AS
	'INSERT INTO sro_trojan_table DEFAULT VALUES
================================
 SELECT true'
================================

REFRESH MATERIALIZED VIEW sro_mv
================================

\c -
REFRESH MATERIALIZED VIEW sro_mv
================================
 REFRESH MATERIALIZED VIEW sro_mv
================================


DROP OWNED BY regress_sro_user
================================



-- has_sequence_privilege tests
\c -

CREATE SEQUENCE x_seq
================================


-- largeobject privilege tests
\c -
SET SESSION AUTHORIZATION regress_priv_user1
================================
	-- to be failed

\c -
SET SESSION AUTHORIZATION regress_priv_user2
================================


\c -
-- confirm ACL setting
SELECT oid, pg_get_userbyid(lomowner) ownername, lomacl FROM pg_largeobject_metadata WHERE oid >= 1000 AND oid < 3000 ORDER BY oid
================================


-- compatibility mode in largeobject permission
\c -
SET lo_compat_privileges = false
================================
			-- to be denied

\c -
SET lo_compat_privileges = true
================================
			-- to be denied

-- don't allow unpriv users to access pg_largeobject contents
\c -
SELECT * FROM pg_largeobject LIMIT 0
================================

DO $$BEGIN EXECUTE format(
	'ALTER DATABASE %I OWNER TO regress_priv_group2', current_catalog)
================================
 END$$
================================

INSERT INTO datdba_only DEFAULT VALUES
================================

INSERT INTO datdba_only DEFAULT VALUES
================================


-- test default ACLs
\c -

CREATE SCHEMA testns
================================
 -- no

-- placeholder for test with duplicated schema and role names
ALTER DEFAULT PRIVILEGES IN SCHEMA testns,testns GRANT SELECT ON TABLES TO public,public
================================
 -- no

ALTER DEFAULT PRIVILEGES IN SCHEMA testns GRANT INSERT ON TABLES TO regress_priv_user1
================================
 -- yes

ALTER DEFAULT PRIVILEGES IN SCHEMA testns REVOKE INSERT ON TABLES FROM regress_priv_user1
================================
 -- no

ALTER DEFAULT PRIVILEGES FOR ROLE regress_priv_user1 REVOKE EXECUTE ON FUNCTIONS FROM public
================================


ALTER DEFAULT PRIVILEGES IN SCHEMA testns GRANT USAGE ON SCHEMAS TO regress_priv_user2
================================


ALTER DEFAULT PRIVILEGES GRANT USAGE ON SCHEMAS TO regress_priv_user2
================================


CREATE SCHEMA testns2
================================
 -- no

ALTER DEFAULT PRIVILEGES REVOKE USAGE ON SCHEMAS FROM regress_priv_user2
================================


CREATE SCHEMA testns3
================================
 -- no

ALTER DEFAULT PRIVILEGES GRANT ALL ON SCHEMAS TO regress_priv_user2
================================


CREATE SCHEMA testns4
================================
 -- yes

ALTER DEFAULT PRIVILEGES REVOKE ALL ON SCHEMAS FROM regress_priv_user2
================================

ALTER DEFAULT PRIVILEGES GRANT ALL ON FUNCTIONS TO regress_priv_user2
================================

ALTER DEFAULT PRIVILEGES GRANT ALL ON SCHEMAS TO regress_priv_user2
================================

ALTER DEFAULT PRIVILEGES GRANT ALL ON SEQUENCES TO regress_priv_user2
================================

ALTER DEFAULT PRIVILEGES GRANT ALL ON TABLES TO regress_priv_user2
================================

ALTER DEFAULT PRIVILEGES GRANT ALL ON TYPES TO regress_priv_user2
================================

DROP OWNED BY regress_priv_user2, regress_priv_user2
================================


CREATE SCHEMA testns5
================================

CREATE AGGREGATE testns.agg1(int) (sfunc = int4pl, stype = int4)
================================
 -- no

ALTER DEFAULT PRIVILEGES IN SCHEMA testns GRANT EXECUTE ON ROUTINES to public
================================

DROP AGGREGATE testns.agg1(int)
================================

CREATE AGGREGATE testns.agg1(int) (sfunc = int4pl, stype = int4)
================================

DROP AGGREGATE testns.agg1(int)
================================


ALTER DEFAULT PRIVILEGES FOR ROLE regress_priv_user1 REVOKE USAGE ON TYPES FROM public
================================
 -- no

ALTER DEFAULT PRIVILEGES IN SCHEMA testns GRANT USAGE ON TYPES to public
================================


DROP SCHEMA testns CASCADE
================================

DROP SCHEMA testns2 CASCADE
================================

DROP SCHEMA testns3 CASCADE
================================

DROP SCHEMA testns4 CASCADE
================================

DROP SCHEMA testns5 CASCADE
================================



-- Grant on all objects of given type in a schema
\c -

CREATE SCHEMA testns
================================
 -- false

CREATE FUNCTION testns.priv_testfunc(int) RETURNS int AS 'select 3 * $1
================================
' LANGUAGE sql
================================

CREATE AGGREGATE testns.priv_testagg(int) (sfunc = int4pl, stype = int4)
================================
 -- true

DROP SCHEMA testns CASCADE
================================



-- Change owner of the schema & and rename of new schema owner
\c -

CREATE ROLE regress_schemauser1 superuser login
================================

CREATE SCHEMA testns
================================


ALTER SCHEMA testns OWNER TO regress_schemauser2
================================

DROP SCHEMA testns CASCADE
================================


-- clean up
\c -

DROP ROLE regress_schemauser1
================================



-- test that dependent privileges are revoked (or not) properly
\c -

set session role regress_priv_user1
================================

\dp dep_priv_test
set session role regress_priv_user2
================================

\dp dep_priv_test
set session role regress_priv_user3
================================

\dp dep_priv_test
set session role regress_priv_user1
================================



-- clean up

\c

drop sequence x_seq
================================


DROP AGGREGATE priv_testagg1(int)
================================


DROP GROUP regress_priv_group1
================================

DROP GROUP regress_priv_group2
================================

DROP OWNED BY regress_priv_user1
================================

LOCK TABLE lock_table IN ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_table IN ACCESS SHARE MODE
================================

LOCK TABLE lock_table IN ACCESS EXCLUSIVE MODE
================================

\c
REVOKE SELECT ON lock_table FROM regress_locktable_user
================================

LOCK TABLE lock_table IN ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_table IN ACCESS SHARE MODE
================================

LOCK TABLE lock_table IN ACCESS EXCLUSIVE MODE
================================

\c
REVOKE INSERT ON lock_table FROM regress_locktable_user
================================

LOCK TABLE lock_table IN ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_table IN ACCESS SHARE MODE
================================

LOCK TABLE lock_table IN ACCESS EXCLUSIVE MODE
================================

\c
REVOKE UPDATE ON lock_table FROM regress_locktable_user
================================

LOCK TABLE lock_table IN ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_table IN ACCESS SHARE MODE
================================

LOCK TABLE lock_table IN ACCESS EXCLUSIVE MODE
================================

\c
REVOKE DELETE ON lock_table FROM regress_locktable_user
================================

LOCK TABLE lock_table IN ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_table IN ACCESS SHARE MODE
================================

LOCK TABLE lock_table IN ACCESS EXCLUSIVE MODE
================================

\c
REVOKE TRUNCATE ON lock_table FROM regress_locktable_user
================================


-- test to check privileges of system views pg_shmem_allocations and
-- pg_backend_memory_contexts.

-- switch to superuser
\c -

CREATE ROLE regress_readallstats
================================


================================


================================
--
-- Regression tests for schemas (namespaces)
--

CREATE SCHEMA test_ns_schema_1
       CREATE UNIQUE INDEX abc_a_idx ON abc (a)

       CREATE VIEW abc_view AS
              SELECT a+1 AS a, b+1 AS b FROM abc

       CREATE TABLE abc (
              a serial,
              b int UNIQUE
       )
================================


INSERT INTO test_ns_schema_1.abc DEFAULT VALUES
================================

INSERT INTO test_ns_schema_1.abc DEFAULT VALUES
================================

INSERT INTO test_ns_schema_1.abc DEFAULT VALUES
================================


ALTER SCHEMA test_ns_schema_1 RENAME TO test_ns_schema_renamed
================================


-- test IF NOT EXISTS cases
CREATE SCHEMA test_ns_schema_renamed
================================
 -- fail, already exists
CREATE SCHEMA IF NOT EXISTS test_ns_schema_renamed
================================
 -- ok with notice
CREATE SCHEMA IF NOT EXISTS test_ns_schema_renamed -- fail, disallowed
       CREATE TABLE abc (
              a serial,
              b int UNIQUE
       )
================================


DROP SCHEMA test_ns_schema_renamed CASCADE
================================


================================

SELECT m / '2'::money FROM money_data
================================


-- input checks
SELECT '1234567890'::money
================================

SELECT '12345678901234567'::money
================================

SELECT '123456789012345678'::money
================================

SELECT '9223372036854775807'::money
================================

SELECT '-12345'::money
================================

SELECT '-1234567890'::money
================================

SELECT '-12345678901234567'::money
================================

SELECT '-123456789012345678'::money
================================

SELECT '-9223372036854775808'::money
================================


-- special characters
SELECT '(1)'::money
================================

SELECT '($123,456.78)'::money
================================


-- documented minimums and maximums
SELECT '-92233720368547758.08'::money
================================

SELECT '92233720368547758.07'::money
================================


SELECT '-92233720368547758.09'::money
================================

SELECT '92233720368547758.08'::money
================================


-- rounding
SELECT '-92233720368547758.085'::money
================================

SELECT '92233720368547758.075'::money
================================


-- rounding vs. truncation in division
SELECT '878.08'::money / 11::float8
================================

SELECT '878.08'::money / 11::float4
================================

SELECT '878.08'::money / 11::bigint
================================

SELECT '878.08'::money / 11::int
================================

SELECT '878.08'::money / 11::smallint
================================


-- check for precision loss in division
SELECT '90000000000000099.00'::money / 10::bigint
================================

SELECT '90000000000000099.00'::money / 10::int
================================

SELECT '90000000000000099.00'::money / 10::smallint
================================


-- Cast int4/int8/numeric to money
SELECT 1234567890::money
================================

SELECT 12345678901234567::money
================================

SELECT (-12345)::money
================================

SELECT (-1234567890)::money
================================

SELECT (-12345678901234567)::money
================================

SELECT 1234567890::int4::money
================================

SELECT 12345678901234567::int8::money
================================

SELECT 12345678901234567::numeric::money
================================

SELECT (-1234567890)::int4::money
================================

SELECT (-12345678901234567)::int8::money
================================

SELECT (-12345678901234567)::numeric::money
================================


-- Cast from money to numeric
SELECT '12345678901234567'::money::numeric
================================

SELECT '-12345678901234567'::money::numeric
================================

SELECT '92233720368547758.07'::money::numeric
================================

SELECT '-92233720368547758.08'::money::numeric
================================


================================


\d itest1_a_seq

CREATE TABLE itest4 (a int, b text)
================================


INSERT INTO itest1 DEFAULT VALUES
================================

INSERT INTO itest1 DEFAULT VALUES
================================

INSERT INTO itest2 DEFAULT VALUES
================================

INSERT INTO itest2 DEFAULT VALUES
================================

INSERT INTO itest3 DEFAULT VALUES
================================

INSERT INTO itest3 DEFAULT VALUES
================================

INSERT INTO itest4 DEFAULT VALUES
================================

INSERT INTO itest4 DEFAULT VALUES
================================

100	foo	200
101	bar	201
\.

COPY itest9 (b, c) FROM stdin
================================

foo2	202
bar2	203
\.

SELECT * FROM itest9 ORDER BY c
================================
  -- noop

INSERT INTO itest4 DEFAULT VALUES
================================

INSERT INTO itest4 DEFAULT VALUES
================================


INSERT INTO itestv10 DEFAULT VALUES
================================

INSERT INTO itestv10 DEFAULT VALUES
================================


INSERT INTO itestv11 DEFAULT VALUES
================================

INSERT INTO itestv11 DEFAULT VALUES
================================

\d itest3

ALTER TABLE itest3 ALTER COLUMN a TYPE text
================================

\d itest3


-- ALTER COLUMN ... SET

CREATE TABLE itest6 (a int GENERATED ALWAYS AS IDENTITY, b text)
================================

INSERT INTO itest6 DEFAULT VALUES
================================

INSERT INTO itest6 DEFAULT VALUES
================================

INSERT INTO itest6 DEFAULT VALUES
================================
  -- fail, not identity


-- prohibited direct modification of sequence

ALTER SEQUENCE itest6_a_seq OWNED BY NONE
================================

INSERT INTO itest7 DEFAULT VALUES
================================

INSERT INTO itest7c DEFAULT VALUES
================================

INSERT INTO itest8 DEFAULT VALUES
================================

\d+ itest8
\d itest8_f2_seq
\d itest8_f3_seq
\d itest8_f4_seq
\d itest8_f5_seq
DROP TABLE itest8
================================



-- typed tables (currently not supported)

CREATE TYPE itest_type AS (f1 integer, f2 text, f3 bigint)
================================
 -- error
DROP TYPE itest_type CASCADE
================================


================================
--
-- Test assorted system views
--
-- This test is mainly meant to provide some code coverage for the
-- set-returning functions that underlie certain system views.
-- The output of most of these functions is very environment-dependent,
-- so our ability to test with fixed expected output is pretty limited
================================


-- At introduction, pg_config had 23 entries
================================
 it may grow
select count(*) > 20 as ok from pg_config
================================


-- We expect no cursors in this test
================================
 see also portals.sql
select count(*) = 0 as ok from pg_cursors
================================


-- We expect no prepared statements in this test
================================
 see also prepare.sql
select count(*) = 0 as ok from pg_prepared_statements
================================


================================


================================


================================

                ln := regexp_replace(ln, 'Hits: \d+', 'Hits: N')
================================

                ln := regexp_replace(ln, 'Misses: 0', 'Misses: Zero')
================================

                ln := regexp_replace(ln, 'Misses: \d+', 'Misses: N')
================================

        ln := regexp_replace(ln, 'Evictions: 0', 'Evictions: Zero')
================================

        ln := regexp_replace(ln, 'Evictions: \d+', 'Evictions: N')
================================

        ln := regexp_replace(ln, 'Memory Usage: \d+', 'Memory Usage: N')
================================

	ln := regexp_replace(ln, 'Heap Fetches: \d+', 'Heap Fetches: N')
================================

	ln := regexp_replace(ln, 'loops=\d+', 'loops=N')
================================

        return next ln
================================

$$
================================


SELECT explain_memoize('
SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1
INNER JOIN tenk1 t2 ON t1.unique1 = t2.twenty
WHERE t2.unique1 < 1000
================================
', false)
================================


-- Try with LATERAL joins
SELECT explain_memoize('
SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,
LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2
WHERE t1.unique1 < 1000
================================
', false)
================================

-- Ensure we get some evictions.  We're unable to validate the hits and misses
-- here as the number of entries that fit in the cache at once will vary
-- between different machines.
SELECT explain_memoize('
SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1
INNER JOIN tenk1 t2 ON t1.unique1 = t2.thousand
WHERE t2.unique1 < 1200
================================
', true)
================================


================================


--
-- Test of SECURITY LABEL statement without a plugin
--
SECURITY LABEL ON TABLE seclabel_tbl1 IS 'classified'
================================
			-- fail
SECURITY LABEL FOR 'dummy' ON TABLE seclabel_tbl1 IS 'classified'
================================
		-- fail
SECURITY LABEL ON TABLE seclabel_tbl1 IS '...invalid label...'
================================
		-- fail
SECURITY LABEL ON TABLE seclabel_tbl3 IS 'unclassified'
================================
			-- fail

SECURITY LABEL ON ROLE regress_seclabel_user1 IS 'classified'
================================
			-- fail
SECURITY LABEL FOR 'dummy' ON ROLE regress_seclabel_user1 IS 'classified'
================================
		-- fail
SECURITY LABEL ON ROLE regress_seclabel_user1 IS '...invalid label...'
================================
		-- fail
SECURITY LABEL ON ROLE regress_seclabel_user3 IS 'unclassified'
================================


================================

-- size is chosen to exceed page size and trigger actual truncation
INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,8000) AS x
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,8000) AS x
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,10) AS x
================================

REINDEX INDEX tbl_gist_idx
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,10) AS x
================================

\d tbl_gist
DROP TABLE tbl_gist
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(x,x+1),point(2*x,2*x+1)) FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_gist SELECT x, 2*x, 3*x, box(point(3*x,2*x),point(3*x+1,2*x+1)) FROM generate_series(1,10) AS x
================================

\d tbl_gist
DROP TABLE tbl_gist
================================


================================

CREATE TABLE reloptions_test2(i INT) WITH (autovacuum_analyze_scale_factor = -10.0)
================================


-- Fail while setting improper values
CREATE TABLE reloptions_test2(i INT) WITH (fillfactor=-30.1)
================================


-- Set boolean option to true without specifying value
ALTER TABLE reloptions_test SET (autovacuum_enabled, fillfactor=32)
================================


-- RESET fails if a value is specified
ALTER TABLE reloptions_test RESET (fillfactor=12)
================================

SELECT reltoastrelid as toast_oid
	FROM pg_class WHERE oid = 'reloptions_test'::regclass \gset
SELECT reloptions FROM pg_class WHERE oid = :toast_oid
================================


ALTER TABLE reloptions_test SET (toast.autovacuum_vacuum_cost_delay = 24)
================================

SELECT reloptions FROM pg_class WHERE oid = :toast_oid
================================


ALTER TABLE reloptions_test RESET (toast.autovacuum_vacuum_cost_delay)
================================

SELECT reloptions FROM pg_class WHERE oid = :toast_oid
================================


================================


================================


INSERT INTO brintest_multi SELECT
	142857 * tenthous,
	thousand,
	twothousand,
	unique1::oid,
	format('(%s,%s)', tenthous, twenty)::tid,
	(four + 1.0)/(hundred+1),
	odd::float8 / (tenthous + 1),
	format('%s:00:%s:00:%s:00', to_hex(odd), to_hex(even), to_hex(hundred))::macaddr,
	substr(md5(unique1::text), 1, 16)::macaddr8,
	inet '10.2.3.4/24' + tenthous,
	cidr '10.2.3/24' + tenthous,
	date '1995-08-15' + tenthous,
	time '01:20:30' + thousand * interval '18.5 second',
	timestamp '1942-07-23 03:05:09' + tenthous * interval '36.38 hours',
	timestamptz '1972-10-10 03:00' + thousand * interval '1 hour',
	justify_days(justify_hours(tenthous * interval '12 minutes')),
	timetz '01:30:20+02' + hundred * interval '15 seconds',
	tenthous::numeric(36,30) * fivethous * even / (hundred + 1),
	format('%s%s-%s-%s-%s-%s%s%s', to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'))::uuid,
	format('%s/%s%s', odd, even, tenthous)::pg_lsn
FROM tenk1 ORDER BY unique2 LIMIT 100
================================


-- throw in some NULL's and different values
INSERT INTO brintest_multi (inetcol, cidrcol) SELECT
	inet 'fe80::6e40:8ff:fea9:8c46' + tenthous,
	cidr 'fe80::6e40:8ff:fea9:8c46' + tenthous
FROM tenk1 ORDER BY thousand, tenthous LIMIT 25
================================


DO $x$
DECLARE
	r record
================================

	r2 record
================================

	cond text
================================

	idx_ctids tid[]
================================

	ss_ctids tid[]
================================

	count int
================================

	plan_ok bool
================================

	plan_line text
================================

		ELSE
			cond := format('%I %s %L::%s', r.colname, r.oper, r.value, r.typ)
================================


		plan_ok := false
================================

		FOR plan_line IN EXECUTE format($y$EXPLAIN SELECT array_agg(ctid) FROM brintest_multi WHERE %s $y$, cond) LOOP
			IF plan_line LIKE '%Bitmap Heap Scan on brintest_multi%' THEN
				plan_ok := true
================================

		IF NOT plan_ok THEN
			RAISE WARNING 'did not get bitmap indexscan plan for %', r
================================


		EXECUTE format($y$SELECT array_agg(ctid) FROM brintest_multi WHERE %s $y$, cond)
			INTO idx_ctids
================================


		plan_ok := false
================================

		FOR plan_line IN EXECUTE format($y$EXPLAIN SELECT array_agg(ctid) FROM brintest_multi WHERE %s $y$, cond) LOOP
			IF plan_line LIKE '%Seq Scan on brintest_multi%' THEN
				plan_ok := true
================================

		IF NOT plan_ok THEN
			RAISE WARNING 'did not get seqscan plan for %', r
================================


		EXECUTE format($y$SELECT array_agg(ctid) FROM brintest_multi WHERE %s $y$, cond)
			INTO ss_ctids
================================


		-- make sure both return the same results
		count := array_length(idx_ctids, 1)
================================


		IF NOT (count = array_length(ss_ctids, 1) AND
				idx_ctids @> ss_ctids AND
				idx_ctids <@ ss_ctids) THEN
			-- report the results of each scan to make the differences obvious
			RAISE WARNING 'something not right in %: count %', r, count
================================

			FOR r2 IN EXECUTE 'SELECT ' || r.colname || ' FROM brintest_multi WHERE ' || cond LOOP
				RAISE NOTICE 'seqscan: %', r2
================================

			FOR r2 IN EXECUTE 'SELECT ' || r.colname || ' FROM brintest_multi WHERE ' || cond LOOP
				RAISE NOTICE 'bitmapscan: %', r2
================================


		-- make sure we found expected number of matches
		IF count != r.matches THEN RAISE WARNING 'unexpected number of results % for %', count, r
================================

$x$
================================


INSERT INTO brintest_multi SELECT
	142857 * tenthous,
	thousand,
	twothousand,
	unique1::oid,
	format('(%s,%s)', tenthous, twenty)::tid,
	(four + 1.0)/(hundred+1),
	odd::float8 / (tenthous + 1),
	format('%s:00:%s:00:%s:00', to_hex(odd), to_hex(even), to_hex(hundred))::macaddr,
	substr(md5(unique1::text), 1, 16)::macaddr8,
	inet '10.2.3.4' + tenthous,
	cidr '10.2.3/24' + tenthous,
	date '1995-08-15' + tenthous,
	time '01:20:30' + thousand * interval '18.5 second',
	timestamp '1942-07-23 03:05:09' + tenthous * interval '36.38 hours',
	timestamptz '1972-10-10 03:00' + thousand * interval '1 hour',
	justify_days(justify_hours(tenthous * interval '12 minutes')),
	timetz '01:30:20' + hundred * interval '15 seconds',
	tenthous::numeric(36,30) * fivethous * even / (hundred + 1),
	format('%s%s-%s-%s-%s-%s%s%s', to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'))::uuid,
	format('%s/%s%s', odd, even, tenthous)::pg_lsn
FROM tenk1 ORDER BY unique2 LIMIT 5 OFFSET 5
================================
  -- force a summarization cycle in brinidx

-- Try inserting a values with NaN, to test distance calculation.
insert into public.brintest_multi (float4col) values (real 'nan')
================================

insert into public.brintest_multi (float8col) values (real 'nan')
================================

INSERT INTO brin_large_range SELECT i FROM generate_series(1,10000) s(i)
================================

-- Fill a few pages
DO $$
DECLARE curtid tid
================================

    EXIT WHEN curtid > tid '(2, 0)'
================================

$$
================================

INSERT INTO brin_test_multi SELECT x/100,x%100 FROM generate_series(1,10000) x(x)
================================


================================


-- Insert enough data to create a tree that's a couple of levels deep.
insert into gist_point_tbl (id, p)
select g,        point(g*10, g*10) from generate_series(1, 10000) g
================================


insert into gist_point_tbl (id, p)
select g+100000, point(g*10+1, g*10+1) from generate_series(1, 10000) g
================================

reindex index gist_pointidx
================================


insert into gist_tbl
select box(point(0.05*i, 0.05*i), point(0.05*i, 0.05*i)),
       point(0.05*i, 0.05*i),
       circle(point(0.05*i, 0.05*i), 1.0)
from generate_series(0,10000) as i
================================


-- Check case with multiple rescans (bug #14641)
explain (costs off)
select p from
  (values (box(point(0,0), point(0.5,0.5))),
          (box(point(0.5,0.5), point(0.75,0.75))),
          (box(point(0.8,0.8), point(1.0,1.0)))) as v(bb)
cross join lateral
  (select p from gist_tbl where p <@ bb order by p <-> bb[0] limit 2) ss
================================


select p from
  (values (box(point(0,0), point(0.5,0.5))),
          (box(point(0.5,0.5), point(0.75,0.75))),
          (box(point(0.8,0.8), point(1.0,1.0)))) as v(bb)
cross join lateral
  (select p from gist_tbl where p <@ bb order by p <-> bb[0] limit 2) ss
================================


================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 <> int2 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 <> int4 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 = int2 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 = int4 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 < int2 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 < int4 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 <= int2 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 <= int4 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 > int2 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 > int4 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 >= int2 '0'
================================


SELECT i.* FROM INT2_TBL i WHERE i.f1 >= int4 '0'
================================


-- positive odds
SELECT i.* FROM INT2_TBL i WHERE (i.f1 % int2 '2') = int2 '1'
================================


-- any evens
SELECT i.* FROM INT2_TBL i WHERE (i.f1 % int4 '2') = int2 '0'
================================


SELECT i.f1, i.f1 * int2 '2' AS x FROM INT2_TBL i
================================


SELECT i.f1, i.f1 * int2 '2' AS x FROM INT2_TBL i
WHERE abs(f1) < 16384
================================


SELECT i.f1, i.f1 * int4 '2' AS x FROM INT2_TBL i
================================


SELECT i.f1, i.f1 + int2 '2' AS x FROM INT2_TBL i
================================


SELECT i.f1, i.f1 + int2 '2' AS x FROM INT2_TBL i
WHERE f1 < 32766
================================


SELECT i.f1, i.f1 + int4 '2' AS x FROM INT2_TBL i
================================


SELECT i.f1, i.f1 / int2 '2' AS x FROM INT2_TBL i
================================


SELECT i.f1, i.f1 / int4 '2' AS x FROM INT2_TBL i
================================


-- corner cases
SELECT (-1::int2<<15)::text
================================

SELECT ((-1::int2<<15)+1::int2)::text
================================


================================
--
-- SELECT
--

-- btree index
-- awk '{if($1<10){print
================================
}else{next
================================
}}' onek.data | sort +0n -1
--
SELECT * FROM onek
   WHERE onek.unique1 < 10
   ORDER BY onek.unique1
================================


--
-- awk '{if($1<20){print $1,$14
================================
}else{next
================================
}}' onek.data | sort +0nr -1
--
SELECT onek.unique1, onek.stringu1 FROM onek
   WHERE onek.unique1 < 20
   ORDER BY unique1 using >
================================


--
-- awk '{if($1>980){print $1,$14
================================
}else{next
================================
}}' onek.data | sort +1d -2
--
SELECT onek.unique1, onek.stringu1 FROM onek
   WHERE onek.unique1 > 980
   ORDER BY stringu1 using <
================================


--
-- awk '{if($1>980){print $1,$16
================================
}else{next
================================
}}' onek.data |
-- sort +1d -2 +0nr -1
--
SELECT onek.unique1, onek.string4 FROM onek
   WHERE onek.unique1 > 980
   ORDER BY string4 using <, unique1 using >
================================


--
-- awk '{if($1>980){print $1,$16
================================
}else{next
================================
}}' onek.data |
-- sort +1dr -2 +0n -1
--
SELECT onek.unique1, onek.string4 FROM onek
   WHERE onek.unique1 > 980
   ORDER BY string4 using >, unique1 using <
================================


--
-- awk '{if($1<20){print $1,$16
================================
}else{next
================================
}}' onek.data |
-- sort +0nr -1 +1d -2
--
SELECT onek.unique1, onek.string4 FROM onek
   WHERE onek.unique1 < 20
   ORDER BY unique1 using >, string4 using <
================================


--
-- awk '{if($1<20){print $1,$16
================================
}else{next
================================
}}' onek.data |
-- sort +0n -1 +1dr -2
--
SELECT onek.unique1, onek.string4 FROM onek
   WHERE onek.unique1 < 20
   ORDER BY unique1 using <, string4 using >
================================


--
-- awk '{if($1<10){print $0
================================
}else{next
================================
}}' onek.data | sort +0n -1
--
SELECT onek2.* FROM onek2 WHERE onek2.unique1 < 10
================================


--
-- awk '{if($1<20){print $1,$14
================================
}else{next
================================
}}' onek.data | sort +0nr -1
--
SELECT onek2.unique1, onek2.stringu1 FROM onek2
    WHERE onek2.unique1 < 20
    ORDER BY unique1 using >
================================


--
-- awk '{if($1>980){print $1,$14
================================
}else{next
================================
}}' onek.data | sort +1d -2
--
SELECT onek2.unique1, onek2.stringu1 FROM onek2
   WHERE onek2.unique1 > 980
================================


--
-- awk '{print $1,$2
================================
}' person.data |
-- awk '{if(NF!=2){print $3,$2
================================
}else{print
================================
}}' - emp.data |
-- awk '{if(NF!=2){print $3,$2
================================
}else{print
================================
}}' - student.data |
-- awk 'BEGIN{FS="      "
================================
}{if(NF!=2){print $4,$5
================================
}else{print
================================
}}' - stud_emp.data
--
-- SELECT name, age FROM person*
================================
 ??? check if different
SELECT p.name, p.age FROM person* p
================================


--
-- awk '{print $1,$2
================================
}' person.data |
-- awk '{if(NF!=2){print $3,$2
================================
}else{print
================================
}}' - emp.data |
-- awk '{if(NF!=2){print $3,$2
================================
}else{print
================================
}}' - student.data |
-- awk 'BEGIN{FS="      "
================================
}{if(NF!=1){print $4,$5
================================
}else{print
================================
}}' - stud_emp.data |
-- sort +1nr -2
--
SELECT p.name, p.age FROM person* p ORDER BY age using >, name
================================


================================


================================


CREATE TYPE person_type AS (id int, name text)
================================

\d persons

CREATE FUNCTION get_all_persons() RETURNS SETOF person_type
LANGUAGE SQL
AS $$
    SELECT * FROM persons
================================

$$
================================
 -- error

CREATE TABLE persons2 OF person_type (
    id WITH OPTIONS PRIMARY KEY,
    UNIQUE (name)
)
================================


\d persons2

CREATE TABLE persons3 OF person_type (
    PRIMARY KEY (id),
    name WITH OPTIONS DEFAULT ''
)
================================


\d persons3

CREATE TABLE persons4 OF person_type (
    name WITH OPTIONS NOT NULL,
    name WITH OPTIONS DEFAULT ''  -- error, specified more than once
)
================================


DROP TYPE person_type RESTRICT
================================

DROP TYPE person_type CASCADE
================================



-- implicit casting

CREATE TYPE person_type AS (id int, name text)
================================


CREATE TABLE persons2 OF person_type (
    id WITH OPTIONS PRIMARY KEY,
    UNIQUE (name)
)
================================


\d persons2

CREATE TABLE persons3 OF person_type (
    PRIMARY KEY (id),
    name NOT NULL DEFAULT ''
)
================================


\d persons3

================================

      PERFORM hs_subxids(n - 1)
================================

      RETURN
================================

    EXCEPTION WHEN raise_exception THEN NULL
================================

$$
================================

		RETURN
================================

      EXECUTE 'CREATE TABLE hs_locks_' || n::text || ' ()'
================================

      PERFORM hs_locks_create(n - 1)
================================

      RETURN
================================

    EXCEPTION WHEN raise_exception THEN NULL
================================

$$
================================

		RETURN
================================

	  EXECUTE 'DROP TABLE IF EXISTS hs_locks_' || n::text
================================

      PERFORM hs_locks_drop(n - 1)
================================

      RETURN
================================

    EXCEPTION WHEN raise_exception THEN NULL
================================

$$
================================


================================


\c

SELECT * FROM temptest
================================

CREATE TEMP TABLE temptest(col) ON COMMIT DELETE ROWS AS SELECT 1
================================

CREATE TEMP TABLE temptest(col) ON COMMIT DROP AS SELECT 1
================================

CREATE TABLE temptest(col) ON COMMIT DELETE ROWS AS SELECT 1
================================

prepare transaction 'twophase_func'
================================

prepare transaction 'twophase_func'
================================

create operator pg_temp.@@ (leftarg = int4, rightarg = int4, procedure = int4mi)
================================

prepare transaction 'twophase_operator'
================================

create type pg_temp.twophase_type as (a int)
================================

prepare transaction 'twophase_type'
================================

prepare transaction 'twophase_view'
================================

prepare transaction 'twophase_sequence'
================================

prepare transaction 'twophase_tab'
================================

prepare transaction 'twophase_tab'
================================

lock twophase_tab in access exclusive mode
================================

prepare transaction 'twophase_tab'
================================

prepare transaction 'twophase_tab'
================================


-- Corner case: current_schema may create a temporary schema if namespace
-- creation is pending, so check after that.  First reset the connection
-- to remove the temporary namespace.
\c -
SET search_path TO 'pg_temp'
================================

PREPARE TRANSACTION 'twophase_search'
================================


================================
CREATE SCHEMA testxmlschema
================================


DECLARE xc CURSOR WITH HOLD FOR SELECT * FROM testxmlschema.test1 ORDER BY 1, 2
================================

MOVE BACKWARD ALL IN xc
================================


CREATE TABLE testxmlschema.test3
    AS SELECT true c1,
              true::testboolxmldomain c2,
              '2013-02-21'::date c3,
              '2013-02-21'::testdatexmldomain c4
================================


================================


--
-- Test \copy (insert/update/delete ...)
--
\copy (insert into copydml_test (t) values ('f') returning id) to stdout
================================

\copy (update copydml_test set t = 'g' where t = 'f' returning id) to stdout
================================

\copy (delete from copydml_test where t = 'g' returning id) to stdout
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

create rule qqq as on insert to copydml_test do instead (delete from copydml_test
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

create rule qqq as on update to copydml_test do instead (delete from copydml_test
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

create rule qqq as on delete to copydml_test do instead (insert into copydml_test default values
================================
 insert into copydml_test default values)
================================

drop rule qqq on copydml_test
================================

drop rule qqq on copydml_test
================================

    return new
================================

else
    raise notice '% % %', tg_when, tg_op, old.id
================================

    return old
================================


================================


EXECUTE foo
================================


EXECUTE foo
================================
  -- fail

================================


================================


SELECT (@@ f1) AS center
   FROM POLYGON_TBL
   WHERE (# f1) > 2
================================


-- "is horizontal" function
SELECT p1.f1
   FROM POINT_TBL p1
   WHERE ishorizontal(p1.f1, point '(0,0)')
================================


-- "is vertical" function
SELECT p1.f1
   FROM POINT_TBL p1
   WHERE isvertical(p1.f1, point '(5.1,34.5)')
================================


-- Multiply with point
SELECT p1.f1, p2.f1, p1.f1 * p2.f1 FROM POINT_TBL p1, POINT_TBL p2 WHERE p1.f1[0] BETWEEN 1 AND 1000
================================


-- Underflow error
SELECT p1.f1, p2.f1, p1.f1 * p2.f1 FROM POINT_TBL p1, POINT_TBL p2 WHERE p1.f1[0] < 1
================================


-- Divide by point
SELECT p1.f1, p2.f1, p1.f1 / p2.f1 FROM POINT_TBL p1, POINT_TBL p2 WHERE p2.f1[0] BETWEEN 1 AND 1000
================================


-- Overflow error
SELECT p1.f1, p2.f1, p1.f1 / p2.f1 FROM POINT_TBL p1, POINT_TBL p2 WHERE p2.f1[0] > 1000
================================


-- Division by 0 error
SELECT p1.f1, p2.f1, p1.f1 / p2.f1 FROM POINT_TBL p1, POINT_TBL p2 WHERE p2.f1 ~= '(0,0)'::point
================================


-- Construct line through two points
SELECT p1.f1, p2.f1, line(p1.f1, p2.f1)
  FROM POINT_TBL p1, POINT_TBL p2 WHERE p1.f1 <> p2.f1
================================


-- Closest point to line
SELECT p.f1, l.s, p.f1 ## l.s FROM POINT_TBL p, LINE_TBL l
================================


-- Closest point to line segment
SELECT p.f1, l.s, p.f1 ## l.s FROM POINT_TBL p, LSEG_TBL l
================================


-- Closest point to box
SELECT p.f1, b.f1, p.f1 ## b.f1 FROM POINT_TBL p, BOX_TBL b
================================


-- Intersect with line
SELECT l1.s, l2.s FROM LINE_TBL l1, LINE_TBL l2 WHERE l1.s ?# l2.s
================================


-- Intersect with box
SELECT l.s, b.f1 FROM LINE_TBL l, BOX_TBL b WHERE l.s ?# b.f1
================================


-- Intersection point with line
SELECT l1.s, l2.s, l1.s # l2.s FROM LINE_TBL l1, LINE_TBL l2
================================


-- Closest point to line segment
SELECT l.s, l1.s, l.s ## l1.s FROM LINE_TBL l, LSEG_TBL l1
================================


-- Closest point to box
SELECT l.s, b.f1, l.s ## b.f1 FROM LINE_TBL l, BOX_TBL b
================================


--
-- Line segments
--

-- intersection
SELECT p.f1, l.s, l.s # p.f1 AS intersection
   FROM LSEG_TBL l, POINT_TBL p
================================


-- Length
SELECT s, @-@ s FROM LSEG_TBL
================================


-- To point
SELECT s, s::point FROM LSEG_TBL
================================


-- Intersect with line segment
SELECT l.s, l1.s FROM LSEG_TBL l, LINE_TBL l1 WHERE l.s ?# l1.s
================================


-- Intersect with box
SELECT l.s, b.f1 FROM LSEG_TBL l, BOX_TBL b WHERE l.s ?# b.f1
================================


-- Intersection point with line segment
SELECT l1.s, l2.s, l1.s # l2.s FROM LSEG_TBL l1, LSEG_TBL l2
================================


-- Closest point to line
SELECT l.s, l1.s, l.s ## l1.s FROM LSEG_TBL l, LINE_TBL l1
================================


-- Closest point to line segment
SELECT l1.s, l2.s, l1.s ## l2.s FROM LSEG_TBL l1, LSEG_TBL l2
================================


-- Closest point to box
SELECT l.s, b.f1, l.s ## b.f1 FROM LSEG_TBL l, BOX_TBL b
================================


-- Multiply with point
SELECT b.f1, p.f1, b.f1 * p.f1 FROM BOX_TBL b, POINT_TBL p WHERE p.f1[0] BETWEEN 1 AND 1000
================================


-- Overflow error
SELECT b.f1, p.f1, b.f1 * p.f1 FROM BOX_TBL b, POINT_TBL p WHERE p.f1[0] > 1000
================================


-- Divide by point
SELECT b.f1, p.f1, b.f1 / p.f1 FROM BOX_TBL b, POINT_TBL p WHERE p.f1[0] BETWEEN 1 AND 1000
================================


-- To box
SELECT f1::box
	FROM POINT_TBL
================================


-- Below box
SELECT b1.f1, b2.f1, b1.f1 <^ b2.f1 FROM BOX_TBL b1, BOX_TBL b2
================================


-- Above box
SELECT b1.f1, b2.f1, b1.f1 >^ b2.f1 FROM BOX_TBL b1, BOX_TBL b2
================================


-- Intersection point with box
SELECT b1.f1, b2.f1, b1.f1 # b2.f1 FROM BOX_TBL b1, BOX_TBL b2
================================


-- Length
SELECT f1, @-@ f1 FROM PATH_TBL
================================


-- To polygon
SELECT f1, f1::polygon FROM PATH_TBL WHERE isclosed(f1)
================================


-- Open path cannot be converted to polygon error
SELECT f1, f1::polygon FROM PATH_TBL WHERE isopen(f1)
================================


-- Divide by point
SELECT p.f1, p1.f1, p.f1 / p1.f1 FROM PATH_TBL p, POINT_TBL p1 WHERE p1.f1[0] BETWEEN 1 AND 1000
================================


-- Division by 0 error
SELECT p.f1, p1.f1, p.f1 / p1.f1 FROM PATH_TBL p, POINT_TBL p1 WHERE p1.f1 ~= '(0,0)'::point
================================


SELECT npoints(f1) AS npoints, f1 AS polygon
   FROM POLYGON_TBL
================================


SELECT f1 AS open_path, polygon( pclose(f1)) AS polygon
   FROM PATH_TBL
   WHERE isopen(f1)
================================


-- To box
SELECT f1, f1::box FROM POLYGON_TBL
================================


-- To path
SELECT f1, f1::path FROM POLYGON_TBL
================================


-- Same as polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 ~= p2.f1
================================


-- Left of polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 << p2.f1
================================


-- Overlap of left of polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 &< p2.f1
================================


-- Right of polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 >> p2.f1
================================


-- Overlap of right of polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 &> p2.f1
================================


-- Below polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 <<| p2.f1
================================


-- Overlap or below polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 &<| p2.f1
================================


-- Above polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 |>> p2.f1
================================


-- Overlap or above polygon
SELECT p1.f1, p2.f1 FROM POLYGON_TBL p1, POLYGON_TBL p2 WHERE p1.f1 |&> p2.f1
================================


SELECT c1.f1 AS circle, p1.f1 AS point, (p1.f1 <-> c1.f1) AS distance
   FROM CIRCLE_TBL c1, POINT_TBL p1
   WHERE (p1.f1 <-> c1.f1) > 0
   ORDER BY distance, area(c1.f1), p1.f1[0]
================================


-- To polygon
SELECT f1, f1::polygon FROM CIRCLE_TBL WHERE f1 >= '<(0,0),1>'
================================


-- To polygon with less points
SELECT f1, polygon(8, f1) FROM CIRCLE_TBL WHERE f1 >= '<(0,0),1>'
================================


-- Error for insufficient number of points
SELECT f1, polygon(1, f1) FROM CIRCLE_TBL WHERE f1 >= '<(0,0),1>'
================================


-- Zero radius error
SELECT f1, polygon(10, f1) FROM CIRCLE_TBL WHERE f1 < '<(0,0),1>'
================================


-- Same as circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 ~= c2.f1
================================


-- Overlap or left of circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 &< c2.f1
================================


-- Left of circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 << c2.f1
================================


-- Right of circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 >> c2.f1
================================


-- Overlap or right of circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 &> c2.f1
================================


-- Below circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 <<| c2.f1
================================


-- Above circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 |>> c2.f1
================================


-- Overlap or below circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 &<| c2.f1
================================


-- Overlap or above circle
SELECT c1.f1, c2.f1 FROM CIRCLE_TBL c1, CIRCLE_TBL c2 WHERE c1.f1 |&> c2.f1
================================


-- Divide by point
SELECT c.f1, p.f1, c.f1 / p.f1 FROM CIRCLE_TBL c, POINT_TBL p WHERE p.f1[0] BETWEEN 1 AND 1000
================================


-- Overflow error
SELECT c.f1, p.f1, c.f1 / p.f1 FROM CIRCLE_TBL c, POINT_TBL p WHERE p.f1[0] > 1000
================================


-- Division by 0 error
SELECT c.f1, p.f1, c.f1 / p.f1 FROM CIRCLE_TBL c, POINT_TBL p WHERE p.f1 ~= '(0,0)'::point
================================


================================


CREATE SCHEMA collate_tests
================================


\d collate_test1

CREATE TABLE collate_test_fail (
    a int,
    b text COLLATE "ja_JP.eucjp"
)
================================


\d collate_test_like

CREATE TABLE collate_test2 (
    a int,
    b text COLLATE "sv_SE"
)
================================

INSERT INTO collate_test2 SELECT * FROM collate_test1
================================

INSERT INTO collate_test3 SELECT * FROM collate_test1
================================

INSERT INTO collate_test4 SELECT * FROM collate_test1
================================

INSERT INTO collate_test5 SELECT * FROM collate_test1
================================

INSERT INTO collate_test6 VALUES (1, 'abc'), (2, 'ABC'), (3, '123'), (4, 'ab1'),
                                 (5, 'a1!'), (6, 'a c'), (7, '!.
================================
'), (8, '   '),
                                 (9, 'b'), (10, 'B')
================================

SELECT to_char(date '2010-02-01', 'DD TMMON YYYY')
================================

SELECT to_char(date '2010-02-01', 'DD TMMON YYYY' COLLATE "tr_TR")
================================

SELECT to_char(date '2010-04-01', 'DD TMMON YYYY')
================================

SELECT to_char(date '2010-04-01', 'DD TMMON YYYY' COLLATE "tr_TR")
================================
 -- fail

CREATE TABLE test_u AS SELECT a, b FROM collate_test1 UNION ALL SELECT a, b FROM collate_test3
================================



-- casting

SELECT CAST('42' AS text COLLATE "C")
================================

  yy text := y
================================

  yy text := y
================================

CREATE SCHEMA test_schema
================================


-- We need to do this this way to cope with varying names for encodings:
do $$
BEGIN
  EXECUTE 'CREATE COLLATION test0 (locale = ' ||
          quote_literal(current_setting('lc_collate')) || ')
================================
'
================================

CREATE COLLATION test0 FROM "C"
================================
 -- fail, duplicate name
CREATE COLLATION IF NOT EXISTS test0 FROM "C"
================================
 -- ok, skipped
CREATE COLLATION IF NOT EXISTS test0 (locale = 'foo')
================================
 -- ok, skipped
do $$
BEGIN
  EXECUTE 'CREATE COLLATION test1 (lc_collate = ' ||
          quote_literal(current_setting('lc_collate')) ||
          ', lc_ctype = ' ||
          quote_literal(current_setting('lc_ctype')) || ')
================================
'
================================

CREATE COLLATION test3 (lc_collate = 'en_US.utf8')
================================
 -- fail, need lc_ctype
CREATE COLLATION testx (locale = 'nonsense')
================================
 -- fail

CREATE COLLATION test4 FROM nonsense
================================

CREATE COLLATION test5 FROM test0
================================


ALTER COLLATION test1 RENAME TO test11
================================

ALTER COLLATION test0 RENAME TO test11
================================
 -- fail
ALTER COLLATION test1 RENAME TO test22
================================
 -- fail

ALTER COLLATION test11 OWNER TO regress_test_role
================================

ALTER COLLATION test11 OWNER TO nonsense
================================

ALTER COLLATION test11 SET SCHEMA test_schema
================================


COMMENT ON COLLATION test0 IS 'US English'
================================


DROP COLLATION test0, test_schema.test11, test5
================================

DROP COLLATION test0
================================
 -- fail
DROP COLLATION IF EXISTS test0
================================


DROP SCHEMA test_schema
================================



-- ALTER

ALTER COLLATION "en_US" REFRESH VERSION
================================



-- dependencies

CREATE COLLATION test0 FROM "C"
================================

CREATE TYPE collate_dep_test2 AS (x int, y text COLLATE test0)
================================


DROP COLLATION test0 RESTRICT
================================
 -- fail
DROP COLLATION test0 CASCADE
================================


\d collate_dep_test1
\d collate_dep_test2

DROP TABLE collate_dep_test1, collate_dep_test4t
================================

DROP TYPE collate_dep_test2
================================


-- test range types and collations

create type textrange_c as range(subtype=text, collation="C")
================================

create type textrange_en_us as range(subtype=text, collation="en_US")
================================


drop type textrange_c
================================

drop type textrange_en_us
================================



-- nondeterministic collations
-- (not supported with libc provider)

CREATE COLLATION ctest_det (locale = 'en_US.utf8', deterministic = true)
================================

CREATE COLLATION ctest_nondet (locale = 'en_US.utf8', deterministic = false)
================================

DROP SCHEMA collate_tests CASCADE
================================


================================


-- Tests for currtid2() with various relation kinds

-- Materialized view
CREATE MATERIALIZED VIEW tid_matview AS SELECT a FROM tid_tab
================================

REFRESH MATERIALIZED VIEW tid_matview
================================
 -- ok
DROP MATERIALIZED VIEW tid_matview
================================


================================


================================
--
-- CREATE_VIEW
-- Virtual class definitions
--	(this also tests the query rewrite system)
--

CREATE VIEW street AS
   SELECT r.name, r.thepath, c.cname AS cname
   FROM ONLY road r, real_city c
   WHERE c.outline ## r.thepath
================================


CREATE VIEW iexit AS
   SELECT ih.name, ih.thepath,
	interpt_pp(ih.thepath, r.thepath) AS exit
   FROM ihighway ih, ramp r
   WHERE ih.thepath ## r.thepath
================================


-- Test comments
COMMENT ON VIEW noview IS 'no view'
================================

COMMENT ON VIEW toyemp IS 'is a view'
================================

COMMENT ON VIEW toyemp IS NULL
================================

5	10
10	15
15	20
20	25
\.

CREATE OR REPLACE VIEW viewtest AS
	SELECT * FROM viewtest_tbl
================================


-- tests for temporary views

CREATE SCHEMA temp_view_test
    CREATE TABLE base_table (a int, id int)
    CREATE TABLE base_table2 (a int, id int)
================================

-- should fail
CREATE SCHEMA test_view_schema
    CREATE TEMP VIEW testview AS SELECT 1
================================


CREATE SCHEMA testviewschm2
================================

\d+ unspecified_types
SELECT * FROM unspecified_types
================================

\d+ tt1
SELECT * FROM tt1
================================


\d+ aliased_view_1
\d+ aliased_view_2
\d+ aliased_view_3
\d+ aliased_view_4

ALTER TABLE tx1 RENAME TO a1
================================


\d+ aliased_view_1
\d+ aliased_view_2
\d+ aliased_view_3
\d+ aliased_view_4

ALTER TABLE tt1 RENAME TO a2
================================


\d+ aliased_view_1
\d+ aliased_view_2
\d+ aliased_view_3
\d+ aliased_view_4

ALTER TABLE a1 RENAME TO tt1
================================


\d+ aliased_view_1
\d+ aliased_view_2
\d+ aliased_view_3
\d+ aliased_view_4

ALTER TABLE a2 RENAME TO tx1
================================


\d+ aliased_view_1
\d+ aliased_view_2
\d+ aliased_view_3
\d+ aliased_view_4

ALTER TABLE temp_view_test.tt1 RENAME TO tmp1
================================


\d+ aliased_view_1
\d+ aliased_view_2
\d+ aliased_view_3
\d+ aliased_view_4

-- Test aliasing of joins

create view view_of_joins as
select * from
  (select * from (tbl1 cross join tbl2) same) ss,
  (tbl3 cross join tbl4) same
================================


\d+ view_of_joins

create table tbl1a (a int, c int)
================================

create view view_of_joins_2d as select * from (tbl1 join tbl1a using (a) as x) as y
================================

alter view v1 rename column a to x
================================

$$
language plpgsql
================================


-- check display of whole-row variables in some corner cases

create type nestedcomposite as (x int8_tbl)
================================

select row(i.*::int8_tbl)::nestedcomposite from int8_tbl i
================================

select * from int8_tbl i, lateral(values(i.*::int8_tbl)) ss
================================

select * from int8_tbl i where i.* in (values(i.*::int8_tbl))
================================


-- reverse-listing of various special function syntaxes required by SQL

create view tt201v as
select
  extract(day from now()) as extr,
  (now(), '1 day'::interval) overlaps
    (current_timestamp(2), '1 day'::interval) as o,
  'foo' is normalized isn,
  'foo' is nfkc normalized isnn,
  normalize('foo') as n,
  normalize('foo', nfkd) as nfkd,
  overlay('foo' placing 'bar' from 2) as ovl,
  overlay('foo' placing 'bar' from 2 for 3) as ovl2,
  position('foo' in 'foobar') as p,
  substring('foo' from 2 for 3) as s,
  substring('foo' similar 'f' escape '#') as ss,
  substring('foo' from 'oo') as ssf,  -- historically-permitted abuse
  trim(' ' from ' foo ') as bt,
  trim(leading ' ' from ' foo ') as lt,
  trim(trailing ' foo ') as rt,
  trim(E'\\000'::bytea from E'\\000Tom\\000'::bytea) as btb,
  trim(leading E'\\000'::bytea from E'\\000Tom\\000'::bytea) as ltb,
  trim(trailing E'\\000'::bytea from E'\\000Tom\\000'::bytea) as rtb
================================


-- test pretty-print parenthesization rules, and SubLink deparsing

create view tt26v as
select x + y + z as c1,
       (x * y) + z as c2,
       x + (y * z) as c3,
       (x + y) * z as c4,
       x * (y + z) as c5,
       x + (y + z) as c6,
       x + (y # z) as c7,
       (x > y) AND (y > z OR x > z) as c8,
       (x > y) OR (y > z AND NOT (x > z)) as c9,
       (x,y) <> ALL (values(1,2),(3,4)) as c10,
       (x,y) <= ANY (values(1,2),(3,4)) as c11
from (values(1,2,3)) v(x,y,z)
================================


-- clean up all the random objects we made above
DROP SCHEMA temp_view_test CASCADE
================================

DROP SCHEMA testviewschm2 CASCADE
================================


================================


================================


-- The above verified that we can change the type of a multiply-inherited
-- column
================================
 but we should reject that if any definition was inherited from
-- an unrelated parent.
create temp table parent1(f1 int, f2 int)
================================

\d p1
\d c1

-- Test that child does not override inheritable constraints of the parent
create table c2 (constraint p2chk check (ff1 > 10) no inherit) inherits (p1)
================================

select p2text(c1.*) from c1
================================

\d c2
create table c3 (f4 int) inherits(c1,c2)
================================

\d c3
drop table p1 cascade
================================

\d cc1
create table cc2(f4 float) inherits(pp1,cc1)
================================

\d cc2
alter table pp1 add column a2 int check (a2 > 0)
================================

\d cc2
drop table pp1 cascade
================================

\d+ inhts

DROP TABLE inhts
================================

\d+ inht4

CREATE TABLE inhts (d int) INHERITS (inht2, inhs1)
================================
                -- to be failed
\d+ inhts

WITH RECURSIVE r AS (
  SELECT 'inht1'::regclass AS inhrelid
UNION ALL
  SELECT c.inhrelid FROM pg_inherits c, r WHERE r.inhrelid = c.inhparent
)
SELECT a.attrelid::regclass, a.attname, a.attinhcount, e.expected
  FROM (SELECT inhrelid, count(*) AS expected FROM pg_inherits
        WHERE inhparent IN (SELECT inhrelid FROM r) GROUP BY inhrelid) e
  JOIN pg_attribute a ON e.inhrelid = a.attrelid WHERE NOT attislocal
  ORDER BY a.attrelid::regclass::name, a.attnum
================================

\d+ test_constraints
ALTER TABLE ONLY test_constraints DROP CONSTRAINT test_constraints_val1_val2_key
================================

\d+ test_constraints
\d+ test_constraints_inh
DROP TABLE test_constraints_inh
================================

\d+ test_ex_constraints
ALTER TABLE test_ex_constraints DROP CONSTRAINT test_ex_constraints_c_excl
================================

\d+ test_ex_constraints
\d+ test_ex_constraints_inh
DROP TABLE test_ex_constraints_inh
================================

\d+ test_primary_constraints
\d+ test_foreign_constraints
ALTER TABLE test_foreign_constraints DROP CONSTRAINT test_foreign_constraints_id1_fkey
================================

\d+ test_foreign_constraints
\d+ test_foreign_constraints_inh
DROP TABLE test_foreign_constraints_inh
================================


-- We don't drop the invalid_check_con* tables, to test dump/reload with

--
-- Test parameterized append plans for inheritance trees
--

create temp table patest0 (id, x) as
  select x, x from generate_series(0,1000) x
================================

insert into patest1
  select x, x from generate_series(0,1000) x
================================

insert into patest2
  select x, x from generate_series(0,1000) x
================================

-- create index matest2i on matest2 ((1-id))
================================

insert into permtest_parent
  select 1, 'a', left(md5(i::text), 5) from generate_series(0, 100) i
================================


================================
 $$ LANGUAGE sql IMMUTABLE
================================


CREATE OPERATOR === (
    LEFTARG = boolean,
    RIGHTARG = boolean,
    PROCEDURE = alter_op_test_fn,
    COMMUTATOR = ===,
    NEGATOR = !==,
    RESTRICT = customcontsel,
    JOIN = contjoinsel,
    HASHES, MERGES
)
================================


--
-- Reset and set params
--

ALTER OPERATOR === (boolean, boolean) SET (RESTRICT = NONE)
================================

ALTER OPERATOR === (boolean, boolean) SET (JOIN = NONE)
================================


ALTER OPERATOR === (boolean, boolean) SET (RESTRICT = contsel)
================================

ALTER OPERATOR === (boolean, boolean) SET (JOIN = contjoinsel)
================================


ALTER OPERATOR === (boolean, boolean) SET (RESTRICT = NONE, JOIN = NONE)
================================


ALTER OPERATOR === (boolean, boolean) SET (RESTRICT = customcontsel, JOIN = contjoinsel)
================================


--
-- Test invalid options.
--
ALTER OPERATOR === (boolean, boolean) SET (COMMUTATOR = ====)
================================

ALTER OPERATOR === (boolean, boolean) SET (NEGATOR = ====)
================================

ALTER OPERATOR === (boolean, boolean) SET (RESTRICT = non_existent_func)
================================

ALTER OPERATOR === (boolean, boolean) SET (JOIN = non_existent_func)
================================

ALTER OPERATOR === (boolean, boolean) SET (COMMUTATOR = !==)
================================

ALTER OPERATOR === (boolean, boolean) SET (NEGATOR = !==)
================================


-- invalid: non-lowercase quoted identifiers
ALTER OPERATOR & (bit, bit) SET ("Restrict" = _int_contsel, "Join" = _int_contjoinsel)
================================


ALTER OPERATOR === (boolean, boolean) SET (RESTRICT = NONE)
================================

DROP OPERATOR === (boolean, boolean)
================================


================================

TRUNCATE ONLY trunc_fb, ONLY trunc_fa
================================

    insert into trunc_trigger_log values
      (TG_OP, TG_LEVEL, TG_WHEN, TG_ARGV[0], tg_table_name, c)
================================

    return null
================================

$$ LANGUAGE plpgsql
================================


DROP TRIGGER t ON trunc_trigger_test
================================


INSERT INTO truncate_a DEFAULT VALUES
================================

INSERT INTO truncate_a DEFAULT VALUES
================================


INSERT INTO truncate_a DEFAULT VALUES
================================

INSERT INTO truncate_a DEFAULT VALUES
================================


INSERT INTO truncate_a DEFAULT VALUES
================================

INSERT INTO truncate_a DEFAULT VALUES
================================


INSERT INTO truncate_b DEFAULT VALUES
================================

INSERT INTO truncate_b DEFAULT VALUES
================================


INSERT INTO truncate_b DEFAULT VALUES
================================

INSERT INTO truncate_b DEFAULT VALUES
================================


INSERT INTO truncate_b DEFAULT VALUES
================================

INSERT INTO truncate_b DEFAULT VALUES
================================

INSERT INTO truncate_a DEFAULT VALUES
================================

INSERT INTO truncate_a DEFAULT VALUES
================================

INSERT INTO truncate_a DEFAULT VALUES
================================


================================


================================

insert into gin_test_tbl select array[1, 2, g] from generate_series(1, 20000) g
================================

insert into gin_test_tbl select array[1, 3, g] from generate_series(1, 1000) g
================================
 -- flush the fastupdate buffers

insert into gin_test_tbl select array[3, 1, g] from generate_series(1, 1000) g
================================


insert into gin_test_tbl select array[1, 2, g] from generate_series(1, 1000) g
================================

insert into gin_test_tbl select array[1, 3, g] from generate_series(1, 1000) g
================================

  return query execute 'EXPLAIN (ANALYZE, FORMAT json) ' || query_sql
================================

$$
================================

  return query execute query_sql
================================

$$
================================

  return query execute query_sql
================================

$$
================================


-- check number of rows returned by index and removed by recheck
select
  query,
  js->0->'Plan'->'Plans'->0->'Actual Rows' as "return by index",
  js->0->'Plan'->'Rows Removed by Index Recheck' as "removed by recheck",
  (res_index = res_heap) as "match"
from
  (values
    ($$ i @> '{}' $$),
    ($$ j @> '{}' $$),
    ($$ i @> '{}' and j @> '{}' $$),
    ($$ i @> '{1}' $$),
    ($$ i @> '{1}' and j @> '{}' $$),
    ($$ i @> '{1}' and i @> '{}' and j @> '{}' $$),
    ($$ j @> '{10}' $$),
    ($$ j @> '{10}' and i @> '{}' $$),
    ($$ j @> '{10}' and j @> '{}' and i @> '{}' $$),
    ($$ i @> '{1}' and j @> '{10}' $$)
  ) q(query),
  lateral explain_query_json($$select * from t_gin_test_tbl where $$ || query) js,
  lateral execute_text_query_index($$select string_agg((i, j)::text, ' ') from t_gin_test_tbl where $$ || query) res_index,
  lateral execute_text_query_heap($$select string_agg((i, j)::text, ' ') from t_gin_test_tbl where $$ || query) res_heap
================================


-- re-purpose t_gin_test_tbl to test scans involving posting trees
insert into t_gin_test_tbl select array[1, g, g/10], array[2, g, g/10]
  from generate_series(1, 20000) g
================================


================================


INSERT INTO NAME_TBL(f1) VALUES ('asdfghjkl
================================
')
================================


DO $$
DECLARE r text[]
================================

  RAISE NOTICE '%', format('%I.%I', r[1], r[2])
================================

  r := parse_ident('"SchemaX"."TableY"')
================================

  RAISE NOTICE '%', format('%I.%I', r[1], r[2])
================================

$$
================================
 -- should fail
SELECT parse_ident('foo.boo[]', strict => false)
================================


SELECT length(a[1]), length(a[2]) from parse_ident('"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx".yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy') as a 
================================


================================


================================
--
-- LINE
-- Infinite lines
--

--DROP TABLE LINE_TBL
================================

-- vertical
INSERT INTO LINE_TBL VALUES (line(point '(3,1)', point '(3,2)'))
================================


INSERT INTO LINE_TBL VALUES (line(point '(1,0)', point '(1,0)'))
================================


select '{nan, 1, nan}'::line = '{nan, 1, nan}'::line as true,
	   '{nan, 1, nan}'::line = '{nan, 2, nan}'::line as false
================================


================================

\d test_replica_identity

-- succeed, nondeferrable unique constraint over nonnullable cols
ALTER TABLE test_replica_identity REPLICA IDENTITY USING INDEX test_replica_identity_unique_nondefer
================================

\d test_replica_identity
SELECT count(*) FROM pg_index WHERE indrelid = 'test_replica_identity'::regclass AND indisreplident
================================

\d+ test_replica_identity
ALTER TABLE test_replica_identity REPLICA IDENTITY NOTHING
================================

\d test_replica_identity2
ALTER TABLE test_replica_identity2 ALTER COLUMN id TYPE bigint
================================

\d test_replica_identity2

-- straight index variant
CREATE TABLE test_replica_identity3 (id int NOT NULL)
================================

\d test_replica_identity3
ALTER TABLE test_replica_identity3 ALTER COLUMN id TYPE bigint
================================

\d test_replica_identity3

DROP TABLE test_replica_identity
================================


================================


-- get test coverage for "single value" deduplication strategy:
insert into btree_bpchar select 'foo' from generate_series(1,1500)
================================

-- Generate enough garbage tuples in index to ensure that even the unique index
-- with deduplication enabled has to check multiple leaf pages during unique
-- checking (at least with a BLCKSZ of 8192 or less)
DO $$
BEGIN
    FOR r IN 1..1350 LOOP
        DELETE FROM dedup_unique_test_table
================================

        INSERT INTO dedup_unique_test_table SELECT 1
================================

END$$
================================

INSERT INTO dedup_unique_test_table SELECT i FROM generate_series(0,450) i
================================

insert into btree_tall_tbl select g, repeat('x', 250)
from generate_series(1, 130) g
================================

INSERT INTO delete_test_table SELECT i, 1, 2, 3 FROM generate_series(1,80000) i
================================


--
-- Test B-tree insertion with a metapage update (XLOG_BTREE_INSERT_META
-- WAL record type). This happens when a "fast root" page is split.  This
-- also creates coverage for nbtree FSM page recycling.
--
-- The vacuum above should've turned the leaf page into a fast root. We just
-- need to insert some rows to cause the fast root page to split.
INSERT INTO delete_test_table SELECT i, 1, 2, 3 FROM generate_series(1,1000) i
================================


================================


explain (costs off)
select x from (values (100::money), (200::money)) _(x) union select x from (values (100::money), (300::money)) _(x)
================================


explain (costs off)
select x from (values (100::money), (200::money)) _(x) union select x from (values (100::money), (300::money)) _(x)
================================


-- non-hashable type
explain (costs off)
select x from (values (array[100::money]), (array[200::money])) _(x) union select x from (values (array[100::money]), (array[300::money])) _(x)
================================

select x from (values (array[100::money]), (array[200::money])) _(x) union select x from (values (array[100::money]), (array[300::money])) _(x)
================================


-- non-hashable type

-- With an anonymous row type, the typcache does not report that the
-- type is hashable.  (Otherwise, this would fail at execution time.)
explain (costs off)
select x from (values (row(100::money)), (row(200::money))) _(x) union select x from (values (row(100::money)), (row(300::money))) _(x)
================================

select x from (values (row(100::money)), (row(200::money))) _(x) union select x from (values (row(100::money)), (row(300::money))) _(x)
================================


-- With a defined row type, the typcache can inspect the type's fields
-- for hashability.
create type ct1 as (f1 money)
================================

explain (costs off)
select x from (values (row(100::money)::ct1), (row(200::money)::ct1)) _(x) union select x from (values (row(100::money)::ct1), (row(300::money)::ct1)) _(x)
================================

select x from (values (row(100::money)::ct1), (row(200::money)::ct1)) _(x) union select x from (values (row(100::money)::ct1), (row(300::money)::ct1)) _(x)
================================

drop type ct1
================================
 end$$
================================


create temp table t3 as select generate_series(-1000,1000) as x
================================


================================
CREATE OPERATOR === (
        PROCEDURE = int8eq,
        LEFTARG = bigint,
        RIGHTARG = bigint,
        COMMUTATOR = ===
)
================================


CREATE OPERATOR !== (
        PROCEDURE = int8ne,
        LEFTARG = bigint,
        RIGHTARG = bigint,
        NEGATOR = ===,
        COMMUTATOR = !==
)
================================


DROP OPERATOR !==(bigint, bigint)
================================


DROP OPERATOR ===(bigint, bigint)
================================


CREATE OPERATOR <| (
        PROCEDURE = int8lt,
        LEFTARG = bigint,
        RIGHTARG = bigint
)
================================


CREATE OPERATOR |> (
        PROCEDURE = int8gt,
        LEFTARG = bigint,
        RIGHTARG = bigint,
        NEGATOR = <|,
        COMMUTATOR = <|
)
================================


DROP OPERATOR |>(bigint, bigint)
================================


DROP OPERATOR <|(bigint, bigint)
================================


================================
 end$$ language plpgsql parallel restricted
================================

-- test prefetching, if the platform allows it
DO $$
BEGIN
 SET effective_io_concurrency = 50
================================

EXCEPTION WHEN invalid_parameter_value THEN
END $$
================================

insert into bmscantest select r, 'fooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo' FROM generate_series(1,100000) r
================================

        return next ln
================================

$$
================================

$$ language plpgsql PARALLEL SAFE
================================

$$
================================

    x[2] := $2
================================

    return x
================================

  end$$ language plpgsql parallel safe
================================


================================

INSERT INTO tbl_include_reg SELECT x, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

\d tbl_include_reg_idx

-- Unique index and unique constraint
CREATE TABLE tbl_include_unique1 (c1 int, c2 int, c3 int, c4 box)
================================

INSERT INTO tbl_include_unique1 SELECT x, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_include_unique2 SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_include_pk SELECT 1, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_include_box SELECT 1, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl_include_box_pk SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

-- ensure that constraint works
INSERT INTO tbl SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

-- ensure that constraint works
INSERT INTO tbl SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT 1, NULL, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT x, 2*x, NULL, NULL FROM generate_series(1,300) AS x
================================

-- ensure that constraint works
INSERT INTO tbl SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

-- ensure that constraint works
INSERT INTO tbl SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT 1, NULL, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT x, 2*x, NULL, NULL FROM generate_series(1,10) AS x
================================

-- ensure that constraint works
INSERT INTO tbl SELECT 1, 2, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT x, 2*x, NULL, NULL FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT x, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,1000) AS x
================================

REINDEX INDEX tbl_c1_c2_c3_c4_key
================================

INSERT INTO tbl SELECT x, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

INSERT INTO tbl SELECT x, 2*x, 3*x, box('4,4,4,4') FROM generate_series(1,10) AS x
================================

\d tbl
DROP TABLE tbl
================================


================================


--
-- sanity check, if we don't have indices the test will take years to
-- complete.  But skip TOAST relations (since they will have varying
-- names depending on the current OID counter) as well as temp tables
-- of other backends (to avoid timing-dependent behavior).
--

-- temporarily disable fancy output, so catalog changes create less diff noise
\a\t

SELECT relname, relhasindex
   FROM pg_class c LEFT JOIN pg_namespace n ON n.oid = relnamespace
   WHERE relkind IN ('r', 'p') AND (nspname ~ '^pg_temp_') IS NOT TRUE
   ORDER BY relname
================================


-- restore normal output mode
\a\t

--
-- another sanity check: every system catalog that has OIDs should have
-- a unique index on OID.  This ensures that the OIDs will be unique,
-- even after the OID counter wraps around.
-- We exclude non-system tables from the check by looking at nspname.
--
SELECT relname, nspname
 FROM pg_class c LEFT JOIN pg_namespace n ON n.oid = relnamespace JOIN pg_attribute a ON (attrelid = c.oid AND attname = 'oid')
 WHERE relkind = 'r' and c.oid < 16384
     AND ((nspname ~ '^pg_') IS NOT FALSE)
     AND NOT EXISTS (SELECT 1 FROM pg_index i WHERE indrelid = c.oid
                     AND indkey[0] = a.attnum AND indnatts = 1
                     AND indisunique AND indimmediate)
================================


================================


CREATE INDEX IF NOT EXISTS ON onek USING btree(unique1 int4_ops)
================================


-- test comments
COMMENT ON INDEX six_wrong IS 'bad index'
================================

COMMENT ON INDEX six IS 'good index'
================================

COMMENT ON INDEX six IS NULL
================================


CREATE TEMP TABLE gpolygon_tbl AS
    SELECT polygon(home_base) AS f1 FROM slow_emp4000
================================


CREATE TEMP TABLE gcircle_tbl AS
    SELECT circle(home_base) AS f1 FROM slow_emp4000
================================


SELECT * FROM fast_emp4000
    WHERE home_base <@ '(200,200),(2000,1000)'::box
    ORDER BY (home_base[0])[0]
================================


SELECT count(*) FROM fast_emp4000 WHERE home_base && '(1000,1000,0,0)'::box
================================


SELECT * FROM polygon_tbl WHERE f1 @> '((1,1),(2,2),(2,1))'::polygon
    ORDER BY (poly_center(f1))[0]
================================


SELECT * FROM circle_tbl WHERE f1 && circle(point(1,-2), 1)
    ORDER BY area(f1)
================================


SELECT count(*) FROM gpolygon_tbl WHERE f1 && '(1000,1000,0,0)'::polygon
================================


SELECT count(*) FROM gcircle_tbl WHERE f1 && '<(500,500),500>'::circle
================================


SELECT count(*) FROM point_tbl WHERE box '(0,0,100,100)' @> f1
================================


SELECT count(*) FROM point_tbl p WHERE p.f1 << '(0.0, 0.0)'
================================


SELECT count(*) FROM point_tbl p WHERE p.f1 >> '(0.0, 0.0)'
================================


SELECT count(*) FROM point_tbl p WHERE p.f1 <<| '(0.0, 0.0)'
================================


SELECT count(*) FROM point_tbl p WHERE p.f1 |>> '(0.0, 0.0)'
================================


SELECT count(*) FROM point_tbl p WHERE p.f1 ~= '(-5, -12)'
================================


SELECT * FROM point_tbl WHERE f1 <@ '(-10,-10),(10,10)':: box ORDER BY f1 <-> '0,1'
================================


SELECT * FROM gpolygon_tbl ORDER BY f1 <-> '(0,0)'::point LIMIT 10
================================


SELECT circle_center(f1), round(radius(f1)) as radius FROM gcircle_tbl ORDER BY f1 <-> '(200,300)'::point LIMIT 10
================================


EXPLAIN (COSTS OFF)
SELECT * FROM fast_emp4000
    WHERE home_base <@ '(200,200),(2000,1000)'::box
    ORDER BY (home_base[0])[0]
================================

SELECT * FROM fast_emp4000
    WHERE home_base <@ '(200,200),(2000,1000)'::box
    ORDER BY (home_base[0])[0]
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM fast_emp4000 WHERE home_base && '(1000,1000,0,0)'::box
================================

SELECT count(*) FROM fast_emp4000 WHERE home_base && '(1000,1000,0,0)'::box
================================


EXPLAIN (COSTS OFF)
SELECT * FROM polygon_tbl WHERE f1 @> '((1,1),(2,2),(2,1))'::polygon
    ORDER BY (poly_center(f1))[0]
================================

SELECT * FROM polygon_tbl WHERE f1 @> '((1,1),(2,2),(2,1))'::polygon
    ORDER BY (poly_center(f1))[0]
================================


EXPLAIN (COSTS OFF)
SELECT * FROM circle_tbl WHERE f1 && circle(point(1,-2), 1)
    ORDER BY area(f1)
================================

SELECT * FROM circle_tbl WHERE f1 && circle(point(1,-2), 1)
    ORDER BY area(f1)
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM gpolygon_tbl WHERE f1 && '(1000,1000,0,0)'::polygon
================================

SELECT count(*) FROM gpolygon_tbl WHERE f1 && '(1000,1000,0,0)'::polygon
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM gcircle_tbl WHERE f1 && '<(500,500),500>'::circle
================================

SELECT count(*) FROM gcircle_tbl WHERE f1 && '<(500,500),500>'::circle
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM point_tbl WHERE box '(0,0,100,100)' @> f1
================================

SELECT count(*) FROM point_tbl WHERE box '(0,0,100,100)' @> f1
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM point_tbl p WHERE p.f1 << '(0.0, 0.0)'
================================

SELECT count(*) FROM point_tbl p WHERE p.f1 << '(0.0, 0.0)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM point_tbl p WHERE p.f1 >> '(0.0, 0.0)'
================================

SELECT count(*) FROM point_tbl p WHERE p.f1 >> '(0.0, 0.0)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM point_tbl p WHERE p.f1 <<| '(0.0, 0.0)'
================================

SELECT count(*) FROM point_tbl p WHERE p.f1 <<| '(0.0, 0.0)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM point_tbl p WHERE p.f1 |>> '(0.0, 0.0)'
================================

SELECT count(*) FROM point_tbl p WHERE p.f1 |>> '(0.0, 0.0)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM point_tbl p WHERE p.f1 ~= '(-5, -12)'
================================

SELECT count(*) FROM point_tbl p WHERE p.f1 ~= '(-5, -12)'
================================


EXPLAIN (COSTS OFF)
SELECT * FROM point_tbl WHERE f1 <@ '(-10,-10),(10,10)':: box ORDER BY f1 <-> '0,1'
================================

SELECT * FROM point_tbl WHERE f1 <@ '(-10,-10),(10,10)':: box ORDER BY f1 <-> '0,1'
================================


EXPLAIN (COSTS OFF)
SELECT * FROM gpolygon_tbl ORDER BY f1 <-> '(0,0)'::point LIMIT 10
================================

SELECT * FROM gpolygon_tbl ORDER BY f1 <-> '(0,0)'::point LIMIT 10
================================


EXPLAIN (COSTS OFF)
SELECT circle_center(f1), round(radius(f1)) as radius FROM gcircle_tbl ORDER BY f1 <-> '(200,300)'::point LIMIT 10
================================

SELECT circle_center(f1), round(radius(f1)) as radius FROM gcircle_tbl ORDER BY f1 <-> '(200,300)'::point LIMIT 10
================================


EXPLAIN (COSTS OFF)
SELECT * FROM point_tbl WHERE f1 <@ '(-10,-10),(10,10)':: box ORDER BY f1 <-> '0,1'
================================

SELECT * FROM point_tbl WHERE f1 <@ '(-10,-10),(10,10)':: box ORDER BY f1 <-> '0,1'
================================


INSERT INTO array_gin_test SELECT ARRAY[1, g%5, g] FROM generate_series(1, 10000) g
================================

\d+ gin_relopts_test

--
-- HASH
--
CREATE INDEX hash_i4_index ON hash_i4_heap USING hash (random int4_ops)
================================


-- CREATE INDEX hash_ovfl_index ON hash_ovfl_heap USING hash (x int4_ops)
================================


-- while we're here, see that the metadata looks sane
\d func_index_heap
\d func_index_index


--
-- Same test, expressional index
--
DROP TABLE func_index_heap
================================


-- while we're here, see that the metadata looks sane
\d func_index_heap
\d func_index_index

-- this should fail because of unsafe column type (anonymous record)
create index on func_index_heap ((f1 || f2), (row(f1, f2)))
================================

  RETURN true
================================
 $$
================================

REINDEX TABLE concur_heap
================================

\d concur_heap
REINDEX TABLE concur_heap
================================

\d concur_heap

-- Temporary tables with concurrent builds and on-commit actions
-- CONCURRENTLY used with CREATE INDEX and DROP INDEX is ignored.
-- PRESERVE ROWS, the default.
CREATE TEMP TABLE concur_temp (f1 int, f2 text)
  ON COMMIT PRESERVE ROWS
================================


\d concur_heap

DROP TABLE concur_heap
================================


\d cwi_test
\d cwi_uniq_idx

CREATE UNIQUE INDEX cwi_uniq2_idx ON cwi_test(b , a)
================================


\d cwi_test
\d cwi_replaced_pkey

DROP INDEX cwi_replaced_pkey
================================
	-- Should fail
================================
 a constraint depends on it

-- Check that non-default index options are rejected
CREATE UNIQUE INDEX cwi_uniq3_idx ON cwi_test(a desc)
================================


--
-- Tests for IS NULL/IS NOT NULL with b-tree indexes
--

CREATE TABLE onek_with_null AS SELECT unique1, unique2 FROM onek
================================


--
-- Check behavior with duplicate index column contents
--

CREATE TABLE dupindexcols AS
  SELECT unique1 as id, stringu2::text as f1 FROM tenk1
================================


EXPLAIN (COSTS OFF)
  SELECT count(*) FROM dupindexcols
    WHERE f1 BETWEEN 'WA' AND 'ZZZ' and id < 1000 and f1 ~<~ 'YX'
================================

SELECT count(*) FROM dupindexcols
  WHERE f1 BETWEEN 'WA' AND 'ZZZ' and id < 1000 and f1 ~<~ 'YX'
================================

\set VERBOSITY terse \\ -- suppress machine-dependent details
REINDEX (VERBOSE) TABLE reindex_verbose
================================

\set VERBOSITY default
DROP TABLE reindex_verbose
================================

-- REINDEX
REINDEX TABLE concur_reindex_tab
================================
 -- notice
REINDEX (CONCURRENTLY) TABLE concur_reindex_tab
================================

REINDEX INDEX CONCURRENTLY  concur_reindex_tab3_c2_excl
================================
  -- error
REINDEX TABLE CONCURRENTLY concur_reindex_tab3
================================

-- Check materialized views
CREATE MATERIALIZED VIEW concur_reindex_matview AS SELECT * FROM concur_reindex_tab
================================

REINDEX INDEX CONCURRENTLY concur_reindex_ind1
================================

REINDEX TABLE CONCURRENTLY concur_reindex_tab
================================

REINDEX TABLE CONCURRENTLY concur_reindex_matview
================================

COMMENT ON INDEX testcomment_idx1 IS 'test comment'
================================

REINDEX TABLE testcomment
================================

REINDEX TABLE CONCURRENTLY testcomment 
================================

REINDEX TABLE CONCURRENTLY concur_clustered
================================

REINDEX TABLE CONCURRENTLY concur_replident
================================

REINDEX TABLE CONCURRENTLY concur_appclass_tab
================================

\d concur_appclass_tab
DROP TABLE concur_appclass_tab
================================

REINDEX INDEX CONCURRENTLY concur_reindex_part_index_0_1
================================

REINDEX INDEX CONCURRENTLY concur_reindex_part_index_0_2
================================

REINDEX TABLE CONCURRENTLY concur_reindex_part_0_1
================================

REINDEX TABLE CONCURRENTLY concur_reindex_part_0_2
================================


-- REINDEX for partitioned indexes
-- REINDEX TABLE fails for partitioned indexes
-- Top-most parent index
REINDEX TABLE concur_reindex_part_index
================================
 -- error
REINDEX TABLE CONCURRENTLY concur_reindex_part_index
================================
 -- error
-- Partitioned index with no leaves
REINDEX TABLE concur_reindex_part_index_10
================================
 -- error
REINDEX TABLE CONCURRENTLY concur_reindex_part_index_10
================================

REINDEX INDEX concur_reindex_part_index
================================
',
	 relname, indname)
================================
', tabname)
================================

REINDEX INDEX concur_reindex_part_index
================================

REINDEX INDEX CONCURRENTLY concur_reindex_part_index
================================


-- REINDEX for partitioned tables
-- REINDEX INDEX fails for partitioned tables
-- Top-most parent
REINDEX INDEX concur_reindex_part
================================
 -- error
REINDEX INDEX CONCURRENTLY concur_reindex_part
================================
 -- error
-- Partitioned with no leaves
REINDEX INDEX concur_reindex_part_10
================================
 -- error
REINDEX INDEX CONCURRENTLY concur_reindex_part_10
================================

REINDEX TABLE concur_reindex_part
================================

REINDEX TABLE concur_reindex_part
================================

REINDEX TABLE CONCURRENTLY concur_reindex_part
================================

REINDEX TABLE CONCURRENTLY concur_reindex_tab
================================

REINDEX TABLE CONCURRENTLY pg_class
================================
 -- no catalog relation
REINDEX INDEX CONCURRENTLY pg_class_oid_index
================================
 -- no catalog index
-- These are the toast table and index of pg_authid.
REINDEX TABLE CONCURRENTLY pg_toast.pg_toast_1260
================================
 -- no catalog toast table
REINDEX INDEX CONCURRENTLY pg_toast.pg_toast_1260_index
================================
 -- no catalog toast index
REINDEX SYSTEM CONCURRENTLY postgres
================================
 -- not allowed for SYSTEM
-- Warns about catalog relations
REINDEX SCHEMA CONCURRENTLY pg_catalog
================================


-- Check the relation status, there should not be invalid indexes
\d concur_reindex_tab
DROP MATERIALIZED VIEW concur_reindex_matview
================================

-- Reindexing concurrently this index fails with the same failure.
-- The extra index created is itself invalid, and can be dropped.
REINDEX INDEX CONCURRENTLY concur_reindex_ind5
================================

\d concur_reindex_tab4
DROP INDEX concur_reindex_ind5_ccnew
================================

-- The invalid index is not processed when running REINDEX TABLE.
REINDEX TABLE CONCURRENTLY concur_reindex_tab4
================================

\d concur_reindex_tab4
-- But it is fixed with REINDEX INDEX.
REINDEX INDEX CONCURRENTLY concur_reindex_ind5
================================

\d concur_reindex_tab4
DROP TABLE concur_reindex_tab4
================================

REINDEX TABLE CONCURRENTLY concur_exprs_tab
================================

REINDEX TABLE CONCURRENTLY concur_temp_tab_1
================================

REINDEX INDEX CONCURRENTLY concur_temp_ind_1
================================

REINDEX INDEX CONCURRENTLY concur_temp_ind_1
================================

REINDEX TABLE CONCURRENTLY concur_temp_tab_2
================================

REINDEX INDEX CONCURRENTLY concur_temp_ind_2
================================

-- Fails when running in a transaction
REINDEX INDEX CONCURRENTLY concur_temp_ind_3
================================

-- REINDEX SCHEMA processes all temporary relations
CREATE TABLE reindex_temp_before AS
SELECT oid, relname, relfilenode, relkind, reltoastrelid
  FROM pg_class
  WHERE relname IN ('concur_temp_ind_1', 'concur_temp_ind_2')
================================


--
-- REINDEX SCHEMA
--
REINDEX SCHEMA schema_to_reindex
================================
 -- failure, schema does not exist
CREATE SCHEMA schema_to_reindex
================================

INSERT INTO table1 SELECT generate_series(1,400)
================================

INSERT INTO table2 SELECT generate_series(1,400), 'abc'
================================

CREATE MATERIALIZED VIEW matview AS SELECT col1 FROM table2
================================

CREATE TABLE reindex_before AS
SELECT oid, relname, relfilenode, relkind, reltoastrelid
	FROM pg_class
	where relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'schema_to_reindex')
================================

INSERT INTO reindex_before
SELECT oid, 'pg_toast_TABLE', relfilenode, relkind, reltoastrelid
FROM pg_class WHERE oid IN
	(SELECT reltoastrelid FROM reindex_before WHERE reltoastrelid > 0)
================================

INSERT INTO reindex_before
SELECT oid, 'pg_toast_TABLE_index', relfilenode, relkind, reltoastrelid
FROM pg_class where oid in
	(select indexrelid from pg_index where indrelid in
		(select reltoastrelid from reindex_before where reltoastrelid > 0))
================================

REINDEX SCHEMA schema_to_reindex
================================

CREATE TABLE reindex_after AS SELECT oid, relname, relfilenode, relkind
	FROM pg_class
	where relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'schema_to_reindex')
================================

REINDEX SCHEMA schema_to_reindex
================================

REINDEX SCHEMA schema_to_reindex
================================


-- concurrently
REINDEX SCHEMA CONCURRENTLY schema_to_reindex
================================

REINDEX SCHEMA schema_to_reindex
================================

REINDEX TABLE pg_toast.pg_toast_1260
================================

REINDEX INDEX pg_toast.pg_toast_1260_index
================================

DROP SCHEMA schema_to_reindex CASCADE
================================


================================


-- overlap
SELECT b.f1
   FROM BOX_TBL b
   WHERE b.f1 && box '(2.5,2.5,1.0,1.0)'
================================


-- left-or-overlap (x only)
SELECT b1.*
   FROM BOX_TBL b1
   WHERE b1.f1 &< box '(2.0,2.0,2.5,2.5)'
================================


-- right-or-overlap (x only)
SELECT b1.*
   FROM BOX_TBL b1
   WHERE b1.f1 &> box '(2.0,2.0,2.5,2.5)'
================================


-- left of
SELECT b.f1
   FROM BOX_TBL b
   WHERE b.f1 << box '(3.0,3.0,5.0,5.0)'
================================


-- area <=
SELECT b.f1
   FROM BOX_TBL b
   WHERE b.f1 <= box '(3.0,3.0,5.0,5.0)'
================================


-- area <
SELECT b.f1
   FROM BOX_TBL b
   WHERE b.f1 < box '(3.0,3.0,5.0,5.0)'
================================


-- area =
SELECT b.f1
   FROM BOX_TBL b
   WHERE b.f1 = box '(3.0,3.0,5.0,5.0)'
================================


-- area >
SELECT b.f1
   FROM BOX_TBL b				-- zero area
   WHERE b.f1 > box '(3.5,3.0,4.5,3.0)'
================================


-- area >=
SELECT b.f1
   FROM BOX_TBL b				-- zero area
   WHERE b.f1 >= box '(3.5,3.0,4.5,3.0)'
================================


-- right of
SELECT b.f1
   FROM BOX_TBL b
   WHERE box '(3.0,3.0,5.0,5.0)' >> b.f1
================================


-- contains
SELECT b.f1
   FROM BOX_TBL b
   WHERE box '(0,0,3,3)' @> b.f1
================================


-- box equality
SELECT b.f1
   FROM BOX_TBL b
   WHERE box '(1,1,3,3)' ~= b.f1
================================


-- wholly-contained
SELECT b1.*, b2.*
   FROM BOX_TBL b1, BOX_TBL b2
   WHERE b1.f1 @> b2.f1 and not b1.f1 ~= b2.f1
================================


INSERT INTO box_temp
	SELECT box(point(i, i), point(i * 2, i * 2))
	FROM generate_series(1, 50) AS i
================================


SELECT * FROM box_temp WHERE f1 << '(10,20),(30,40)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 << '(10,20),(30,40)'
================================


SELECT * FROM box_temp WHERE f1 &< '(10,4.333334),(5,100)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 &< '(10,4.333334),(5,100)'
================================


SELECT * FROM box_temp WHERE f1 &> '(40,30),(45,50)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 &> '(40,30),(45,50)'
================================


SELECT * FROM box_temp WHERE f1 >> '(30,40),(40,30)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 >> '(30,40),(40,30)'
================================


SELECT * FROM box_temp WHERE f1 <<| '(10,4.33334),(5,100)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 <<| '(10,4.33334),(5,100)'
================================


SELECT * FROM box_temp WHERE f1 &<| '(10,4.3333334),(5,1)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 &<| '(10,4.3333334),(5,1)'
================================


SELECT * FROM box_temp WHERE f1 |&> '(49.99,49.99),(49.99,49.99)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 |&> '(49.99,49.99),(49.99,49.99)'
================================


SELECT * FROM box_temp WHERE f1 |>> '(37,38),(39,40)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 |>> '(37,38),(39,40)'
================================


SELECT * FROM box_temp WHERE f1 ~= '(20,20),(40,40)'
================================

EXPLAIN (COSTS OFF) SELECT * FROM box_temp WHERE f1 ~= '(20,20),(40,40)'
================================


INSERT INTO quad_box_tbl
  SELECT (x - 1) * 100 + y, box(point(x * 10, y * 10), point(x * 10 + 5, y * 10 + 5))
  FROM generate_series(1, 100) x,
       generate_series(1, 100) y
================================


-- insert repeating data to test allTheSame
INSERT INTO quad_box_tbl
  SELECT i, '((200, 300),(210, 310))'
  FROM generate_series(10001, 11000) AS i
================================


CREATE TABLE quad_box_tbl_ord_seq1 AS
SELECT rank() OVER (ORDER BY b <-> point '123,456') n, b <-> point '123,456' dist, id
FROM quad_box_tbl
================================


CREATE TABLE quad_box_tbl_ord_seq2 AS
SELECT rank() OVER (ORDER BY b <-> point '123,456') n, b <-> point '123,456' dist, id
FROM quad_box_tbl WHERE b <@ box '((200,300),(500,600))'
================================


SELECT count(*) FROM quad_box_tbl WHERE b <<  box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b &<  box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b &&  box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b &>  box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b >>  box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b >>  box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b <<| box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b &<| box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b |&> box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b |>> box '((100,200),(300,500))'
================================

SELECT count(*) FROM quad_box_tbl WHERE b ~=  box '((200,300),(205,305))'
================================


EXPLAIN (COSTS OFF)
SELECT rank() OVER (ORDER BY b <-> point '123,456') n, b <-> point '123,456' dist, id
FROM quad_box_tbl
================================


CREATE TEMP TABLE quad_box_tbl_ord_idx1 AS
SELECT rank() OVER (ORDER BY b <-> point '123,456') n, b <-> point '123,456' dist, id
FROM quad_box_tbl
================================



EXPLAIN (COSTS OFF)
SELECT rank() OVER (ORDER BY b <-> point '123,456') n, b <-> point '123,456' dist, id
FROM quad_box_tbl WHERE b <@ box '((200,300),(500,600))'
================================


CREATE TEMP TABLE quad_box_tbl_ord_idx2 AS
SELECT rank() OVER (ORDER BY b <-> point '123,456') n, b <-> point '123,456' dist, id
FROM quad_box_tbl WHERE b <@ box '((200,300),(500,600))'
================================


================================
--
-- LSEG
-- Line segments
--

--DROP TABLE LSEG_TBL
================================

INSERT INTO LSEG_TBL VALUES (lseg(point(11, 22), point(33,44)))
================================


================================


-- Test comments
COMMENT ON CONSTRAINT constrname_wrong ON FKTABLE IS 'fk constraint comment'
================================

COMMENT ON CONSTRAINT constrname ON FKTABLE IS 'fk constraint comment'
================================

COMMENT ON CONSTRAINT constrname ON FKTABLE IS NULL
================================

create table pktable(ptest1 inet, ptest2 inet[], primary key(base1, ptest1), foreign key(base2, ptest2) references
                                             pktable(base1, ptest1)) inherits (pktable_base)
================================


-- these updates would leave lingering rows in the referencing table
================================
 disallow
UPDATE fk_notpartitioned_pk SET b = 502 WHERE a = 500
================================

-- check psql behavior
\d fk_notpartitioned_pk
ALTER TABLE fk_partitioned_fk DROP CONSTRAINT fk_partitioned_fk_a_b_fkey
================================

-- constraint should still be there
\d fk_partitioned_fk_2
================================

-- should have only one constraint
\d fk_partitioned_fk_2
DROP TABLE fk_partitioned_fk_2
================================

-- should only have one constraint
\d fk_partitioned_fk_4
\d fk_partitioned_fk_4_1
-- this one has an FK with mismatched properties
\d fk_partitioned_fk_4_2

CREATE TABLE fk_partitioned_fk_5 (a int, b int,
	FOREIGN KEY (a, b) REFERENCES fk_notpartitioned_pk(a, b) ON UPDATE CASCADE ON DELETE CASCADE DEFERRABLE,
	FOREIGN KEY (a, b) REFERENCES fk_notpartitioned_pk(a, b) MATCH FULL ON UPDATE CASCADE ON DELETE CASCADE)
  PARTITION BY RANGE (a)
================================

-- this one has two constraints, similar but not quite the one in the parent,
-- so it gets a new one
\d fk_partitioned_fk_5
-- verify that it works to reattaching a child with multiple candidate
-- constraints
ALTER TABLE fk_partitioned_fk_5 DETACH PARTITION fk_partitioned_fk_5_1
================================

\d fk_partitioned_fk_5_1

-- verify that attaching a table checks that the existing data satisfies the
-- constraint
CREATE TABLE fk_partitioned_fk_2 (a int, b int) PARTITION BY RANGE (b)
================================

insert into other_partitioned_fk
  select 2048, x from generate_series(1,10) x
================================

insert into fk_notpartitioned_pk (a, b)
  select 2048, x from generate_series(1,10) x
================================


-- Test creating a constraint at the parent that already exists in partitions.
-- There should be no duplicated constraints, and attempts to drop the
-- constraint in partitions should raise appropriate errors.
create schema fkpart0
  create table pkey (a int primary key)
  create table fk_part (a int) partition by list (a)
  create table fk_part_1 partition of fk_part
      (foreign key (a) references fkpart0.pkey) for values in (1)
  create table fk_part_23 partition of fk_part
      (foreign key (a) references fkpart0.pkey) for values in (2, 3)
      partition by list (a)
  create table fk_part_23_2 partition of fk_part_23 for values in (2)
================================

\d fkpart0.fk_part_1	\\ -- should have only one FK
alter table fkpart0.fk_part_1 drop constraint fk_part_1_a_fkey
================================


\d fkpart0.fk_part_23	\\ -- should have only one FK
\d fkpart0.fk_part_23_2	\\ -- should have only one FK
alter table fkpart0.fk_part_23 drop constraint fk_part_23_a_fkey
================================

\d fkpart0.fk_part_4
alter table fkpart0.fk_part_4 drop constraint fk_part_a_fkey
================================

\d fkpart0.fk_part_56
alter table fkpart0.fk_part_56 drop constraint fk_part_a_fkey
================================


-- verify that attaching and detaching partitions maintains the right set of
-- triggers
create schema fkpart1
  create table pkey (a int primary key)
  create table fk_part (a int) partition by list (a)
  create table fk_part_1 partition of fk_part for values in (1) partition by list (a)
  create table fk_part_1_1 partition of fk_part_1 for values in (1)
================================


-- verify that attaching and detaching partitions manipulates the inheritance
-- properties of their FK constraints correctly
create schema fkpart2
  create table pkey (a int primary key)
  create table fk_part (a int, constraint fkey foreign key (a) references fkpart2.pkey) partition by list (a)
  create table fk_part_1 partition of fkpart2.fk_part for values in (1) partition by list (a)
  create table fk_part_1_1 (a int, constraint my_fkey foreign key (a) references fkpart2.pkey)
================================
	-- doesn't exist

-- verify constraint deferrability
create schema fkpart3
  create table pkey (a int primary key)
  create table fk_part (a int, constraint fkey foreign key (a) references fkpart3.pkey deferrable initially immediate) partition by list (a)
  create table fk_part_1 partition of fkpart3.fk_part for values in (1) partition by list (a)
  create table fk_part_1_1 partition of fkpart3.fk_part_1 for values in (1)
  create table fk_part_2 partition of fkpart3.fk_part for values in (2)
================================


drop schema fkpart0, fkpart1, fkpart2, fkpart3 cascade
================================


-- Test a partitioned table as referenced table.

-- Verify basic functionality with a regular partition creation and a partition
-- with a different column layout, as well as partitions added (created and
-- attached) after creating the foreign key.
CREATE SCHEMA fkpart3
================================


CREATE SCHEMA fkpart4
================================


-- Verify that initial constraint creation and cloning behave correctly
CREATE SCHEMA fkpart5
================================


-- Verify constraint deferrability
CREATE SCHEMA fkpart9
================================


DROP SCHEMA fkpart9 CASCADE
================================


-- Verify ON UPDATE/DELETE behavior
CREATE SCHEMA fkpart6
================================


-- test for reported bug: relispartition not set
-- https://postgr.es/m/CA+HiwqHMsRtRYRWYTWavKJ8x14AFsv7bmAV46mYwnfD3vy8goQ@mail.gmail.com
CREATE SCHEMA fkpart7
  CREATE TABLE pkpart (a int) PARTITION BY LIST (a)
  CREATE TABLE pkpart1 PARTITION OF pkpart FOR VALUES IN (1)
================================

DROP SCHEMA fkpart7 CASCADE
================================


-- ensure we check partitions are "not used" when dropping constraints
CREATE SCHEMA fkpart8
  CREATE TABLE tbl1(f1 int PRIMARY KEY)
  CREATE TABLE tbl2(f1 int REFERENCES tbl1 DEFERRABLE INITIALLY DEFERRED) PARTITION BY RANGE(f1)
  CREATE TABLE tbl2_p1 PARTITION OF tbl2 FOR VALUES FROM (minvalue) TO (maxvalue)
================================

DROP SCHEMA fkpart8 CASCADE
================================


-- ensure FK referencing a multi-level partitioned table are
-- enforce reference to sub-children.
CREATE SCHEMA fkpart9
  CREATE TABLE pk (a INT PRIMARY KEY) PARTITION BY RANGE (a)
  CREATE TABLE fk (
    fk_a INT REFERENCES pk(a) ON DELETE CASCADE
  )
  CREATE TABLE pk1 PARTITION OF pk FOR VALUES FROM (30) TO (50) PARTITION BY RANGE (a)
  CREATE TABLE pk11 PARTITION OF pk1 FOR VALUES FROM (30) TO (40)
================================

DROP SCHEMA fkpart9 CASCADE
================================


-- test that ri_Check_Pk_Match() scans the correct partition for a deferred
-- ON DELETE/UPDATE NO ACTION constraint
CREATE SCHEMA fkpart10
  CREATE TABLE tbl1(f1 int PRIMARY KEY) PARTITION BY RANGE(f1)
  CREATE TABLE tbl1_p1 PARTITION OF tbl1 FOR VALUES FROM (minvalue) TO (1)
  CREATE TABLE tbl1_p2 PARTITION OF tbl1 FOR VALUES FROM (1) TO (maxvalue)
  CREATE TABLE tbl2(f1 int REFERENCES tbl1 DEFERRABLE INITIALLY DEFERRED)
================================

DROP SCHEMA fkpart10 CASCADE
================================


================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

INSERT INTO vactst SELECT * FROM vactst
================================

CLUSTER vaccluster
================================


-- Test ANALYZE in transaction, where the transaction surrounding
-- analyze performed modifications. This tests for the bug at
-- https://postgr.es/m/c7988239-d42c-ddc4-41db-171b23b35e4f%40ssinger.info
-- (which hopefully is unlikely to be reintroduced), but also seems
-- independently worthwhile to cover.
INSERT INTO vactst SELECT generate_series(1, 300)
================================

INSERT INTO vactst SELECT generate_series(301, 400)
================================

INSERT INTO pvactst SELECT i, array[1,2,3], point(i, i+1) FROM generate_series(1,1000) i
================================

-- Only toast index is cleaned up.
ALTER TABLE no_index_cleanup SET (vacuum_index_cleanup = off,
    toast.vacuum_index_cleanup = yes)
================================

-- Only parent is cleaned up.
ALTER TABLE no_index_cleanup SET (vacuum_index_cleanup = true,
    toast.vacuum_index_cleanup = false)
================================

INSERT INTO vacparted_i SELECT i, 'test_'|| i from generate_series(1,10) i
================================

ANALYZE (nonexistent-arg) does_not_exist
================================


================================


-- schema

DROP SCHEMA test_schema_exists
================================


DROP SCHEMA IF EXISTS test_schema_exists
================================


CREATE SCHEMA test_schema_exists
================================


DROP SCHEMA IF EXISTS test_schema_exists
================================


DROP SCHEMA test_schema_exists
================================


-- type

DROP TYPE test_type_exists
================================


DROP TYPE IF EXISTS test_type_exists
================================


CREATE type test_type_exists as (a int, b text)
================================


DROP TYPE IF EXISTS test_type_exists
================================


DROP TYPE test_type_exists
================================

CREATE GROUP regress_test_g1
================================


DROP GROUP regress_test_g2
================================


DROP GROUP IF EXISTS regress_test_g1, regress_test_g2
================================


DROP GROUP regress_test_g1
================================


-- collation
DROP COLLATION IF EXISTS test_collation_exists
================================


-- conversion
DROP CONVERSION test_conversion_exists
================================

DROP CONVERSION IF EXISTS test_conversion_exists
================================

CREATE CONVERSION test_conversion_exists
    FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

DROP CONVERSION test_conversion_exists
================================


-- text search parser
DROP TEXT SEARCH PARSER test_tsparser_exists
================================

DROP TEXT SEARCH PARSER IF EXISTS test_tsparser_exists
================================


-- text search dictionary
DROP TEXT SEARCH DICTIONARY test_tsdict_exists
================================

DROP TEXT SEARCH DICTIONARY IF EXISTS test_tsdict_exists
================================

CREATE TEXT SEARCH DICTIONARY test_tsdict_exists (
        Template=ispell,
        DictFile=ispell_sample,
        AffFile=ispell_sample
)
================================

DROP TEXT SEARCH DICTIONARY test_tsdict_exists
================================


-- test search template
DROP TEXT SEARCH TEMPLATE test_tstemplate_exists
================================

DROP TEXT SEARCH TEMPLATE IF EXISTS test_tstemplate_exists
================================


-- text search configuration
DROP TEXT SEARCH CONFIGURATION test_tsconfig_exists
================================

DROP TEXT SEARCH CONFIGURATION IF EXISTS test_tsconfig_exists
================================

CREATE TEXT SEARCH CONFIGURATION test_tsconfig_exists (COPY=english)
================================

DROP TEXT SEARCH CONFIGURATION test_tsconfig_exists
================================


-- extension
DROP EXTENSION test_extension_exists
================================

DROP EXTENSION IF EXISTS test_extension_exists
================================


-- aggregate
DROP AGGREGATE test_aggregate_exists(*)
================================

DROP AGGREGATE IF EXISTS test_aggregate_exists(*)
================================


DROP AGGREGATE test_aggregate_exists(int)
================================

DROP AGGREGATE IF EXISTS test_aggregate_exists(int)
================================


-- operator
DROP OPERATOR @#@ (int, int)
================================

DROP OPERATOR IF EXISTS @#@ (int, int)
================================

CREATE OPERATOR @#@
        (leftarg = int8, rightarg = int8, procedure = int8xor)
================================

DROP OPERATOR @#@ (int8, int8)
================================


-- language
DROP LANGUAGE test_language_exists
================================

DROP LANGUAGE IF EXISTS test_language_exists
================================


-- cast
DROP CAST (text AS text)
================================

DROP CAST IF EXISTS (text AS text)
================================


-- trigger
DROP TRIGGER test_trigger_exists ON test_exists
================================

DROP TRIGGER IF EXISTS test_trigger_exists ON test_exists
================================


DROP TRIGGER test_trigger_exists ON no_such_table
================================

DROP TRIGGER IF EXISTS test_trigger_exists ON no_such_table
================================


DROP TRIGGER test_trigger_exists ON no_such_schema.no_such_table
================================

DROP TRIGGER IF EXISTS test_trigger_exists ON no_such_schema.no_such_table
================================

DROP TRIGGER test_trigger_exists ON test_exists
================================


-- rule
DROP RULE test_rule_exists ON test_exists
================================

DROP RULE IF EXISTS test_rule_exists ON test_exists
================================


DROP RULE test_rule_exists ON no_such_table
================================

DROP RULE IF EXISTS test_rule_exists ON no_such_table
================================


DROP RULE test_rule_exists ON no_such_schema.no_such_table
================================

DROP RULE IF EXISTS test_rule_exists ON no_such_schema.no_such_table
================================

DROP RULE test_rule_exists ON test_exists
================================


-- foreign data wrapper
DROP FOREIGN DATA WRAPPER test_fdw_exists
================================

DROP FOREIGN DATA WRAPPER IF EXISTS test_fdw_exists
================================


-- foreign server
DROP SERVER test_server_exists
================================

DROP SERVER IF EXISTS test_server_exists
================================


-- operator class
DROP OPERATOR CLASS test_operator_class USING btree
================================

DROP OPERATOR CLASS IF EXISTS test_operator_class USING btree
================================


DROP OPERATOR CLASS test_operator_class USING no_such_am
================================

DROP OPERATOR CLASS IF EXISTS test_operator_class USING no_such_am
================================


-- operator family
DROP OPERATOR FAMILY test_operator_family USING btree
================================

DROP OPERATOR FAMILY IF EXISTS test_operator_family USING btree
================================


DROP OPERATOR FAMILY test_operator_family USING no_such_am
================================

DROP OPERATOR FAMILY IF EXISTS test_operator_family USING no_such_am
================================


-- access method
DROP ACCESS METHOD no_such_am
================================

DROP ACCESS METHOD IF EXISTS no_such_am
================================


-- be tolerant with missing schemas, types, etc

DROP AGGREGATE IF EXISTS no_such_schema.foo(int)
================================

DROP AGGREGATE IF EXISTS foo(no_such_type)
================================

DROP AGGREGATE IF EXISTS foo(no_such_schema.no_such_type)
================================

DROP CAST IF EXISTS (INTEGER AS no_such_type2)
================================

DROP CAST IF EXISTS (no_such_type1 AS INTEGER)
================================

DROP CAST IF EXISTS (INTEGER AS no_such_schema.bar)
================================

DROP CAST IF EXISTS (no_such_schema.foo AS INTEGER)
================================

DROP COLLATION IF EXISTS no_such_schema.foo
================================

DROP CONVERSION IF EXISTS no_such_schema.foo
================================

DROP FOREIGN TABLE IF EXISTS no_such_schema.foo
================================

DROP MATERIALIZED VIEW IF EXISTS no_such_schema.foo
================================

DROP OPERATOR IF EXISTS no_such_schema.+ (int, int)
================================

DROP OPERATOR IF EXISTS + (no_such_type, no_such_type)
================================

DROP OPERATOR IF EXISTS + (no_such_schema.no_such_type, no_such_schema.no_such_type)
================================

DROP OPERATOR IF EXISTS # (NONE, no_such_schema.no_such_type)
================================

DROP OPERATOR CLASS IF EXISTS no_such_schema.widget_ops USING btree
================================

DROP OPERATOR FAMILY IF EXISTS no_such_schema.float_ops USING btree
================================

DROP RULE IF EXISTS foo ON no_such_schema.bar
================================

DROP TEXT SEARCH CONFIGURATION IF EXISTS no_such_schema.foo
================================

DROP TEXT SEARCH DICTIONARY IF EXISTS no_such_schema.foo
================================

DROP TEXT SEARCH PARSER IF EXISTS no_such_schema.foo
================================

DROP TEXT SEARCH TEMPLATE IF EXISTS no_such_schema.foo
================================

DROP TRIGGER IF EXISTS foo ON no_such_schema.bar
================================

DROP TYPE IF EXISTS no_such_schema.foo
================================
 $$ language sql
================================
 $$ language sql
================================
 $$ language plpgsql
================================
 $$ language plpgsql
================================


-- Check we get a similar error if we use ROUTINE instead of PROCEDURE.
DROP ROUTINE IF EXISTS test_ambiguous_procname
================================


================================


--Base tsvector test

SELECT '1'::tsvector
================================

SELECT '1 '::tsvector
================================

SELECT ' 1'::tsvector
================================

SELECT ' 1 '::tsvector
================================

SELECT '1 2'::tsvector
================================

SELECT '''1 2'''::tsvector
================================

SELECT E'''1 \\''2'''::tsvector
================================

SELECT E'''1 \\''2''3'::tsvector
================================

SELECT E'''1 \\''2'' 3'::tsvector
================================

SELECT E'''1 \\''2'' '' 3'' 4 '::tsvector
================================

SELECT tsvectorin(tsvectorout($$'\\as' ab\c ab\\c AB\\\c ab\\\\c$$::tsvector))
================================

SELECT 'a:3A b:2a'::tsvector || 'ba:1234 a:1B'
================================
  -- error, empty lexeme is not allowed

--Base tsquery test
SELECT '1'::tsquery
================================

SELECT '1 '::tsquery
================================

SELECT ' 1'::tsquery
================================

SELECT ' 1 '::tsquery
================================

SELECT '''1 2'''::tsquery
================================

SELECT E'''1 \\''2'''::tsquery
================================

SELECT '!1'::tsquery
================================

SELECT '1|2'::tsquery
================================

SELECT '1|!2'::tsquery
================================

SELECT '!1|2'::tsquery
================================

SELECT '!1|!2'::tsquery
================================

SELECT '!(!1|!2)'::tsquery
================================

SELECT '!(!1|2)'::tsquery
================================

SELECT '!(1|!2)'::tsquery
================================

SELECT '!(1|2)'::tsquery
================================

SELECT '1&2'::tsquery
================================

SELECT '!1&2'::tsquery
================================

SELECT '1&!2'::tsquery
================================

SELECT '!1&!2'::tsquery
================================

SELECT '(1&2)'::tsquery
================================

SELECT '1&(2)'::tsquery
================================

SELECT '!(1)&2'::tsquery
================================

SELECT '!(1&2)'::tsquery
================================

SELECT '1|2&3'::tsquery
================================

SELECT '1|(2&3)'::tsquery
================================

SELECT '(1|2)&3'::tsquery
================================

SELECT '1|2&!3'::tsquery
================================

SELECT '1|!2&3'::tsquery
================================

SELECT '!1|2&3'::tsquery
================================

SELECT '!1|(2&3)'::tsquery
================================

SELECT '!(1|2)&3'::tsquery
================================

SELECT '(!1|2)&3'::tsquery
================================

SELECT '1|(2|(4|(5|6)))'::tsquery
================================

SELECT '1|2|4|5|6'::tsquery
================================

SELECT '1&(2&(4&(5&6)))'::tsquery
================================

SELECT '1&2&4&5&6'::tsquery
================================

SELECT '1&(2&(4&(5|6)))'::tsquery
================================

SELECT '1&(2&(4&(5|!6)))'::tsquery
================================

SELECT E'1&(''2''&('' 4''&(\\|5 | ''6 \\'' !|&'')))'::tsquery
================================

SELECT 'a:* & nbb:*ac | doo:a* | goo'::tsquery
================================

SELECT '!!b'::tsquery
================================

SELECT '!!!b'::tsquery
================================

SELECT '!(!b)'::tsquery
================================

SELECT 'a & !!b'::tsquery
================================

SELECT '!!a & b'::tsquery
================================

SELECT '!!a & !!b'::tsquery
================================


--comparisons
SELECT 'a' < 'b & c'::tsquery as "true"
================================

SELECT 'a' > 'b & c'::tsquery as "false"
================================

SELECT 'a | f' < 'b & c'::tsquery as "false"
================================

SELECT 'a | ff' < 'b & c'::tsquery as "false"
================================

SELECT 'a | f | g' < 'b & c'::tsquery as "false"
================================


--concatenation
SELECT numnode( 'new'::tsquery )
================================

SELECT numnode( 'new & york'::tsquery )
================================

SELECT numnode( 'new & york | qwery'::tsquery )
================================


SELECT 'foo & bar'::tsquery && 'asd'
================================

SELECT 'foo & bar'::tsquery || 'asd & fg'
================================

SELECT 'foo & bar'::tsquery || !!'asd & fg'::tsquery
================================

SELECT 'foo & bar'::tsquery && 'asd | fg'
================================

SELECT 'a' <-> 'b & d'::tsquery
================================

SELECT 'a & g' <-> 'b & d'::tsquery
================================

SELECT 'a & g' <-> 'b | d'::tsquery
================================

SELECT 'a & g' <-> 'b <-> d'::tsquery
================================


-- tsvector-tsquery operations

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & ca' as "true"
================================

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & ca:B' as "true"
================================

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & ca:A' as "true"
================================

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & ca:C' as "false"
================================

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & ca:CB' as "true"
================================

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & c:*C' as "false"
================================

SELECT 'a b:89  ca:23A,64b d:34c'::tsvector @@ 'd:AC & c:*CB' as "true"
================================

SELECT 'a b:89  ca:23A,64b cb:80c d:34c'::tsvector @@ 'd:AC & c:*C' as "true"
================================

SELECT 'a b:89  ca:23A,64c cb:80b d:34c'::tsvector @@ 'd:AC & c:*C' as "true"
================================

SELECT 'a b:89  ca:23A,64c cb:80b d:34c'::tsvector @@ 'd:AC & c:*B' as "true"
================================

SELECT 'wa:1D wb:2A'::tsvector @@ 'w:*D & w:*A'::tsquery as "true"
================================

SELECT 'wa:1D wb:2A'::tsvector @@ 'w:*D <-> w:*A'::tsquery as "true"
================================

SELECT 'wa:1A wb:2D'::tsvector @@ 'w:*D <-> w:*A'::tsquery as "false"
================================

SELECT 'wa:1A'::tsvector @@ 'w:*A'::tsquery as "true"
================================

SELECT 'wa:1A'::tsvector @@ 'w:*D'::tsquery as "false"
================================

SELECT 'wa:1A'::tsvector @@ '!w:*A'::tsquery as "false"
================================

SELECT 'wa:1A'::tsvector @@ '!w:*D'::tsquery as "true"
================================

-- historically, a stripped tsvector matches queries ignoring weights:
SELECT strip('wa:1A'::tsvector) @@ 'w:*A'::tsquery as "true"
================================

SELECT strip('wa:1A'::tsvector) @@ 'w:*D'::tsquery as "true"
================================

SELECT strip('wa:1A'::tsvector) @@ '!w:*A'::tsquery as "false"
================================

SELECT strip('wa:1A'::tsvector) @@ '!w:*D'::tsquery as "false"
================================


SELECT 'supernova'::tsvector @@ 'super'::tsquery AS "false"
================================

SELECT 'supeanova supernova'::tsvector @@ 'super'::tsquery AS "false"
================================

SELECT 'supeznova supernova'::tsvector @@ 'super'::tsquery AS "false"
================================

SELECT 'supernova'::tsvector @@ 'super:*'::tsquery AS "true"
================================

SELECT 'supeanova supernova'::tsvector @@ 'super:*'::tsquery AS "true"
================================

SELECT 'supeznova supernova'::tsvector @@ 'super:*'::tsquery AS "true"
================================


--ranking
SELECT ts_rank(' a:1 s:2C d g'::tsvector, 'a | s')
================================

SELECT ts_rank(' a:1 sa:2C d g'::tsvector, 'a | s')
================================

SELECT ts_rank(' a:1 sa:2C d g'::tsvector, 'a | s:*')
================================

SELECT ts_rank(' a:1 sa:2C d g'::tsvector, 'a | sa:*')
================================

SELECT ts_rank(' a:1 s:2B d g'::tsvector, 'a | s')
================================

SELECT ts_rank(' a:1 s:2 d g'::tsvector, 'a | s')
================================

SELECT ts_rank(' a:1 s:2C d g'::tsvector, 'a & s')
================================

SELECT ts_rank(' a:1 s:2B d g'::tsvector, 'a & s')
================================

SELECT ts_rank(' a:1 s:2 d g'::tsvector, 'a & s')
================================


SELECT ts_rank_cd(' a:1 s:2C d g'::tsvector, 'a | s')
================================

SELECT ts_rank_cd(' a:1 sa:2C d g'::tsvector, 'a | s')
================================

SELECT ts_rank_cd(' a:1 sa:2C d g'::tsvector, 'a | s:*')
================================

SELECT ts_rank_cd(' a:1 sa:2C d g'::tsvector, 'a | sa:*')
================================

SELECT ts_rank_cd(' a:1 sa:3C sab:2c d g'::tsvector, 'a | sa:*')
================================

SELECT ts_rank_cd(' a:1 s:2B d g'::tsvector, 'a | s')
================================

SELECT ts_rank_cd(' a:1 s:2 d g'::tsvector, 'a | s')
================================

SELECT ts_rank_cd(' a:1 s:2C d g'::tsvector, 'a & s')
================================

SELECT ts_rank_cd(' a:1 s:2B d g'::tsvector, 'a & s')
================================

SELECT ts_rank_cd(' a:1 s:2 d g'::tsvector, 'a & s')
================================


SELECT ts_rank_cd(' a:1 s:2A d g'::tsvector, 'a <-> s')
================================

SELECT ts_rank_cd(' a:1 s:2C d g'::tsvector, 'a <-> s')
================================

SELECT ts_rank_cd(' a:1 s:2 d g'::tsvector, 'a <-> s')
================================

SELECT ts_rank_cd(' a:1 s:2 d:2A g'::tsvector, 'a <-> s')
================================

SELECT ts_rank_cd(' a:1 s:2,3A d:2A g'::tsvector, 'a <2> s:A')
================================

SELECT ts_rank_cd(' a:1 b:2 s:3A d:2A g'::tsvector, 'a <2> s:A')
================================

SELECT ts_rank_cd(' a:1 sa:2D sb:2A g'::tsvector, 'a <-> s:*')
================================

SELECT ts_rank_cd(' a:1 sa:2A sb:2D g'::tsvector, 'a <-> s:*')
================================

SELECT ts_rank_cd(' a:1 sa:2A sb:2D g'::tsvector, 'a <-> s:* <-> sa:A')
================================

SELECT ts_rank_cd(' a:1 sa:2A sb:2D g'::tsvector, 'a <-> s:* <-> sa:B')
================================


SELECT 'a:1 b:2'::tsvector @@ 'a <-> b'::tsquery AS "true"
================================

SELECT 'a:1 b:2'::tsvector @@ 'a <0> b'::tsquery AS "false"
================================

SELECT 'a:1 b:2'::tsvector @@ 'a <1> b'::tsquery AS "true"
================================

SELECT 'a:1 b:2'::tsvector @@ 'a <2> b'::tsquery AS "false"
================================

SELECT 'a:1 b:3'::tsvector @@ 'a <-> b'::tsquery AS "false"
================================

SELECT 'a:1 b:3'::tsvector @@ 'a <0> b'::tsquery AS "false"
================================

SELECT 'a:1 b:3'::tsvector @@ 'a <1> b'::tsquery AS "false"
================================

SELECT 'a:1 b:3'::tsvector @@ 'a <2> b'::tsquery AS "true"
================================

SELECT 'a:1 b:3'::tsvector @@ 'a <3> b'::tsquery AS "false"
================================

SELECT 'a:1 b:3'::tsvector @@ 'a <0> a:*'::tsquery AS "true"
================================


-- tsvector editing operations

SELECT strip('w:12B w:13* w:12,5,6 a:1,3* a:3 w asd:1dc asd'::tsvector)
================================

SELECT strip('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector)
================================

SELECT strip('base hidden rebel spaceship strike'::tsvector)
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, 'base')
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, 'bas')
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, 'bases')
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, 'spaceship')
================================

SELECT ts_delete('base hidden rebel spaceship strike'::tsvector, 'spaceship')
================================


SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, ARRAY['spaceship','rebel'])
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, ARRAY['spaceships','rebel'])
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, ARRAY['spaceshi','rebel'])
================================

SELECT ts_delete('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector, ARRAY['spaceship','leya','rebel'])
================================

SELECT ts_delete('base hidden rebel spaceship strike'::tsvector, ARRAY['spaceship','leya','rebel'])
================================

SELECT ts_delete('base hidden rebel spaceship strike'::tsvector, ARRAY['spaceship','leya','rebel','rebel'])
================================

SELECT ts_delete('base hidden rebel spaceship strike'::tsvector, ARRAY['spaceship','leya','rebel', '', NULL])
================================


SELECT unnest('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector)
================================

SELECT unnest('base hidden rebel spaceship strike'::tsvector)
================================

SELECT * FROM unnest('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector)
================================

SELECT * FROM unnest('base hidden rebel spaceship strike'::tsvector)
================================

SELECT lexeme, positions[1] from unnest('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector)
================================


SELECT tsvector_to_array('base:7 hidden:6 rebel:1 spaceship:2,33A,34B,35C,36D strike:3'::tsvector)
================================

SELECT tsvector_to_array('base hidden rebel spaceship strike'::tsvector)
================================


SELECT setweight('w:12B w:13* w:12,5,6 a:1,3* a:3 w asd:1dc asd zxc:81,567,222A'::tsvector, 'c')
================================

SELECT setweight('a:1,3A asd:1C w:5,6,12B,13A zxc:81,222A,567'::tsvector, 'c')
================================

SELECT setweight('a:1,3A asd:1C w:5,6,12B,13A zxc:81,222A,567'::tsvector, 'c', '{a}')
================================

SELECT setweight('a:1,3A asd:1C w:5,6,12B,13A zxc:81,222A,567'::tsvector, 'c', '{a}')
================================

SELECT setweight('a:1,3A asd:1C w:5,6,12B,13A zxc:81,222A,567'::tsvector, 'c', '{a,zxc}')
================================

SELECT setweight('a asd w:5,6,12B,13A zxc'::tsvector, 'c', ARRAY['a', 'zxc', '', NULL])
================================


SELECT ts_filter('base:7A empir:17 evil:15 first:11 galact:16 hidden:6A rebel:1A spaceship:2A strike:3A victori:12 won:9'::tsvector, '{a}')
================================

SELECT ts_filter('base hidden rebel spaceship strike'::tsvector, '{a}')
================================

SELECT ts_filter('base hidden rebel spaceship strike'::tsvector, '{a,b,NULL}')
================================


================================

-- subtract time from date should not make sense
================================
 use interval instead
SELECT date '1991-02-03' - time '04:05:06' AS "Subtract Time"
================================


-- Shorthand values
-- Not directly usable for regression testing since these are not constants.
-- So, just try to test parser and hope for the best - thomas 97/04/26
SELECT (timestamp without time zone 'today' = (timestamp without time zone 'yesterday' + interval '1 day')) as "True"
================================

SELECT (timestamp without time zone 'today' = (timestamp without time zone 'tomorrow' - interval '1 day')) as "True"
================================

SELECT (timestamp without time zone 'today 10:30' = (timestamp without time zone 'yesterday' + interval '1 day 10 hr 30 min')) as "True"
================================

SELECT (timestamp without time zone '10:30 today' = (timestamp without time zone 'yesterday' + interval '1 day 10 hr 30 min')) as "True"
================================

SELECT (timestamp without time zone 'tomorrow' = (timestamp without time zone 'yesterday' + interval '2 days')) as "True"
================================

SELECT (timestamp without time zone 'tomorrow 16:00:00' = (timestamp without time zone 'today' + interval '1 day 16 hours')) as "True"
================================

SELECT (timestamp without time zone '16:00:00 tomorrow' = (timestamp without time zone 'today' + interval '1 day 16 hours')) as "True"
================================

SELECT (timestamp without time zone 'yesterday 12:34:56' = (timestamp without time zone 'tomorrow' - interval '2 days - 12:34:56')) as "True"
================================

SELECT (timestamp without time zone '12:34:56 yesterday' = (timestamp without time zone 'tomorrow' - interval '2 days - 12:34:56')) as "True"
================================

SELECT (timestamp without time zone 'tomorrow' > 'now') as "True"
================================

SELECT timestamptz(date '1994-01-01', time with time zone '11:00-5') AS "Jan_01_1994_8am"
================================


SELECT d1 + interval '1 year' AS one_year FROM TIMESTAMP_TBL
================================


SELECT (timestamp with time zone 'today' = (timestamp with time zone 'yesterday' + interval '1 day')) as "True"
================================

SELECT (timestamp with time zone 'today' = (timestamp with time zone 'tomorrow' - interval '1 day')) as "True"
================================

SELECT (timestamp with time zone 'tomorrow' = (timestamp with time zone 'yesterday' + interval '2 days')) as "True"
================================

SELECT (timestamp with time zone 'tomorrow' > 'now') as "True"
================================

RESET TIME ZONE
================================



SELECT timestamptz(date '1994-01-01', time '11:00') AS "Jan_01_1994_10am"
================================

SELECT timestamptz(date '1994-01-01', time '10:00') AS "Jan_01_1994_9am"
================================

SELECT timestamptz(date '1994-01-01', time with time zone '11:00-8') AS "Jan_01_1994_11am"
================================

SELECT timestamptz(date '1994-01-01', time with time zone '10:00-8') AS "Jan_01_1994_10am"
================================

SELECT timestamptz(date '1994-01-01', time with time zone '11:00-5') AS "Jan_01_1994_8am"
================================


SELECT d1 + interval '1 year' AS one_year FROM TIMESTAMPTZ_TBL
================================


--
-- time, interval arithmetic
--

SELECT CAST(time '01:02' AS interval) AS "+01:02"
================================

SELECT CAST(interval '02:03' AS time) AS "02:03:00"
================================

SELECT CAST(time with time zone '01:02-08' AS interval) AS "+00:01"
================================

SELECT CAST(interval '02:03' AS time with time zone) AS "02:03:00-08"
================================


-- These two tests cannot be used because they default to current timezone,
-- which may be either -08 or -07 depending on the time of year.
-- SELECT time with time zone '01:30' + interval '02:01' AS "03:31:00-08"
================================

-- SELECT time with time zone '03:30' + interval '1 month 04:01' AS "07:31:00-08"
================================

-- Try the following two tests instead, as a poor substitute

SELECT CAST(CAST(date 'today' + time with time zone '05:30'
            + interval '02:01' AS time with time zone) AS time) AS "07:31:00"
================================


SELECT CAST(cast(date 'today' + time with time zone '03:30'
  + interval '1 month 04:01' as timestamp without time zone) AS time) AS "07:31:00"
================================


-- SQL9x OVERLAPS operator
-- test with time zone
SELECT (timestamp with time zone '2000-11-27', timestamp with time zone '2000-11-28')
  OVERLAPS (timestamp with time zone '2000-11-27 12:00', timestamp with time zone '2000-11-30') AS "True"
================================


SELECT (timestamp with time zone '2000-11-26', timestamp with time zone '2000-11-27')
  OVERLAPS (timestamp with time zone '2000-11-27 12:00', timestamp with time zone '2000-11-30') AS "False"
================================


SELECT (timestamp with time zone '2000-11-27', timestamp with time zone '2000-11-28')
  OVERLAPS (timestamp with time zone '2000-11-27 12:00', interval '1 day') AS "True"
================================


SELECT (timestamp with time zone '2000-11-27', interval '12 hours')
  OVERLAPS (timestamp with time zone '2000-11-27 12:00', timestamp with time zone '2000-11-30') AS "False"
================================


SELECT (timestamp with time zone '2000-11-27', interval '12 hours')
  OVERLAPS (timestamp with time zone '2000-11-27', interval '12 hours') AS "True"
================================


SELECT (timestamp with time zone '2000-11-27', interval '12 hours')
  OVERLAPS (timestamp with time zone '2000-11-27 12:00', interval '12 hours') AS "False"
================================


-- test without time zone
SELECT (timestamp without time zone '2000-11-27', timestamp without time zone '2000-11-28')
  OVERLAPS (timestamp without time zone '2000-11-27 12:00', timestamp without time zone '2000-11-30') AS "True"
================================


SELECT (timestamp without time zone '2000-11-26', timestamp without time zone '2000-11-27')
  OVERLAPS (timestamp without time zone '2000-11-27 12:00', timestamp without time zone '2000-11-30') AS "False"
================================


SELECT (timestamp without time zone '2000-11-27', timestamp without time zone '2000-11-28')
  OVERLAPS (timestamp without time zone '2000-11-27 12:00', interval '1 day') AS "True"
================================


SELECT (timestamp without time zone '2000-11-27', interval '12 hours')
  OVERLAPS (timestamp without time zone '2000-11-27 12:00', timestamp without time zone '2000-11-30') AS "False"
================================


SELECT (timestamp without time zone '2000-11-27', interval '12 hours')
  OVERLAPS (timestamp without time zone '2000-11-27', interval '12 hours') AS "True"
================================


SELECT (timestamp without time zone '2000-11-27', interval '12 hours')
  OVERLAPS (timestamp without time zone '2000-11-27 12:00', interval '12 hours') AS "False"
================================


-- test time and interval
SELECT (time '00:00', time '01:00')
  OVERLAPS (time '00:30', time '01:30') AS "True"
================================


SELECT (time '00:00', interval '1 hour')
  OVERLAPS (time '00:30', interval '1 hour') AS "True"
================================


SELECT (time '00:00', interval '1 hour')
  OVERLAPS (time '01:30', interval '1 hour') AS "False"
================================


-- SQL99 seems to want this to be false (and we conform to the spec).
-- istm that this *should* return true, on the theory that time
-- intervals can wrap around the day boundary - thomas 2001-09-25
SELECT (time '00:00', interval '1 hour')
  OVERLAPS (time '01:30', interval '1 day') AS "False"
================================


-- get some candidate input values

INSERT INTO TEMP_TIMESTAMP (f1)
  SELECT d1 FROM TIMESTAMP_TBL
  WHERE d1 BETWEEN '13-jun-1957' AND '1-jan-1997'
   OR d1 BETWEEN '1-jan-1999' AND '1-jan-2010'
================================


SELECT d.f1 AS "timestamp",
   timestamp with time zone '1980-01-06 00:00 GMT' AS gpstime_zero,
   d.f1 - timestamp with time zone '1980-01-06 00:00 GMT' AS difference
  FROM TEMP_TIMESTAMP d
  ORDER BY difference
================================


--
-- Conversions
--

SELECT f1 AS "timestamp", date(f1) AS date
  FROM TEMP_TIMESTAMP
  WHERE f1 <> timestamp 'now'
  ORDER BY date, "timestamp"
================================


RESET TIME ZONE
================================


================================
--
-- TEXT
--

SELECT text 'this is a text string' = text 'this is a text string' AS true
================================


SELECT text 'this is a text string' = text 'this is a text strin' AS false
================================


================================


--
-- Test write operations that has an underlying query that is eligible
-- for parallel plans
--
explain (costs off) create table parallel_write as
    select length(stringu1) from tenk1 group by length(stringu1)
================================

create table parallel_write as
    select length(stringu1) from tenk1 group by length(stringu1)
================================


explain (costs off) create materialized view parallel_mat_view as
    select length(stringu1) from tenk1 group by length(stringu1)
================================

create materialized view parallel_mat_view as
    select length(stringu1) from tenk1 group by length(stringu1)
================================

refresh materialized view parallel_mat_view
================================

refresh materialized view concurrently parallel_mat_view
================================

drop materialized view parallel_mat_view
================================

explain (costs off) create table parallel_write as execute prep_stmt
================================

create table parallel_write as execute prep_stmt
================================


================================


SELECT name, #thepath FROM iexit ORDER BY name COLLATE "C", 2
================================
 RETURN true
================================


CREATE TABLE credit_usage (
       cid      int references customer(cid),
       ymd      date,
       usage    int
)
================================

EXECUTE p1
================================

EXECUTE p2
================================

ALTER VIEW my_property_normal SET (security_barrier=true)
================================

ALTER VIEW my_property_secure SET (security_barrier=false)
================================

EXECUTE p1
================================
		-- To be perform as a view with security-barrier
EXECUTE p2
================================


================================
/*
 * This test is intended to pass on all platforms supported by Postgres.
 * We can therefore only assume that the default, C, and POSIX collations
 * are available --- and since the regression tests are often run in a
 * C-locale database, these may well all have the same behavior.  But
 * fortunately, the system doesn't know that and will treat them as
 * incompatible collations.  It is therefore at least possible to test
 * parser behaviors such as collation conflict resolution.  This test will,
 * however, be more revealing when run in a database with non-C locale,
 * since any departure from C sorting behavior will show as a failure.
 */

CREATE SCHEMA collate_tests
================================


\d collate_test1

CREATE TABLE collate_test_fail (
    a int COLLATE "C",
    b text
)
================================


\d collate_test_like

CREATE TABLE collate_test2 (
    a int,
    b text COLLATE "POSIX"
)
================================

INSERT INTO collate_test2 SELECT * FROM collate_test1
================================

INSERT INTO collate_test4 SELECT * FROM collate_test1
================================

INSERT INTO collate_test5 SELECT * FROM collate_test1
================================
 -- fail

CREATE TABLE test_u AS SELECT a, b FROM collate_test1 UNION ALL SELECT a, b FROM collate_test2
================================


-- casting

SELECT CAST('42' AS text COLLATE "C")
================================



-- CREATE/DROP COLLATION

CREATE COLLATION mycoll1 FROM "C"
================================

CREATE COLLATION mycoll2 ( LC_COLLATE = "POSIX", LC_CTYPE = "POSIX" )
================================

CREATE COLLATION mycoll3 FROM "default"
================================
  -- intentionally unsupported

DROP COLLATION mycoll1
================================

DROP COLLATION mycoll2
================================
  -- fail

-- invalid: non-lowercase quoted identifiers
CREATE COLLATION case_coll ("Lc_Collate" = "POSIX", "Lc_Ctype" = "POSIX")
================================

\d+ collate_on_int

-- Check conflicting or redundant options in CREATE COLLATION
-- LC_COLLATE
CREATE COLLATION coll_dup_chk (LC_COLLATE = "POSIX", LC_COLLATE = "NONSENSE", LC_CTYPE = "POSIX")
================================

-- LC_CTYPE
CREATE COLLATION coll_dup_chk (LC_CTYPE = "POSIX", LC_CTYPE = "NONSENSE", LC_COLLATE = "POSIX")
================================

-- PROVIDER
CREATE COLLATION coll_dup_chk (PROVIDER = icu, PROVIDER = NONSENSE, LC_COLLATE = "POSIX", LC_CTYPE = "POSIX")
================================

-- LOCALE
CREATE COLLATION case_sensitive (LOCALE = '', LOCALE = "NONSENSE")
================================

-- DETERMINISTIC
CREATE COLLATION coll_dup_chk (DETERMINISTIC = TRUE, DETERMINISTIC = NONSENSE, LOCALE = '')
================================

-- VERSION
CREATE COLLATION coll_dup_chk (VERSION = '1', VERSION = "NONSENSE", LOCALE = '')
================================

-- LOCALE conflicts with LC_COLLATE and LC_CTYPE
CREATE COLLATION coll_dup_chk (LC_COLLATE = "POSIX", LC_CTYPE = "POSIX", LOCALE = '')
================================

-- LOCALE conflicts with LC_COLLATE
CREATE COLLATION coll_dup_chk (LC_COLLATE = "POSIX", LOCALE = '')
================================

-- LOCALE conflicts with LC_CTYPE
CREATE COLLATION coll_dup_chk (LC_CTYPE = "POSIX", LOCALE = '')
================================

-- FROM conflicts with any other option
CREATE COLLATION coll_dup_chk (FROM = "C", VERSION = "1")
================================


--
-- Clean up.  Many of these table names will be re-used if the user is
-- trying to run any platform-specific collation tests later, so we
-- must get rid of them.
--
DROP SCHEMA collate_tests CASCADE
================================


================================
  -- not allowed

--
-- TIME simple math
--
-- We now make a distinction between time and intervals,
-- and adding two times together makes no sense at all.
-- Leave in one query to show that it is rejected,
-- and do the rest of the testing in horology.sql
-- where we do mixed-type arithmetic. - thomas 2000-12-02

SELECT f1 + time with time zone '00:01' AS "Illegal" FROM TIMETZ_TBL
================================


--
-- test EXTRACT
--
SELECT EXTRACT(MICROSECOND FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT EXTRACT(MILLISECOND FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT EXTRACT(SECOND      FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT EXTRACT(MINUTE      FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT EXTRACT(HOUR        FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT EXTRACT(DAY         FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================
  -- error
SELECT EXTRACT(FORTNIGHT   FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================
  -- error
SELECT EXTRACT(TIMEZONE    FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04:30')
================================

SELECT EXTRACT(TIMEZONE_HOUR   FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04:30')
================================

SELECT EXTRACT(TIMEZONE_MINUTE FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04:30')
================================

SELECT EXTRACT(EPOCH       FROM TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================


-- date_part implementation is mostly the same as extract, so only
-- test a few cases for additional coverage.
SELECT date_part('microsecond', TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT date_part('millisecond', TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT date_part('second',      TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================

SELECT date_part('epoch',       TIME WITH TIME ZONE '2020-05-26 13:30:25.575401-04')
================================


================================
--
-- SP-GiST index tests
--

CREATE TABLE quad_point_tbl AS
    SELECT point(unique1,unique2) AS p FROM tenk1
================================


INSERT INTO quad_point_tbl
    SELECT '(333.0,400.0)'::point FROM generate_series(1,1000)
================================


CREATE TABLE kd_point_tbl AS SELECT * FROM quad_point_tbl
================================


CREATE TABLE radix_text_tbl AS
    SELECT name AS t FROM road WHERE name !~ '^[0-9]'
================================


INSERT INTO radix_text_tbl
    SELECT 'P0123456789abcdef' FROM generate_series(1,1000)
================================


SELECT count(*) FROM quad_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================


SELECT count(*) FROM quad_point_tbl WHERE p << '(5000, 4000)'
================================


SELECT count(*) FROM quad_point_tbl WHERE p >> '(5000, 4000)'
================================


SELECT count(*) FROM quad_point_tbl WHERE p <<| '(5000, 4000)'
================================


SELECT count(*) FROM quad_point_tbl WHERE p |>> '(5000, 4000)'
================================


SELECT count(*) FROM quad_point_tbl WHERE p ~= '(4585, 365)'
================================


CREATE TEMP TABLE quad_point_tbl_ord_seq1 AS
SELECT row_number() OVER (ORDER BY p <-> '0,0') n, p <-> '0,0' dist, p
FROM quad_point_tbl
================================


CREATE TEMP TABLE quad_point_tbl_ord_seq2 AS
SELECT row_number() OVER (ORDER BY p <-> '0,0') n, p <-> '0,0' dist, p
FROM quad_point_tbl WHERE p <@ box '(200,200,1000,1000)'
================================


CREATE TEMP TABLE quad_point_tbl_ord_seq3 AS
SELECT row_number() OVER (ORDER BY p <-> '333,400') n, p <-> '333,400' dist, p
FROM quad_point_tbl WHERE p IS NOT NULL
================================


SELECT count(*) FROM radix_text_tbl WHERE t ~<~  'Aztec                         Ct  '
================================


SELECT count(*) FROM radix_text_tbl WHERE t ~<=~ 'Aztec                         Ct  '
================================


SELECT count(*) FROM radix_text_tbl WHERE t ~>=~ 'Worth                         St  '
================================


SELECT count(*) FROM radix_text_tbl WHERE t ~>~  'Worth                         St  '
================================


SELECT count(*) FROM radix_text_tbl WHERE t ^@  'Worth'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================

SELECT count(*) FROM quad_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p << '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p << '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p >> '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p >> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p <<| '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p <<| '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p |>> '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p |>> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p ~= '(4585, 365)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p ~= '(4585, 365)'
================================

CREATE TEMP TABLE quad_point_tbl_ord_idx1 AS
SELECT row_number() OVER (ORDER BY p <-> '0,0') n, p <-> '0,0' dist, p
FROM quad_point_tbl
================================

CREATE TEMP TABLE quad_point_tbl_ord_idx2 AS
SELECT row_number() OVER (ORDER BY p <-> '0,0') n, p <-> '0,0' dist, p
FROM quad_point_tbl WHERE p <@ box '(200,200,1000,1000)'
================================

CREATE TEMP TABLE quad_point_tbl_ord_idx3 AS
SELECT row_number() OVER (ORDER BY p <-> '333,400') n, p <-> '333,400' dist, p
FROM quad_point_tbl WHERE p IS NOT NULL
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================

SELECT count(*) FROM kd_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p << '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p << '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p >> '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p >> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p <<| '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p <<| '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p |>> '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p |>> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p ~= '(4585, 365)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p ~= '(4585, 365)'
================================

CREATE TEMP TABLE kd_point_tbl_ord_idx1 AS
SELECT row_number() OVER (ORDER BY p <-> '0,0') n, p <-> '0,0' dist, p
FROM kd_point_tbl
================================

CREATE TEMP TABLE kd_point_tbl_ord_idx2 AS
SELECT row_number() OVER (ORDER BY p <-> '0,0') n, p <-> '0,0' dist, p
FROM kd_point_tbl WHERE p <@ box '(200,200,1000,1000)'
================================

CREATE TEMP TABLE kd_point_tbl_ord_idx3 AS
SELECT row_number() OVER (ORDER BY p <-> '333,400') n, p <-> '333,400' dist, p
FROM kd_point_tbl WHERE p IS NOT NULL
================================


-- check ORDER BY distance to NULL
SELECT (SELECT p FROM kd_point_tbl ORDER BY p <-> pt, p <-> '0,0' LIMIT 1)
FROM (VALUES (point '1,2'), (NULL), ('1234,5678')) pts(pt)
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~<~  'Aztec                         Ct  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~<~  'Aztec                         Ct  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~<=~ 'Aztec                         Ct  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~<=~ 'Aztec                         Ct  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~>=~ 'Worth                         St  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~>=~ 'Worth                         St  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~>~  'Worth                         St  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~>~  'Worth                         St  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ^@	 'Worth'
================================

SELECT count(*) FROM radix_text_tbl WHERE t ^@	 'Worth'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================

SELECT count(*) FROM quad_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p << '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p << '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p >> '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p >> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p <<| '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p <<| '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p |>> '(5000, 4000)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p |>> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_point_tbl WHERE p ~= '(4585, 365)'
================================

SELECT count(*) FROM quad_point_tbl WHERE p ~= '(4585, 365)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================

SELECT count(*) FROM kd_point_tbl WHERE box '(200,200,1000,1000)' @> p
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p << '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p << '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p >> '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p >> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p <<| '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p <<| '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p |>> '(5000, 4000)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p |>> '(5000, 4000)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM kd_point_tbl WHERE p ~= '(4585, 365)'
================================

SELECT count(*) FROM kd_point_tbl WHERE p ~= '(4585, 365)'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~<~  'Aztec                         Ct  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~<~  'Aztec                         Ct  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~<=~ 'Aztec                         Ct  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~<=~ 'Aztec                         Ct  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~>=~ 'Worth                         St  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~>=~ 'Worth                         St  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ~>~  'Worth                         St  '
================================

SELECT count(*) FROM radix_text_tbl WHERE t ~>~  'Worth                         St  '
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM radix_text_tbl WHERE t ^@	 'Worth'
================================

SELECT count(*) FROM radix_text_tbl WHERE t ^@	 'Worth'
================================


================================


COMMENT ON TABLE attmp_wrong IS 'table comment'
================================

COMMENT ON TABLE attmp IS 'table comment'
================================

COMMENT ON TABLE attmp IS NULL
================================


--ALTER TABLE attmp ADD COLUMN o lock
================================


--ALTER TABLE attmp ADD COLUMN o lock
================================


\d+ attmp_idx

ALTER INDEX attmp_idx ALTER COLUMN 3 SET STATISTICS 1000
================================

ALTER VIEW attmp_view_new RENAME TO fail
================================

\d constraint_rename_test
CREATE TABLE constraint_rename_test2 (a int CONSTRAINT con1 CHECK (a > 0), d int) INHERITS (constraint_rename_test)
================================

\d constraint_rename_test2
ALTER TABLE constraint_rename_test2 RENAME CONSTRAINT con1 TO con1foo
================================
 -- ok
\d constraint_rename_test
\d constraint_rename_test2
ALTER TABLE constraint_rename_test ADD CONSTRAINT con2 CHECK (b > 0) NO INHERIT
================================
 -- ok
\d constraint_rename_test
\d constraint_rename_test2
ALTER TABLE constraint_rename_test ADD CONSTRAINT con3 PRIMARY KEY (a)
================================
 -- ok
\d constraint_rename_test
\d constraint_rename_test2
DROP TABLE constraint_rename_test2
================================

\d like_constraint_rename_cache
DROP TABLE constraint_rename_cache
================================
 RETURN $1
================================
 $$
================================

create table nv_parent (d date, check (false) no inherit not valid)
================================

-- not valid constraint added at creation time should automatically become valid
\d nv_parent

create table nv_child_2010 () inherits (nv_parent)
================================

\d nv_child_2009
-- we leave nv_parent and children around to help test pg_dump logic

-- Foreign key adding test with mixed types

-- Note: these tables are TEMP to avoid name conflicts when this test
-- is run in parallel with foreign_key.sql.

CREATE TEMP TABLE PKTABLE (ptest1 int PRIMARY KEY)
================================

insert into def_test default values
================================

insert into def_test default values
================================

insert into def_test default values
================================

insert into def_test default values
================================

insert into def_view_test default values
================================

insert into def_view_test default values
================================

insert into def_view_test default values
================================


drop rule def_view_test_ins on def_view_test
================================

comment on column atacc1.a is 'testing'
================================

comment on column atacc1."........pg.dropped.1........" is 'testing'
================================

create table attest1 as select * from atacc1
================================

10	11	12
\.
select * from attest
================================

21	22
\.
select * from attest
================================

31	32
\.
select * from attest
================================


\d anothertab
alter table anothertab alter column f1 type bigint
================================

\d anothertab

drop table anothertab
================================

\d at_part_1
\d at_part_2
alter table at_partitioned attach partition at_part_2 for values from (1000) to (2000)
================================

\d at_part_2
alter table at_partitioned alter column b type numeric using b::numeric
================================

\d at_part_1
\d at_part_2
drop table at_partitioned
================================

comment on constraint at_partitioned_id_name_key on at_partitioned is 'parent constraint'
================================

comment on index at_partitioned_id_name_key is 'parent index'
================================

comment on constraint at_partitioned_0_id_name_key on at_partitioned_0 is 'child 0 constraint'
================================

comment on index at_partitioned_0_id_name_key is 'child 0 index'
================================

comment on constraint at_partitioned_1_id_name_key on at_partitioned_1 is 'child 1 constraint'
================================

comment on index at_partitioned_1_id_name_key is 'child 1 index'
================================


create temp table old_oids as
  select relname, oid as oldoid, relfilenode as oldfilenode
  from pg_class where relname like 'at_partitioned%'
================================


select relname,
  c.oid = oldoid as orig_oid,
  case relfilenode
    when 0 then 'none'
    when c.oid then 'own'
    when oldfilenode then 'orig'
    else 'OTHER'
    end as storage,
  obj_description(c.oid, 'pg_class') as desc
  from pg_class c left join old_oids using (relname)
  where relname like 'at_partitioned%'
  order by relname
================================


select conname, obj_description(oid, 'pg_constraint') as desc
  from pg_constraint where conname like 'at_partitioned%'
  order by conname
================================


-- Note: these tests currently show the wrong behavior for comments :-(

select relname,
  c.oid = oldoid as orig_oid,
  case relfilenode
    when 0 then 'none'
    when c.oid then 'own'
    when oldfilenode then 'orig'
    else 'OTHER'
    end as storage,
  obj_description(c.oid, 'pg_class') as desc
  from pg_class c left join old_oids using (relname)
  where relname like 'at_partitioned%'
  order by relname
================================


select conname, obj_description(oid, 'pg_constraint') as desc
  from pg_constraint where conname like 'at_partitioned%'
  order by conname
================================

\d+ test_storage
\d+ test_storage_idx

-- ALTER COLUMN TYPE with a check constraint and a child table (bug #13779)
CREATE TABLE test_inh_check (a float check (a > 10.2), b float)
================================

\d test_inh_check
\d test_inh_check_child
select relname, conname, coninhcount, conislocal, connoinherit
  from pg_constraint c, pg_class r
  where relname like 'test_inh_check%' and c.conrelid = r.oid
  order by 1, 2
================================

\d test_inh_check
\d test_inh_check_child
select relname, conname, coninhcount, conislocal, connoinherit
  from pg_constraint c, pg_class r
  where relname like 'test_inh_check%' and c.conrelid = r.oid
  order by 1, 2
================================

\d test_inh_check
\d test_inh_check_child
select relname, conname, coninhcount, conislocal, connoinherit
  from pg_constraint c, pg_class r
  where relname like 'test_inh_check%' and c.conrelid = r.oid
  order by 1, 2
================================

\d test_inh_check
\d test_inh_check_child
select relname, conname, coninhcount, conislocal, connoinherit
  from pg_constraint c, pg_class r
  where relname like 'test_inh_check%' and c.conrelid = r.oid
  order by 1, 2
================================

\d check_fk_presence_2
DROP TABLE check_fk_presence_1, check_fk_presence_2
================================

\d+ at_view_1
\d+ at_view_2
explain (verbose, costs off) select * from at_view_2
================================

\d+ at_view_1
\d+ at_view_2
explain (verbose, costs off) select * from at_view_2
================================


    EXECUTE p_ddl
================================


    RETURN v_relfilenode <> (SELECT relfilenode FROM pg_class WHERE oid = p_tablename)
================================

$$
================================


-- empty[12] don't need rewrite, but notempty[12]_rewrite will force one
SELECT check_ddl_rewrite('rewrite_test', $$
  ALTER TABLE rewrite_test
      ADD COLUMN empty1 text,
      ADD COLUMN notempty1_rewrite serial
================================

$$)
================================

SELECT check_ddl_rewrite('rewrite_test', $$
    ALTER TABLE rewrite_test
        ADD COLUMN notempty2_rewrite serial,
        ADD COLUMN empty2 text
================================

$$)
================================

-- also check that fast defaults cause no problem, first without rewrite
SELECT check_ddl_rewrite('rewrite_test', $$
    ALTER TABLE rewrite_test
        ADD COLUMN empty3 text,
        ADD COLUMN notempty3_norewrite int default 42
================================

$$)
================================

SELECT check_ddl_rewrite('rewrite_test', $$
    ALTER TABLE rewrite_test
        ADD COLUMN notempty4_norewrite int default 42,
        ADD COLUMN empty4 text
================================

$$)
================================

-- then with rewrite
SELECT check_ddl_rewrite('rewrite_test', $$
    ALTER TABLE rewrite_test
        ADD COLUMN empty5 text,
        ADD COLUMN notempty5_norewrite int default 42,
        ADD COLUMN notempty5_rewrite serial
================================

$$)
================================

SELECT check_ddl_rewrite('rewrite_test', $$
    ALTER TABLE rewrite_test
        ADD COLUMN notempty6_rewrite serial,
        ADD COLUMN empty6 text,
        ADD COLUMN notempty6_norewrite int default 42
================================

$$)
================================


--
-- lock levels
--
drop type lockmodes
================================

create type lockmodes as enum (
 'SIReadLock'
,'AccessShareLock'
,'RowShareLock'
,'RowExclusiveLock'
,'ShareUpdateExclusiveLock'
,'ShareLock'
,'ShareRowExclusiveLock'
,'ExclusiveLock'
,'AccessExclusiveLock'
)
================================
 alter table alterlock set (toast.autovacuum_enabled = off)
================================

alter view my_locks set (autovacuum_enabled = false)
================================

alter view my_locks reset (autovacuum_enabled)
================================

alter view my_locks set (security_barrier=off)
================================

alter view my_locks reset (security_barrier)
================================

drop type lockmodes
================================
'
    language sql returns null on null input
================================
'
    language sql called on null input
================================


--
-- alter object set schema
--

create schema alter1
================================

create schema alter2
================================


create type alter1.ctype as (f1 int, f2 text)
================================


create operator alter1.=(procedure = alter1.same, leftarg  = alter1.ctype, rightarg = alter1.ctype)
================================


create operator class alter1.ctype_hash_ops default for type alter1.ctype using hash as
  operator 1 alter1.=(alter1.ctype, alter1.ctype)
================================


create conversion alter1.latin1_to_utf8 for 'latin1' to 'utf8' from iso8859_1_to_utf8
================================


create text search parser alter1.prs(start = prsd_start, gettoken = prsd_nexttoken, end = prsd_end, lextypes = prsd_lextype)
================================

create text search configuration alter1.cfg(parser = alter1.prs)
================================

create text search template alter1.tmpl(init = dsimple_init, lexize = dsimple_lexize)
================================

create text search dictionary alter1.dict(template = alter1.tmpl)
================================

alter domain alter1.posint set schema alter2
================================

alter operator class alter1.ctype_hash_ops using hash set schema alter2
================================

alter operator family alter1.ctype_hash_ops using hash set schema alter2
================================

alter operator alter1.=(alter1.ctype, alter1.ctype) set schema alter2
================================

alter type alter1.ctype set schema alter1
================================
 -- no-op, same schema
alter type alter1.ctype set schema alter2
================================

alter conversion alter1.latin1_to_utf8 set schema alter2
================================

alter text search parser alter1.prs set schema alter2
================================

alter text search configuration alter1.cfg set schema alter2
================================

alter text search template alter1.tmpl set schema alter2
================================

alter text search dictionary alter1.dict set schema alter2
================================


-- this should succeed because nothing is left in alter1
drop schema alter1
================================


-- clean up
drop schema alter2 cascade
================================


--
-- composite types
--

CREATE TYPE test_type AS (a int)
================================

\d test_type

ALTER TYPE nosuchtype ADD ATTRIBUTE b text
================================
 -- fails

ALTER TYPE test_type ADD ATTRIBUTE b text
================================

\d test_type

ALTER TYPE test_type ADD ATTRIBUTE b text
================================
 -- fails

ALTER TYPE test_type ALTER ATTRIBUTE b SET DATA TYPE varchar
================================

\d test_type

ALTER TYPE test_type ALTER ATTRIBUTE b SET DATA TYPE integer
================================

\d test_type

ALTER TYPE test_type DROP ATTRIBUTE b
================================

\d test_type

ALTER TYPE test_type DROP ATTRIBUTE c
================================
 -- fails

ALTER TYPE test_type DROP ATTRIBUTE IF EXISTS c
================================


ALTER TYPE test_type DROP ATTRIBUTE a, ADD ATTRIBUTE d boolean
================================

\d test_type

ALTER TYPE test_type RENAME ATTRIBUTE a TO aa
================================

ALTER TYPE test_type RENAME ATTRIBUTE d TO dd
================================

\d test_type

DROP TYPE test_type
================================


CREATE TYPE test_type1 AS (a int, b text)
================================

ALTER TYPE test_type1 ALTER ATTRIBUTE b TYPE varchar
================================
 -- fails

CREATE TYPE test_type2 AS (a int, b text)
================================

\d test_type2
\d test_tbl2

ALTER TYPE test_type2 ADD ATTRIBUTE c text
================================
 -- fails
ALTER TYPE test_type2 ADD ATTRIBUTE c text CASCADE
================================

\d test_type2
\d test_tbl2

ALTER TYPE test_type2 ALTER ATTRIBUTE b TYPE varchar
================================
 -- fails
ALTER TYPE test_type2 ALTER ATTRIBUTE b TYPE varchar CASCADE
================================

\d test_type2
\d test_tbl2

ALTER TYPE test_type2 DROP ATTRIBUTE b
================================
 -- fails
ALTER TYPE test_type2 DROP ATTRIBUTE b CASCADE
================================

\d test_type2
\d test_tbl2

ALTER TYPE test_type2 RENAME ATTRIBUTE a TO aa
================================
 -- fails
ALTER TYPE test_type2 RENAME ATTRIBUTE a TO aa CASCADE
================================

\d test_type2
\d test_tbl2
\d test_tbl2_subclass

DROP TABLE test_tbl2_subclass
================================


CREATE TYPE test_typex AS (a int, b text)
================================

ALTER TYPE test_typex DROP ATTRIBUTE a
================================
 -- fails
ALTER TYPE test_typex DROP ATTRIBUTE a CASCADE
================================

\d test_tblx
DROP TABLE test_tblx
================================

DROP TYPE test_typex
================================


-- This test isn't that interesting on its own, but the purpose is to leave
-- behind a table to test pg_upgrade with. The table has a composite type
-- column in it, and the composite type has a dropped attribute.
CREATE TYPE test_type3 AS (a int)
================================

CREATE TABLE test_tbl3 (c) AS SELECT '(1)'::test_type3
================================

ALTER TYPE test_type3 DROP ATTRIBUTE a, ADD ATTRIBUTE b int
================================


CREATE TYPE test_type_empty AS ()
================================

DROP TYPE test_type_empty
================================


--
-- typed tables: OF / NOT OF
--

CREATE TYPE tt_t0 AS (z inet, x int, y numeric(8,2))
================================

ALTER TYPE tt_t0 DROP ATTRIBUTE z
================================


CREATE TYPE tt_t1 AS (x int, y numeric(8,2))
================================

\d tt7

-- make sure we can drop a constraint on the parent but it remains on the child
CREATE TABLE test_drop_constr_parent (c text CHECK (c IS NOT NULL))
================================

CREATE SCHEMA alter2
================================


\d alter2.tt8

DROP TABLE alter2.tt8
================================

DROP SCHEMA alter2
================================
  -- picks nonconflicting name
\d tt9
DROP TABLE tt9
================================


COMMENT ON COLUMN comment_test.id IS 'Column ''id'' on comment_test'
================================

COMMENT ON INDEX comment_test_index IS 'Simple index on comment_test'
================================

COMMENT ON CONSTRAINT comment_test_positive_col_check ON comment_test IS 'CHECK constraint on comment_test.positive_col'
================================

COMMENT ON CONSTRAINT comment_test_pk ON comment_test IS 'PRIMARY KEY constraint of comment_test'
================================

COMMENT ON INDEX comment_test_pk IS 'Index backing the PRIMARY KEY of comment_test'
================================

SELECT conname as constraint, obj_description(oid, 'pg_constraint') as comment FROM pg_constraint where conrelid = 'comment_test'::regclass ORDER BY 1, 2
================================

SELECT conname as constraint, obj_description(oid, 'pg_constraint') as comment FROM pg_constraint where conrelid = 'comment_test'::regclass ORDER BY 1, 2
================================

COMMENT ON COLUMN comment_test_child.id IS 'Column ''id'' on comment_test_child'
================================

COMMENT ON INDEX comment_test_child_fk IS 'Index backing the FOREIGN KEY of comment_test_child'
================================

COMMENT ON CONSTRAINT comment_test_child_fk ON comment_test_child IS 'FOREIGN KEY constraint of comment_test_child'
================================

SELECT conname as constraint, obj_description(oid, 'pg_constraint') as comment FROM pg_constraint where conrelid = 'comment_test_child'::regclass ORDER BY 1, 2
================================


-- Check that we map relation oids to filenodes and back correctly.  Only
-- display bad mappings so the test output doesn't change all the time.  A
-- filenode function call can return NULL for a relation dropped concurrently
-- with the call's surrounding query, so ignore a NULL mapped_oid for
-- relations that no longer exist after all calls finish.
CREATE TEMP TABLE filenode_mapping AS
SELECT
    oid, mapped_oid, reltablespace, relfilenode, relname
FROM pg_class,
    pg_filenode_relation(reltablespace, pg_relation_filenode(oid)) AS mapped_oid
WHERE relkind IN ('r', 'i', 'S', 't', 'm') AND mapped_oid IS DISTINCT FROM oid
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN c2 integer
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN c2 integer
================================
 -- fail because c2 already exists
\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c2 integer
================================
 -- skipping because c2 already exists
\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN c2 integer, -- fail because c2 already exists
	ADD COLUMN c3 integer primary key
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c2 integer, -- skipping because c2 already exists
	ADD COLUMN c3 integer primary key
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c2 integer, -- skipping because c2 already exists
	ADD COLUMN IF NOT EXISTS c3 integer primary key
================================
 -- skipping because c3 already exists
\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c2 integer, -- skipping because c2 already exists
	ADD COLUMN IF NOT EXISTS c3 integer, -- skipping because c3 already exists
	ADD COLUMN c4 integer REFERENCES test_add_column
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c4 integer REFERENCES test_add_column
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c5 SERIAL CHECK (c5 > 8)
================================

\d test_add_column
ALTER TABLE test_add_column
	ADD COLUMN IF NOT EXISTS c5 SERIAL CHECK (c5 > 10)
================================

\d test_add_column*
DROP TABLE test_add_column
================================

\d test_add_column*

-- assorted cases with multiple ALTER TABLE steps
CREATE TABLE ataddindex(f1 INT)
================================

\d ataddindex
DROP TABLE ataddindex
================================

\d ataddindex
DROP TABLE ataddindex
================================

\d ataddindex
DROP TABLE ataddindex
================================

\d ataddindex
DROP TABLE ataddindex
================================


-- check that the table being attached is not a typed table
CREATE TYPE mytype AS (a int)
================================

DROP TYPE mytype CASCADE
================================


-- however, if a list partition does not accept nulls, there should be
-- an explicit NOT NULL constraint on the partition key column for the
-- validation scan to be skipped
================================

\d+ range_parted2
-- constraint should be created
\d part_rp
CREATE TABLE part_rp100 PARTITION OF range_parted2 (CHECK (a>=123 AND a<133 AND a IS NOT NULL)) FOR VALUES FROM (100) to (200)
================================

-- redundant constraint should not be created
\d part_rp100
DROP TABLE range_parted2
================================


-- cannot drop or alter type of partition key columns of lower level
-- partitioned tables
================================
 for example, part_5, which is list_parted2's
-- partition, is partitioned on b
================================

ALTER TABLE attmp ALTER COLUMN i RESET (n_distinct_inherited)
================================

    execute 'alter table tab_part_attach attach partition tab_part_attach_1 for values in (1)'
================================

    return null
================================
 $$
================================

create operator class at_test_sql_partop for type int4 using btree as
    operator 1 < (int4, int4), operator 2 <= (int4, int4),
    operator 3 = (int4, int4), operator 4 >= (int4, int4),
    operator 5 > (int4, int4), function 1 at_test_sql_partop(int4, int4)
================================

drop operator class at_test_sql_partop using btree
================================

    return NULL
================================

$$
================================


-- Test altering table having publication
create schema alter1
================================

create schema alter2
================================

create publication pub1 for table alter1.t1, all tables in schema alter2
================================
 -- should fail
drop publication pub1
================================

drop schema alter1 cascade
================================

drop schema alter2 cascade
================================


================================


CREATE SCHEMA collate_tests
================================


\d collate_test1

CREATE TABLE collate_test_fail (
    a int,
    b text COLLATE "ja_JP.eucjp-x-icu"
)
================================


\d collate_test_like

CREATE TABLE collate_test2 (
    a int,
    b text COLLATE "sv-x-icu"
)
================================

INSERT INTO collate_test2 SELECT * FROM collate_test1
================================

INSERT INTO collate_test3 SELECT * FROM collate_test1
================================

INSERT INTO collate_test4 SELECT * FROM collate_test1
================================

INSERT INTO collate_test5 SELECT * FROM collate_test1
================================

INSERT INTO collate_test6 VALUES (1, 'abc'), (2, 'ABC'), (3, '123'), (4, 'ab1'),
                                 (5, 'a1!'), (6, 'a c'), (7, '!.
================================
'), (8, '   '),
                                 (9, 'b'), (10, 'B')
================================



/* not run by default because it requires tr_TR system locale
-- to_char

SET lc_time TO 'tr_TR'
================================

SELECT to_char(date '2010-04-01', 'DD TMMON YYYY')
================================

SELECT to_char(date '2010-04-01', 'DD TMMON YYYY' COLLATE "tr-x-icu")
================================

*/


-- backwards parsing

CREATE VIEW collview1 AS SELECT * FROM collate_test1 WHERE b COLLATE "C" >= 'bbc'
================================
 -- fail

CREATE TABLE test_u AS SELECT a, b FROM collate_test1 UNION ALL SELECT a, b FROM collate_test3
================================



-- casting

SELECT CAST('42' AS text COLLATE "C")
================================

  yy text := y
================================

  yy text := y
================================

CREATE SCHEMA test_schema
================================


-- We need to do this this way to cope with varying names for encodings:
do $$
BEGIN
  EXECUTE 'CREATE COLLATION test0 (provider = icu, locale = ' ||
          quote_literal(current_setting('lc_collate')) || ')
================================
'
================================

CREATE COLLATION test0 FROM "C"
================================
 -- fail, duplicate name
do $$
BEGIN
  EXECUTE 'CREATE COLLATION test1 (provider = icu, lc_collate = ' ||
          quote_literal(current_setting('lc_collate')) ||
          ', lc_ctype = ' ||
          quote_literal(current_setting('lc_ctype')) || ')
================================
'
================================

CREATE COLLATION test3 (provider = icu, lc_collate = 'en_US.utf8')
================================
 -- fail, need lc_ctype
CREATE COLLATION testx (provider = icu, locale = 'nonsense')
================================
 /* never fails with ICU */  DROP COLLATION testx
================================


CREATE COLLATION test4 FROM nonsense
================================

CREATE COLLATION test5 FROM test0
================================


ALTER COLLATION test1 RENAME TO test11
================================

ALTER COLLATION test0 RENAME TO test11
================================
 -- fail
ALTER COLLATION test1 RENAME TO test22
================================
 -- fail

ALTER COLLATION test11 OWNER TO regress_test_role
================================

ALTER COLLATION test11 OWNER TO nonsense
================================

ALTER COLLATION test11 SET SCHEMA test_schema
================================


COMMENT ON COLLATION test0 IS 'US English'
================================


DROP COLLATION test0, test_schema.test11, test5
================================

DROP COLLATION test0
================================
 -- fail
DROP COLLATION IF EXISTS test0
================================


DROP SCHEMA test_schema
================================



-- ALTER

ALTER COLLATION "en-x-icu" REFRESH VERSION
================================



-- dependencies

CREATE COLLATION test0 FROM "C"
================================

CREATE TYPE collate_dep_test2 AS (x int, y text COLLATE test0)
================================


DROP COLLATION test0 RESTRICT
================================
 -- fail
DROP COLLATION test0 CASCADE
================================


\d collate_dep_test1
\d collate_dep_test2

DROP TABLE collate_dep_test1, collate_dep_test4t
================================

DROP TYPE collate_dep_test2
================================


-- test range types and collations

create type textrange_c as range(subtype=text, collation="C")
================================

create type textrange_en_us as range(subtype=text, collation="en-x-icu")
================================


drop type textrange_c
================================

drop type textrange_en_us
================================



-- test ICU collation customization

-- test the attributes handled by icu_set_collation_attributes()

CREATE COLLATION testcoll_ignore_accents (provider = icu, locale = '@colStrength=primary
================================
colCaseLevel=yes')
================================


CREATE COLLATION testcoll_backwards (provider = icu, locale = '@colBackwards=yes')
================================


CREATE COLLATION testcoll_lower_first (provider = icu, locale = '@colCaseFirst=lower')
================================

CREATE COLLATION testcoll_upper_first (provider = icu, locale = '@colCaseFirst=upper')
================================


CREATE COLLATION testcoll_shifted (provider = icu, locale = '@colAlternate=shifted')
================================


CREATE COLLATION testcoll_numeric (provider = icu, locale = '@colNumeric=yes')
================================


CREATE COLLATION testcoll_error1 (provider = icu, locale = '@colNumeric=lower')
================================


-- test that attributes not handled by icu_set_collation_attributes()
-- (handled by ucol_open() directly) also work
CREATE COLLATION testcoll_de_phonebook (provider = icu, locale = 'de@collation=phonebook')
================================



-- nondeterministic collations

CREATE COLLATION ctest_det (provider = icu, locale = '', deterministic = true)
================================

CREATE COLLATION ctest_nondet (provider = icu, locale = '', deterministic = false)
================================

-- same string in different normal forms
INSERT INTO test6 VALUES (1, U&'\00E4bc')
================================

INSERT INTO test6 VALUES (2, U&'\0061\0308bc')
================================

INSERT INTO test6a VALUES (1, ARRAY[U&'\00E4bc'])
================================

INSERT INTO test6a VALUES (2, ARRAY[U&'\0061\0308bc'])
================================


CREATE COLLATION case_sensitive (provider = icu, locale = '')
================================

CREATE COLLATION case_insensitive (provider = icu, locale = '@colStrength=secondary', deterministic = false)
================================


-- accents
CREATE COLLATION ignore_accents (provider = icu, locale = '@colStrength=primary
================================
colCaseLevel=yes', deterministic = false)
================================

DROP SCHEMA collate_tests CASCADE
================================


-- leave a collation for pg_upgrade test
CREATE COLLATION coll_icu_upgrade FROM "und-x-icu"
================================


================================


================================


-- fail - no publications
CREATE SUBSCRIPTION regress_testsub CONNECTION 'foo'
================================


-- fail - no connection
CREATE SUBSCRIPTION regress_testsub PUBLICATION foo
================================

CREATE SUBSCRIPTION regress_testsub CONNECTION 'testconn' PUBLICATION testpub WITH (create_slot)
================================


-- fail - invalid connection string
CREATE SUBSCRIPTION regress_testsub CONNECTION 'testconn' PUBLICATION testpub
================================


-- fail - duplicate publications
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION foo, testpub, foo WITH (connect = false)
================================


-- ok
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false)
================================


COMMENT ON SUBSCRIPTION regress_testsub IS 'test subscription'
================================


-- fail - name already exists
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION foo WITH (connect = false)
================================


-- fail - invalid option combinations
CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, copy_data = true)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, enabled = true)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, create_slot = true)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (slot_name = NONE, enabled = true)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (slot_name = NONE, create_slot = true)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (slot_name = NONE)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (slot_name = NONE, enabled = false)
================================

CREATE SUBSCRIPTION regress_testsub2 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (slot_name = NONE, create_slot = false)
================================


-- ok - with slot_name = NONE
CREATE SUBSCRIPTION regress_testsub3 CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (slot_name = NONE, connect = false)
================================

-- fail
ALTER SUBSCRIPTION regress_testsub3 ENABLE
================================

ALTER SUBSCRIPTION regress_testsub3 REFRESH PUBLICATION
================================


DROP SUBSCRIPTION regress_testsub3
================================


-- fail - invalid connection string
ALTER SUBSCRIPTION regress_testsub CONNECTION 'foobar'
================================


\dRs+

ALTER SUBSCRIPTION regress_testsub SET PUBLICATION testpub2, testpub3 WITH (refresh = false)
================================

ALTER SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist2'
================================

ALTER SUBSCRIPTION regress_testsub SET (slot_name = 'newname')
================================


-- fail
ALTER SUBSCRIPTION regress_testsub SET (slot_name = '')
================================


-- fail
ALTER SUBSCRIPTION regress_doesnotexist CONNECTION 'dbname=regress_doesnotexist2'
================================

ALTER SUBSCRIPTION regress_testsub SET (create_slot = false)
================================


\dRs+

BEGIN
================================

ALTER SUBSCRIPTION regress_testsub ENABLE
================================


\dRs

ALTER SUBSCRIPTION regress_testsub DISABLE
================================


\dRs

COMMIT
================================

ALTER SUBSCRIPTION regress_testsub RENAME TO regress_testsub_dummy
================================


ALTER SUBSCRIPTION regress_testsub RENAME TO regress_testsub_foo
================================

ALTER SUBSCRIPTION regress_testsub_foo SET (synchronous_commit = local)
================================

ALTER SUBSCRIPTION regress_testsub_foo SET (synchronous_commit = foobar)
================================


\dRs+

-- rename back to keep the rest simple
ALTER SUBSCRIPTION regress_testsub_foo RENAME TO regress_testsub
================================


-- fail - new owner must be superuser
ALTER SUBSCRIPTION regress_testsub OWNER TO regress_subscription_user2
================================

-- now it works
ALTER SUBSCRIPTION regress_testsub OWNER TO regress_subscription_user2
================================

DROP SUBSCRIPTION regress_testsub
================================


ALTER SUBSCRIPTION regress_testsub SET (slot_name = NONE)
================================

DROP SUBSCRIPTION regress_testsub
================================


DROP SUBSCRIPTION IF EXISTS regress_testsub
================================

DROP SUBSCRIPTION regress_testsub
================================
  -- fail

-- fail - binary must be boolean
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, binary = foo)
================================


-- now it works
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, binary = true)
================================


\dRs+

ALTER SUBSCRIPTION regress_testsub SET (binary = false)
================================

ALTER SUBSCRIPTION regress_testsub SET (slot_name = NONE)
================================


\dRs+

DROP SUBSCRIPTION regress_testsub
================================


-- fail - streaming must be boolean
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, streaming = foo)
================================


-- now it works
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, streaming = true)
================================


\dRs+

ALTER SUBSCRIPTION regress_testsub SET (streaming = false)
================================

ALTER SUBSCRIPTION regress_testsub SET (slot_name = NONE)
================================


\dRs+

-- fail - publication already exists
ALTER SUBSCRIPTION regress_testsub ADD PUBLICATION testpub WITH (refresh = false)
================================


-- fail - publication used more than once
ALTER SUBSCRIPTION regress_testsub ADD PUBLICATION testpub1, testpub1 WITH (refresh = false)
================================


-- ok - add two publications into subscription
ALTER SUBSCRIPTION regress_testsub ADD PUBLICATION testpub1, testpub2 WITH (refresh = false)
================================


-- fail - publications already exist
ALTER SUBSCRIPTION regress_testsub ADD PUBLICATION testpub1, testpub2 WITH (refresh = false)
================================


\dRs+

-- fail - publication used more then once
ALTER SUBSCRIPTION regress_testsub DROP PUBLICATION testpub1, testpub1 WITH (refresh = false)
================================


-- fail - all publications are deleted
ALTER SUBSCRIPTION regress_testsub DROP PUBLICATION testpub, testpub1, testpub2 WITH (refresh = false)
================================


-- fail - publication does not exist in subscription
ALTER SUBSCRIPTION regress_testsub DROP PUBLICATION testpub3 WITH (refresh = false)
================================


-- ok - delete publications
ALTER SUBSCRIPTION regress_testsub DROP PUBLICATION testpub1, testpub2 WITH (refresh = false)
================================


\dRs+

DROP SUBSCRIPTION regress_testsub
================================


CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION mypub
       WITH (connect = false, create_slot = false, copy_data = false)
================================


ALTER SUBSCRIPTION regress_testsub ENABLE
================================

ALTER SUBSCRIPTION regress_testsub SET PUBLICATION mypub WITH (refresh = true)
================================

ALTER SUBSCRIPTION regress_testsub REFRESH PUBLICATION
================================


ALTER SUBSCRIPTION regress_testsub DISABLE
================================

ALTER SUBSCRIPTION regress_testsub SET (slot_name = NONE)
================================

DROP SUBSCRIPTION regress_testsub
================================


-- fail - two_phase must be boolean
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, two_phase = foo)
================================


-- now it works
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, two_phase = true)
================================


\dRs+
--fail - alter of two_phase option not supported.
ALTER SUBSCRIPTION regress_testsub SET (two_phase = false)
================================


-- but can alter streaming when two_phase enabled
ALTER SUBSCRIPTION regress_testsub SET (streaming = true)
================================


\dRs+

ALTER SUBSCRIPTION regress_testsub SET (slot_name = NONE)
================================

DROP SUBSCRIPTION regress_testsub
================================


-- two_phase and streaming are compatible.
CREATE SUBSCRIPTION regress_testsub CONNECTION 'dbname=regress_doesnotexist' PUBLICATION testpub WITH (connect = false, streaming = true, two_phase = true)
================================


\dRs+

ALTER SUBSCRIPTION regress_testsub SET (slot_name = NONE)
================================

DROP SUBSCRIPTION regress_testsub
================================


================================

comment on domain domaindroptest is 'About to drop this..'
================================
 -- fail
notsoshorttext
\.

COPY basictest (testvarchar) FROM stdin
================================

short
\.

select * from basictest
================================

INSERT INTO domarrtest (testint4arr[1], testint4arr[3]) values (11,22)
================================

select testint4arr[1], testchar4arr[2:2] from domarrtest
================================

{3,4}	{q,w,e}
\N	\N
\.

COPY domarrtest FROM stdin
================================
	-- fail
{3,4}	{qwerty,w,e}
\.

select * from domarrtest
================================


update domarrtest set
  testint4arr[1] = testint4arr[1] + 1,
  testint4arr[3] = testint4arr[3] - 1
where testchar4arr is null
================================



-- Test domains over composites

create type comptype as (r float8, i float8)
================================

select (d1).r, (d1).i, (d1).* from dcomptable
================================

update dcomptable set d1.r = (d1).r + 1 where (d1).i > 0
================================


alter domain dcomptype add constraint c1 check ((value).r <= (value).i)
================================

alter domain dcomptype add constraint c2 check ((value).r > (value).i)
================================
  -- fail
update dcomptable set d1.r = (d1).r + 1 where (d1).i > 0
================================
  -- fail
update dcomptable set d1.r = (d1).r - 1, d1.i = (d1).i + 1 where (d1).i > 0
================================


explain (verbose, costs off)
  update dcomptable set d1.r = (d1).r - 1, d1.i = (d1).i + 1 where (d1).i > 0
================================

create rule silly as on delete to dcomptable do instead
  update dcomptable set d1.r = (d1).r - 1, d1.i = (d1).i + 1 where (d1).i > 0
================================

\d+ dcomptable

drop table dcomptable
================================

drop type comptype cascade
================================



-- check altering and dropping columns used by domain constraints
create type comptype as (r float8, i float8)
================================

alter domain dcomptype add constraint c1 check ((value).r > 0)
================================

comment on constraint c1 on domain dcomptype is 'random commentary'
================================
  -- fail

alter type comptype alter attribute r type varchar
================================
  -- fail
alter type comptype alter attribute r type bigint
================================


alter type comptype drop attribute r
================================
  -- fail
alter type comptype drop attribute i
================================
  -- check comment is still there

drop type comptype cascade
================================



-- Test domains over arrays of composite

create type comptype as (r float8, i float8)
================================
  -- fail on uniqueness
insert into dcomptable (d1[1]) values(row(9,10))
================================

select d1[2], d1[1].r, d1[1].i from dcomptable
================================

update dcomptable set d1[2] = row(d1[2].i, d1[2].r)
================================

update dcomptable set d1[1].r = d1[1].r + 1 where d1[1].i > 0
================================


alter domain dcomptypea add constraint c1 check (value[1].r <= value[1].i)
================================

alter domain dcomptypea add constraint c2 check (value[1].r > value[1].i)
================================
  -- fail
update dcomptable set d1[1].r = d1[1].r + 1 where d1[1].i > 0
================================
  -- fail
update dcomptable set d1[1].r = d1[1].r - 1, d1[1].i = d1[1].i + 1
  where d1[1].i > 0
================================


explain (verbose, costs off)
  update dcomptable set d1[1].r = d1[1].r - 1, d1[1].i = d1[1].i + 1
    where d1[1].i > 0
================================

create rule silly as on delete to dcomptable do instead
  update dcomptable set d1[1].r = d1[1].r - 1, d1[1].i = d1[1].i + 1
    where d1[1].i > 0
================================

\d+ dcomptable

drop table dcomptable
================================

drop type comptype cascade
================================
  -- fail
update pitable set f1[1] = f1[1] + 1
================================

update pitable set f1[1] = 0
================================

drop type vc4
================================
 -- but this works
select f1, f1[1], (f1[1])[1] from dposintatable
================================

select pg_typeof(f1[1]) from dposintatable
================================

select pg_typeof(f1[1][1]) from dposintatable
================================

select pg_typeof((f1[1])[1]) from dposintatable
================================

update dposintatable set f1[2] = array[99]
================================

select f1, f1[1], (f1[2])[1] from dposintatable
================================

-- it'd be nice if you could do something like this, but for now you can't:
update dposintatable set f1[2][1] = array[97]
================================

-- maybe someday we can make this syntax work:
update dposintatable set (f1[2])[1] = array[98]
================================



-- Test arrays over domains of composite

create type comptype as (cf1 int, cf2 int)
================================

update dcomptable set f1[1].cf2 = 5
================================

update dcomptable set f1[1].cf1 = -1
================================
  -- fail
update dcomptable set f1[1].cf1 = 1
================================

drop type comptype cascade
================================

INSERT INTO nulltest DEFAULT VALUES
================================
 --fail
a	b	\N	d	d
\.

COPY nulltest FROM stdin
================================
 --fail
a	b	c	d	\N
\.

-- Last row is bad
COPY nulltest FROM stdin
================================

a	b	c	\N	c
a	b	c	\N	d
a	b	c	\N	a
\.

select * from nulltest
================================

insert into defaulttest default values
================================
 -- succeeds, inserts domain default
-- We used to treat SET DEFAULT NULL as equivalent to DROP DEFAULT
================================
 wrong
alter table defaulttest alter column col5 set default null
================================

insert into defaulttest default values
================================

insert into defaulttest default values
================================

42
\.

select * from defaulttest
================================


insert into domnotnull default values
================================

alter domain dnotnulltest set not null
================================

alter domain dnotnulltest set not null
================================


alter domain dnotnulltest set not null
================================
 -- fails

alter domain dnotnulltest drop not null
================================


insert into domdeftest default values
================================


alter domain ddef1 set default '42'
================================

insert into domdeftest default values
================================


alter domain ddef1 drop default
================================

insert into domdeftest default values
================================

alter domain con add constraint t check (VALUE < 1)
================================
 -- fails

alter domain con add constraint t check (VALUE < 34)
================================

alter domain con add check (VALUE > 0)
================================


alter domain con drop constraint t
================================


alter domain con drop constraint nonexistent
================================

alter domain con drop constraint if exists nonexistent
================================

ALTER DOMAIN things ADD CONSTRAINT meow CHECK (VALUE < 11)
================================

ALTER DOMAIN things ADD CONSTRAINT meow CHECK (VALUE < 11) NOT VALID
================================

ALTER DOMAIN things VALIDATE CONSTRAINT meow
================================

ALTER DOMAIN things VALIDATE CONSTRAINT meow
================================


alter domain dom set not null
================================
 -- fail

alter domain dom drop not null
================================


alter domain dom add constraint domchkgt6 check(value > 6)
================================
 --fail

alter domain dom drop constraint domchkgt6 restrict
================================

end$$ language plpgsql
================================

end$$ language plpgsql
================================

    return v - 1
================================

end$$ language plpgsql
================================


-- Currently, this doesn't work for composite types, but verify it complains
create type ddtest1 as (f1 posint)
================================

alter domain posint add constraint c1 check(value >= 0)
================================

alter domain posint add constraint c1 check(value >= 0)
================================

alter domain posint add constraint c1 check(value >= 0)
================================

alter domain posint add constraint c1 check(value >= 0)
================================


-- Doesn't work for ranges, either
create type rposint as range (subtype = posint)
================================

alter domain posint add constraint c1 check(value >= 0)
================================

drop type rposint
================================


alter domain posint add constraint c1 check(value >= 0)
================================


alter domain posint add constraint c2 check(value >= 10)
================================
 -- fail
alter domain posint add constraint c2 check(value > 0)
================================

drop type ddtest1
================================

  return x[1]
================================

end$$ language plpgsql
================================

  return x[1]
================================

end$$ language plpgsql
================================

  return x[1]
================================

end$$ language plpgsql
================================
  -- fail

update op set f1[2] = 3
================================

update op set f1[2] = 0
================================

  return x[2]
================================

end$$ language plpgsql
================================

  return d
================================


alter domain di add constraint pos check (value > 0)
================================
 -- fail

alter domain di drop constraint pos
================================

  return d
================================


alter domain di add constraint pos check (value > 0)
================================
 -- fail

alter domain di drop constraint pos
================================


--
-- Check use of a (non-inline-able) SQL function in a domain constraint
================================

alter domain testdomain1 rename to testdomain2
================================

alter type testdomain2 rename to testdomain3
================================

alter domain testdomain1 rename constraint unsigned to unsigned_foo
================================

alter domain testdomain1 drop constraint unsigned_foo
================================


================================


INSERT INTO abbrev_abort_uuids (abort_increasing, abort_decreasing, noabort_increasing, noabort_decreasing)
    SELECT
        ('00000000-0000-0000-0000-'||to_char(g.i, '000000000000FM'))::uuid abort_increasing,
        ('00000000-0000-0000-0000-'||to_char(20000 - g.i, '000000000000FM'))::uuid abort_decreasing,
        (to_char(g.i % 10009, '00000000FM')||'-0000-0000-0000-'||to_char(g.i, '000000000000FM'))::uuid noabort_increasing,
        (to_char(((20000 - g.i) % 10009), '00000000FM')||'-0000-0000-0000-'||to_char(20000 - g.i, '000000000000FM'))::uuid noabort_decreasing
    FROM generate_series(0, 20000, 1) g(i)
================================

INSERT INTO abbrev_abort_uuids DEFAULT VALUES
================================

INSERT INTO abbrev_abort_uuids DEFAULT VALUES
================================


-- add just a few duplicates
INSERT INTO abbrev_abort_uuids (abort_increasing, abort_decreasing, noabort_increasing, noabort_decreasing)
    SELECT abort_increasing, abort_decreasing, noabort_increasing, noabort_decreasing
    FROM abbrev_abort_uuids
    WHERE (id < 10 OR id > 19990) AND id % 3 = 0 AND abort_increasing is not null
================================


----
-- Check sort node uses of tuplesort wrt. abbreviated keys
----

-- plain sort triggering abbreviated abort
SELECT abort_increasing, abort_decreasing FROM abbrev_abort_uuids ORDER BY abort_increasing OFFSET 20000 - 4
================================

SELECT abort_increasing, abort_decreasing FROM abbrev_abort_uuids ORDER BY abort_decreasing NULLS FIRST OFFSET 20000 - 4
================================


-- plain sort not triggering abbreviated abort
SELECT noabort_increasing, noabort_decreasing FROM abbrev_abort_uuids ORDER BY noabort_increasing OFFSET 20000 - 4
================================

SELECT noabort_increasing, noabort_decreasing FROM abbrev_abort_uuids ORDER BY noabort_decreasing NULLS FIRST OFFSET 20000 - 4
================================

CLUSTER abbrev_abort_uuids USING abbrev_abort_uuids__abort_increasing_idx
================================

CLUSTER abbrev_abort_uuids USING abbrev_abort_uuids__abort_decreasing_idx
================================

CLUSTER abbrev_abort_uuids USING abbrev_abort_uuids__noabort_increasing_idx
================================

CLUSTER abbrev_abort_uuids USING abbrev_abort_uuids__noabort_decreasing_idx
================================

DECLARE c SCROLL CURSOR FOR SELECT noabort_decreasing FROM abbrev_abort_uuids ORDER BY noabort_decreasing
================================


-- first and second
FETCH NEXT FROM c
================================

FETCH NEXT FROM c
================================


-- scroll beyond beginning
FETCH BACKWARD FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH NEXT FROM c
================================


-- scroll beyond end end
FETCH LAST FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH NEXT FROM c
================================

FETCH NEXT FROM c
================================

FETCH NEXT FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH NEXT FROM c
================================

DECLARE c SCROLL CURSOR FOR SELECT noabort_decreasing FROM abbrev_abort_uuids ORDER BY noabort_decreasing
================================


-- first and second
FETCH NEXT FROM c
================================

FETCH NEXT FROM c
================================


-- scroll beyond beginning
FETCH BACKWARD FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH NEXT FROM c
================================


-- scroll beyond end end
FETCH LAST FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH NEXT FROM c
================================

FETCH NEXT FROM c
================================

FETCH NEXT FROM c
================================

FETCH BACKWARD FROM c
================================

FETCH NEXT FROM c
================================

-- need a few duplicates for mark/restore to matter
INSERT INTO test_mark_restore(col1, col2, col12)
   SELECT a.i, b.i, a.i * b.i FROM generate_series(1, 500) a(i), generate_series(1, 5) b(i)
================================

:qry
================================

EXPLAIN (COSTS OFF) :qry
================================

:qry
================================


================================

-- check that CIDR rejects invalid input when converting from text:
INSERT INTO INET_TBL (c, i) VALUES (cidr('192.168.1.2/30'), '192.168.1.226')
================================

INSERT INTO INET_TBL (c, i) VALUES (cidr('ffff:ffff:ffff:ffff::/24'), '::192.168.1.226')
================================

SELECT c AS cidr, i AS inet FROM INET_TBL
================================


-- now test some support functions

SELECT i AS inet, host(i), text(i), family(i) FROM INET_TBL
================================

SELECT c AS cidr, abbrev(c) FROM INET_TBL
================================

SELECT c AS cidr, broadcast(c),
  i AS inet, broadcast(i) FROM INET_TBL
================================

SELECT c AS cidr, network(c) AS "network(cidr)",
  i AS inet, network(i) AS "network(inet)" FROM INET_TBL
================================

SELECT c AS cidr, masklen(c) AS "masklen(cidr)",
  i AS inet, masklen(i) AS "masklen(inet)" FROM INET_TBL
================================


SELECT c AS cidr, masklen(c) AS "masklen(cidr)",
  i AS inet, masklen(i) AS "masklen(inet)" FROM INET_TBL
  WHERE masklen(c) <= 8
================================


SELECT c AS cidr, i AS inet FROM INET_TBL
  WHERE c = i
================================


SELECT i, c,
  i < c AS lt, i <= c AS le, i = c AS eq,
  i >= c AS ge, i > c AS gt, i <> c AS ne,
  i << c AS sb, i <<= c AS sbe,
  i >> c AS sup, i >>= c AS spe,
  i && c AS ovr
  FROM INET_TBL
================================


-- check the conversion to/from text and set_netmask
SELECT set_masklen(inet(text(i)), 24) FROM INET_TBL
================================

EXPLAIN (COSTS OFF)
SELECT * FROM inet_tbl WHERE i<<'192.168.1.0/24'::cidr
================================

SELECT * FROM inet_tbl WHERE i<<'192.168.1.0/24'::cidr
================================

EXPLAIN (COSTS OFF)
SELECT * FROM inet_tbl WHERE i<<='192.168.1.0/24'::cidr
================================

SELECT * FROM inet_tbl WHERE i<<='192.168.1.0/24'::cidr
================================

EXPLAIN (COSTS OFF)
SELECT * FROM inet_tbl WHERE '192.168.1.0/24'::cidr >>= i
================================

SELECT * FROM inet_tbl WHERE '192.168.1.0/24'::cidr >>= i
================================

EXPLAIN (COSTS OFF)
SELECT * FROM inet_tbl WHERE '192.168.1.0/24'::cidr >> i
================================

SELECT * FROM inet_tbl WHERE '192.168.1.0/24'::cidr >> i
================================

SELECT * FROM inet_tbl WHERE i << '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i <<= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i && '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i >>= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i >> '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i < '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i <= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i = '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i >= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i > '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i <> '192.168.1.0/24'::cidr ORDER BY i
================================


-- test index-only scans
EXPLAIN (COSTS OFF)
SELECT i FROM inet_tbl WHERE i << '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT i FROM inet_tbl WHERE i << '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i << '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i <<= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i && '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i >>= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i >> '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i < '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i <= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i = '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i >= '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i > '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT * FROM inet_tbl WHERE i <> '192.168.1.0/24'::cidr ORDER BY i
================================


-- test index-only scans
EXPLAIN (COSTS OFF)
SELECT i FROM inet_tbl WHERE i << '192.168.1.0/24'::cidr ORDER BY i
================================

SELECT i FROM inet_tbl WHERE i << '192.168.1.0/24'::cidr ORDER BY i
================================


-- simple tests of inet boolean and arithmetic operators
SELECT i, ~i AS "~i" FROM inet_tbl
================================

SELECT '127.0.0.1'::inet + 257
================================

SELECT ('127.0.0.1'::inet + 257) - 257
================================

SELECT '127::1'::inet + 257
================================

SELECT ('127::1'::inet + 257) - 257
================================

SELECT '127.0.0.2'::inet  - ('127.0.0.2'::inet + 500)
================================

SELECT '127.0.0.2'::inet  - ('127.0.0.2'::inet - 500)
================================

SELECT '127::2'::inet  - ('127::2'::inet + 500)
================================

SELECT '127::2'::inet  - ('127::2'::inet - 500)
================================

-- these should give overflow errors:
SELECT '127.0.0.1'::inet + 10000000000
================================

SELECT '127.0.0.1'::inet - 10000000000
================================

SELECT '126::1'::inet - '127::2'::inet
================================

SELECT '127::1'::inet - '126::2'::inet
================================

-- but not these
SELECT '127::1'::inet + 10000000000
================================

SELECT '127::1'::inet - '127::2'::inet
================================


-- Test inet sortsupport with a variety of boundary inputs:
SELECT a FROM (VALUES
  ('0.0.0.0/0'::inet),
  ('0.0.0.0/1'::inet),
  ('0.0.0.0/32'::inet),
  ('0.0.0.1/0'::inet),
  ('0.0.0.1/1'::inet),
  ('127.126.127.127/0'::inet),
  ('127.127.127.127/0'::inet),
  ('127.128.127.127/0'::inet),
  ('192.168.1.0/24'::inet),
  ('192.168.1.0/25'::inet),
  ('192.168.1.1/23'::inet),
  ('192.168.1.1/5'::inet),
  ('192.168.1.1/6'::inet),
  ('192.168.1.1/25'::inet),
  ('192.168.1.2/25'::inet),
  ('192.168.1.1/26'::inet),
  ('192.168.1.2/26'::inet),
  ('192.168.1.2/23'::inet),
  ('192.168.1.255/5'::inet),
  ('192.168.1.255/6'::inet),
  ('192.168.1.3/1'::inet),
  ('192.168.1.3/23'::inet),
  ('192.168.1.4/0'::inet),
  ('192.168.1.5/0'::inet),
  ('255.0.0.0/0'::inet),
  ('255.1.0.0/0'::inet),
  ('255.2.0.0/0'::inet),
  ('255.255.000.000/0'::inet),
  ('255.255.000.000/0'::inet),
  ('255.255.000.000/15'::inet),
  ('255.255.000.000/16'::inet),
  ('255.255.255.254/32'::inet),
  ('255.255.255.000/32'::inet),
  ('255.255.255.001/31'::inet),
  ('255.255.255.002/31'::inet),
  ('255.255.255.003/31'::inet),
  ('255.255.255.003/32'::inet),
  ('255.255.255.001/32'::inet),
  ('255.255.255.255/0'::inet),
  ('255.255.255.255/0'::inet),
  ('255.255.255.255/0'::inet),
  ('255.255.255.255/1'::inet),
  ('255.255.255.255/16'::inet),
  ('255.255.255.255/16'::inet),
  ('255.255.255.255/31'::inet),
  ('255.255.255.255/32'::inet),
  ('255.255.255.253/32'::inet),
  ('255.255.255.252/32'::inet),
  ('255.3.0.0/0'::inet),
  ('0000:0000:0000:0000:0000:0000:0000:0000/0'::inet),
  ('0000:0000:0000:0000:0000:0000:0000:0000/128'::inet),
  ('0000:0000:0000:0000:0000:0000:0000:0001/128'::inet),
  ('10:23::f1/64'::inet),
  ('10:23::f1/65'::inet),
  ('10:23::ffff'::inet),
  ('127::1'::inet),
  ('127::2'::inet),
  ('8000:0000:0000:0000:0000:0000:0000:0000/1'::inet),
  ('::1:ffff:ffff:ffff:ffff/128'::inet),
  ('::2:ffff:ffff:ffff:ffff/128'::inet),
  ('::4:3:2:0/24'::inet),
  ('::4:3:2:1/24'::inet),
  ('::4:3:2:2/24'::inet),
  ('ffff:83e7:f118:57dc:6093:6d92:689d:58cf/70'::inet),
  ('ffff:84b0:4775:536e:c3ed:7116:a6d6:34f0/44'::inet),
  ('ffff:8566:f84:5867:47f1:7867:d2ba:8a1a/69'::inet),
  ('ffff:8883:f028:7d2:4d68:d510:7d6b:ac43/73'::inet),
  ('ffff:8ae8:7c14:65b3:196:8e4a:89ae:fb30/89'::inet),
  ('ffff:8dd0:646:694c:7c16:7e35:6a26:171/104'::inet),
  ('ffff:8eef:cbf:700:eda3:ae32:f4b4:318b/121'::inet),
  ('ffff:90e7:e744:664:a93:8efe:1f25:7663/122'::inet),
  ('ffff:9597:c69c:8b24:57a:8639:ec78:6026/111'::inet),
  ('ffff:9e86:79ea:f16e:df31:8e4d:7783:532e/88'::inet),
  ('ffff:a0c7:82d3:24de:f762:6e1f:316d:3fb2/23'::inet),
  ('ffff:fffa:ffff:ffff:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:fffb:ffff:ffff:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:fffc:ffff:ffff:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:fffd:ffff:ffff:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:fffe:ffff:ffff:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:fffa:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:fffb:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:fffc:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:fffd::/128'::inet),
  ('ffff:ffff:ffff:fffd:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:fffe::/128'::inet),
  ('ffff:ffff:ffff:fffe:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:ffff:4:3:2:0/24'::inet),
  ('ffff:ffff:ffff:ffff:4:3:2:1/24'::inet),
  ('ffff:ffff:ffff:ffff:4:3:2:2/24'::inet),
  ('ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff/0'::inet),
  ('ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff/128'::inet)
) AS i(a) ORDER BY a
================================


================================


================================


-- create page w/ free space in range [nearlyEmptyFreeSpace, MaxHeapTupleSize)
INSERT INTO large_tuple_test (select 1, NULL)
================================


-- should still fit on the page
INSERT INTO large_tuple_test (select 2, repeat('a', 1000))
================================


-- add small record to the second page
INSERT INTO large_tuple_test (select 3, NULL)
================================


-- now this tuple won't fit on the second page, but the insert should
-- still succeed by extending the relation
INSERT INTO large_tuple_test (select 4, repeat('a', 8126))
================================


--
-- check indirection (field/array assignment), cf bug #14265
--
-- these tests are aware that transformInsertStmt has 3 separate code paths
--

create type insert_test_type as (if1 int, if2 text[])
================================


insert into inserttest (f2[1], f2[2]) values (1,2)
================================

insert into inserttest (f2[1], f2[2]) values (3,4), (5,6)
================================

insert into inserttest (f2[1], f2[2]) select 7,8
================================

insert into inserttest (f2[1], f2[2]) values (1,default)
================================

insert into inserttest (f3.if1, f3.if2) select 3, '{baz,quux}'
================================
  -- not supported

insert into inserttest (f3.if2[1], f3.if2[2]) values ('foo', 'bar')
================================

insert into inserttest (f3.if2[1], f3.if2[2]) values ('foo', 'bar'), ('baz', 'quux')
================================

insert into inserttest (f3.if2[1], f3.if2[2]) select 'bear', 'beer'
================================


insert into inserttest (f4[1].if2[1], f4[1].if2[2]) values ('foo', 'bar')
================================

insert into inserttest (f4[1].if2[1], f4[1].if2[2]) values ('foo', 'bar'), ('baz', 'quux')
================================

insert into inserttest (f4[1].if2[1], f4[1].if2[2]) select 'bear', 'beer'
================================

\d+ inserttest2

drop table inserttest2
================================

drop type insert_test_type
================================

insert into list_parted select 'Ff', s.a from generate_series(1, 29) s(a)
================================

insert into list_parted select 'gg', s.a from generate_series(1, 9) s(a)
================================

$$ language sql immutable
================================


create operator class part_test_int4_ops
for type int4
using hash as
operator 1 =,
function 2 part_hashint4_noop(int4, int8)
================================


create operator class part_test_text_ops
for type text
using hash as
operator 1 =,
function 2 part_hashtext_length(text, int8)
================================


-- direct insert of values divisible by 4 - ok
================================

-- fail
================================


-- test \d+ output on a table which has both partitioned and unpartitioned
-- partitions
\d+ list_parted

-- cleanup
drop table range_parted, list_parted
================================

\d+ part_default
insert into part_default values (null)
================================

  return NEW
================================

$$
language plpgsql
================================

drop trigger mlparted11_trig on mlparted11
================================
 return new
================================
 $$ language plpgsql
================================
 return new
================================
 end$$ language plpgsql
================================

with result as (insert into brtrigpartcon values (1, 'hi there') returning 1)
  insert into inserttest3 (f3) select * from result
================================
 return NULL
================================
 end$$ language plpgsql
================================

1	baz
2	qux
\.
select tableoid::regclass, * from donothingbrtrig_test
================================


\d+ mcrparted
\d+ mcrparted1_lt_b
\d+ mcrparted2_b
\d+ mcrparted3_c_to_common
\d+ mcrparted4_common_lt_0
\d+ mcrparted5_common_0_to_10
\d+ mcrparted6_common_ge_10
\d+ mcrparted7_gt_common_lt_d
\d+ mcrparted8_ge_d

insert into mcrparted values ('aaa', 0), ('b', 0), ('bz', 10), ('c', -10),
    ('comm', -10), ('common', -10), ('common', 0), ('common', 10),
    ('commons', 0), ('d', -10), ('e', 0)
================================


================================


================================

1	1	1	1	1	1	1	1
1	1	1	1	1	1	1	2
1	1	1	1	1	1	2	2
1	1	1	1	1	2	2	2
1	1	1	1	2	2	2	2
1	1	1	2	2	2	2	2
1	1	2	2	2	2	2	2
1	2	2	2	2	2	2	2
2	2	2	2	2	2	2	2
\.

create temp table gstest3 (a integer, b integer, c integer, d integer)
================================

1	1	1	1
2	2	2	2
\.
alter table gstest3 add primary key (a)
================================

insert into gstest4
values (1,1,b'0000','1'), (2,2,b'0001','1'),
       (3,4,b'0010','2'), (4,8,b'0011','2'),
       (5,16,b'0000','2'), (6,32,b'0001','2'),
       (7,64,b'0010','1'), (8,128,b'0011','1')
================================

  $f$ language plpgsql
================================

select array(select row(v.a,s1.*) from (select two,four, count(*) from onek group by cube(two,four) order by two,four) s1) from (values (1),(2)) v(a)
================================

select array(select row(v.a,s1.*) from (select two,four, count(*) from onek group by cube(two,four) order by two,four) s1) from (values (1),(2)) v(a)
================================

select array(select row(v.a,s1.*) from (select two,four, count(*) from onek group by grouping sets(two,four) order by two,four) s1) from (values (1),(2)) v(a)
================================


insert into bug_16784 select g/10, g from generate_series(1,40) g
================================


--
-- Compare results between plans using sorting and plans using hash
-- aggregation. Force spilling in both cases by setting work_mem low
-- and altering the statistics.
--

create table gs_data_1 as
select g%1000 as g1000, g%100 as g100, g%10 as g10, g
   from generate_series(0,1999) g
================================


create table gs_group_1 as
select g100, g10, sum(g::numeric), count(*), max(g::text)
from gs_data_1 group by cube (g1000, g100,g10)
================================


create table gs_hash_1 as
select g100, g10, sum(g::numeric), count(*), max(g::text)
from gs_data_1 group by cube (g1000, g100,g10)
================================


-- GROUP BY DISTINCT

-- "normal" behavior...
select a, b, c
from (values (1, 2, 3), (4, null, 6), (7, 8, 9)) as t (a, b, c)
group by all rollup(a, b), rollup(a, c)
order by a, b, c
================================


-- "group by distinct" behavior...
select a, b, c
from (values (1, 2, 3), (4, null, 6), (7, 8, 9)) as t (a, b, c)
group by distinct rollup(a, b), rollup(a, c)
order by a, b, c
================================


-- end

================================


CREATE FOREIGN DATA WRAPPER dummy
================================

COMMENT ON FOREIGN DATA WRAPPER dummy IS 'useless'
================================

CREATE FOREIGN DATA WRAPPER postgresql VALIDATOR postgresql_fdw_validator
================================


-- CREATE FOREIGN DATA WRAPPER
CREATE FOREIGN DATA WRAPPER foo VALIDATOR bar
================================
            -- ERROR
CREATE FOREIGN DATA WRAPPER foo
================================

\dew

CREATE FOREIGN DATA WRAPPER foo
================================
 -- duplicate
DROP FOREIGN DATA WRAPPER foo
================================

CREATE FOREIGN DATA WRAPPER foo OPTIONS (testing '1')
================================

\dew+

DROP FOREIGN DATA WRAPPER foo
================================

CREATE FOREIGN DATA WRAPPER foo OPTIONS (testing '1', testing '2')
================================
   -- ERROR
CREATE FOREIGN DATA WRAPPER foo OPTIONS (testing '1', another '2')
================================

\dew+

DROP FOREIGN DATA WRAPPER foo
================================

CREATE FOREIGN DATA WRAPPER foo
================================

CREATE FOREIGN DATA WRAPPER foo VALIDATOR postgresql_fdw_validator
================================

\dew+

-- HANDLER related checks
CREATE FUNCTION invalid_fdw_handler() RETURNS int LANGUAGE SQL AS 'SELECT 1
================================
'
================================

CREATE FOREIGN DATA WRAPPER test_fdw HANDLER invalid_fdw_handler
================================
  -- ERROR
CREATE FOREIGN DATA WRAPPER test_fdw HANDLER test_fdw_handler HANDLER invalid_fdw_handler
================================
  -- ERROR
CREATE FOREIGN DATA WRAPPER test_fdw HANDLER test_fdw_handler
================================

DROP FOREIGN DATA WRAPPER test_fdw
================================


-- ALTER FOREIGN DATA WRAPPER
ALTER FOREIGN DATA WRAPPER foo OPTIONS (nonexistent 'fdw')
================================
         -- ERROR

ALTER FOREIGN DATA WRAPPER foo
================================
                             -- ERROR
ALTER FOREIGN DATA WRAPPER foo VALIDATOR bar
================================
               -- ERROR
ALTER FOREIGN DATA WRAPPER foo NO VALIDATOR
================================

\dew+

ALTER FOREIGN DATA WRAPPER foo OPTIONS (a '1', b '2')
================================

ALTER FOREIGN DATA WRAPPER foo OPTIONS (SET c '4')
================================
         -- ERROR
ALTER FOREIGN DATA WRAPPER foo OPTIONS (DROP c)
================================
            -- ERROR
ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD x '1', DROP x)
================================

\dew+

ALTER FOREIGN DATA WRAPPER foo OPTIONS (DROP a, SET b '3', ADD c '4')
================================

\dew+

ALTER FOREIGN DATA WRAPPER foo OPTIONS (a '2')
================================

ALTER FOREIGN DATA WRAPPER foo OPTIONS (b '4')
================================
             -- ERROR
\dew+

SET ROLE regress_test_role
================================

ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD d '5')
================================

ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD d '5')
================================

\dew+

ALTER FOREIGN DATA WRAPPER foo OWNER TO regress_test_role
================================
  -- ERROR
ALTER FOREIGN DATA WRAPPER foo OWNER TO regress_test_role_super
================================

ALTER FOREIGN DATA WRAPPER foo OPTIONS (ADD e '6')
================================

\dew+

ALTER FOREIGN DATA WRAPPER foo RENAME TO foo1
================================

\dew+
ALTER FOREIGN DATA WRAPPER foo1 RENAME TO foo
================================


-- HANDLER related checks
ALTER FOREIGN DATA WRAPPER foo HANDLER invalid_fdw_handler
================================
  -- ERROR
ALTER FOREIGN DATA WRAPPER foo HANDLER test_fdw_handler HANDLER anything
================================
  -- ERROR
ALTER FOREIGN DATA WRAPPER foo HANDLER test_fdw_handler
================================


-- DROP FOREIGN DATA WRAPPER
DROP FOREIGN DATA WRAPPER nonexistent
================================
                      -- ERROR
DROP FOREIGN DATA WRAPPER IF EXISTS nonexistent
================================

\dew+

DROP ROLE regress_test_role_super
================================

DROP FOREIGN DATA WRAPPER foo
================================

\dew+

CREATE FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s1 FOREIGN DATA WRAPPER foo
================================

COMMENT ON SERVER s1 IS 'foreign server'
================================
 -- NOTICE
\dew+
\des+
\deu+
DROP FOREIGN DATA WRAPPER foo
================================

DROP FOREIGN DATA WRAPPER foo CASCADE
================================

DROP FOREIGN DATA WRAPPER foo CASCADE
================================

\dew+
\des+
\deu+

-- exercise CREATE SERVER
CREATE SERVER s1 FOREIGN DATA WRAPPER foo
================================
                  -- ERROR
CREATE FOREIGN DATA WRAPPER foo OPTIONS ("test wrapper" 'true')
================================

CREATE SERVER s1 FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s1 FOREIGN DATA WRAPPER foo
================================
                  -- ERROR
CREATE SERVER IF NOT EXISTS s1 FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s3 TYPE 'oracle' FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s5 VERSION '15.0' FOREIGN DATA WRAPPER foo
================================

\des+
SET ROLE regress_test_role
================================

CREATE SERVER t1 FOREIGN DATA WRAPPER foo
================================

CREATE SERVER t1 FOREIGN DATA WRAPPER foo
================================

\des+

REVOKE USAGE ON FOREIGN DATA WRAPPER foo FROM regress_test_role
================================

CREATE SERVER t2 FOREIGN DATA WRAPPER foo
================================

CREATE SERVER t2 FOREIGN DATA WRAPPER foo
================================

\des+
RESET ROLE
================================


-- ALTER SERVER
ALTER SERVER s0
================================

\des+
SET ROLE regress_test_role
================================
                              -- ERROR
ALTER SERVER s1 OWNER TO regress_test_role
================================

ALTER SERVER s1 OWNER TO regress_test_role
================================

ALTER SERVER s1 OWNER TO regress_test_role2
================================

ALTER SERVER s1 OWNER TO regress_test_indirect
================================

ALTER SERVER s1 OWNER TO regress_test_indirect
================================

ALTER SERVER s1 OWNER TO regress_test_indirect
================================
                            -- ERROR
\des+

ALTER SERVER s8 RENAME to s8new
================================

\des+
ALTER SERVER s8new RENAME to s8
================================


-- DROP SERVER
DROP SERVER nonexistent
================================
                                    -- ERROR
DROP SERVER IF EXISTS nonexistent
================================

\des
SET ROLE regress_test_role
================================

DROP SERVER s2
================================
                                             -- ERROR
DROP SERVER s1
================================

\des
ALTER SERVER s2 OWNER TO regress_test_role
================================

DROP SERVER s2
================================

\des
CREATE USER MAPPING FOR current_user SERVER s3
================================

\deu
DROP SERVER s3
================================

\des
\deu

-- CREATE USER MAPPING
CREATE USER MAPPING FOR regress_test_missing_role SERVER s1
================================

ALTER SERVER s5 OWNER TO regress_test_role
================================

ALTER SERVER s6 OWNER TO regress_test_indirect
================================


ALTER SERVER t1 OWNER TO regress_test_indirect
================================

\deu

-- ALTER USER MAPPING
ALTER USER MAPPING FOR regress_test_missing_role SERVER s4 OPTIONS (gotcha 'true')
================================

\deu+

-- DROP USER MAPPING
DROP USER MAPPING FOR regress_test_missing_role SERVER s4
================================

DROP SERVER s7
================================

\deu

-- CREATE FOREIGN TABLE
CREATE SCHEMA foreign_schema
================================

CREATE SERVER s0 FOREIGN DATA WRAPPER dummy
================================

CREATE FOREIGN TABLE ft1 ()
================================
                                    -- ERROR
CREATE FOREIGN TABLE ft1 () SERVER no_server
================================
                   -- ERROR
CREATE FOREIGN TABLE ft1 (
	c1 integer OPTIONS ("param 1" 'val1') PRIMARY KEY,
	c2 text OPTIONS (param2 'val2', param3 'val3'),
	c3 date
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

CREATE FOREIGN TABLE ft1 (
	c1 integer OPTIONS ("param 1" 'val1') REFERENCES ref_table (id),
	c2 text OPTIONS (param2 'val2', param3 'val3'),
	c3 date
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

CREATE FOREIGN TABLE ft1 (
	c1 integer OPTIONS ("param 1" 'val1') NOT NULL,
	c2 text OPTIONS (param2 'val2', param3 'val3'),
	c3 date,
	UNIQUE (c3)
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================
 -- ERROR
CREATE FOREIGN TABLE ft1 (
	c1 integer OPTIONS ("param 1" 'val1') NOT NULL,
	c2 text OPTIONS (param2 'val2', param3 'val3') CHECK (c2 <> ''),
	c3 date,
	CHECK (c3 BETWEEN '1994-01-01'::date AND '1994-01-31'::date)
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

COMMENT ON FOREIGN TABLE ft1 IS 'ft1'
================================

COMMENT ON COLUMN ft1.c1 IS 'ft1.c1'
================================

\d+ ft1
\det+
CREATE INDEX id_ft1_c2 ON ft1 (c2)
================================

CREATE FOREIGN TABLE ft_part1
  PARTITION OF lt1 FOR VALUES FROM (0) TO (1000) SERVER s0
================================

CREATE FOREIGN TABLE ft_part1
  PARTITION OF lt1 FOR VALUES FROM (0) TO (1000) SERVER s0
================================

CREATE FOREIGN TABLE ft_part2 (a INT) SERVER s0
================================

DROP FOREIGN TABLE ft_part1, ft_part2
================================

CREATE FOREIGN TABLE ft_part1
  PARTITION OF lt1 FOR VALUES FROM (0) TO (1000) SERVER s0
================================
     -- ERROR
CREATE FOREIGN TABLE ft_part2 (a INT NOT NULL) SERVER s0
================================

DROP FOREIGN TABLE ft_part2
================================

CREATE FOREIGN TABLE ft_part_1_1
  PARTITION OF lt1_part1 FOR VALUES FROM (0) TO (100) SERVER s0
================================

CREATE FOREIGN TABLE ft_part_1_2 (a INT) SERVER s0
================================

DROP FOREIGN TABLE ft_part_1_1, ft_part_1_2
================================

CREATE FOREIGN TABLE ft_part_1_1
  PARTITION OF lt1_part1 FOR VALUES FROM (0) TO (100) SERVER s0
================================

CREATE FOREIGN TABLE ft_part_1_2 (a INT NOT NULL) SERVER s0
================================

DROP FOREIGN TABLE ft_part_1_2
================================


-- ALTER FOREIGN TABLE
COMMENT ON FOREIGN TABLE ft1 IS 'foreign table'
================================

COMMENT ON FOREIGN TABLE ft1 IS NULL
================================

COMMENT ON COLUMN ft1.c1 IS 'foreign column'
================================

COMMENT ON COLUMN ft1.c1 IS NULL
================================


ALTER FOREIGN TABLE ft1 ADD COLUMN c4 integer
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c5 integer DEFAULT 0
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c6 integer
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c7 integer NOT NULL
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c8 integer
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c9 integer
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c10 integer OPTIONS (p1 'v1')
================================


ALTER FOREIGN TABLE ft1 ALTER COLUMN c4 SET DEFAULT 0
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c5 DROP DEFAULT
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c6 SET NOT NULL
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c7 DROP NOT NULL
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 TYPE char(10) USING '0'
================================
 -- ERROR
ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 TYPE char(10)
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 SET DATA TYPE text
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN xmin OPTIONS (ADD p1 'v1')
================================
 -- ERROR
ALTER FOREIGN TABLE ft1 ALTER COLUMN c7 OPTIONS (ADD p1 'v1', ADD p2 'v2'),
                        ALTER COLUMN c8 OPTIONS (ADD p1 'v1', ADD p2 'v2')
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 OPTIONS (SET p2 'V2', DROP p1)
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c1 SET STATISTICS 10000
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c1 SET (n_distinct = 100)
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 SET STATISTICS -1
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 SET STORAGE PLAIN
================================

\d+ ft1
-- can't change the column type if it's used elsewhere
CREATE TABLE use_ft1_column_type (x ft1)
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 SET DATA TYPE integer
================================

ALTER FOREIGN TABLE ft1 ADD PRIMARY KEY (c7)
================================
                   -- ERROR
ALTER FOREIGN TABLE ft1 ADD CONSTRAINT ft1_c9_check CHECK (c9 < 0) NOT VALID
================================

ALTER FOREIGN TABLE ft1 ALTER CONSTRAINT ft1_c9_check DEFERRABLE
================================
 -- ERROR
ALTER FOREIGN TABLE ft1 DROP CONSTRAINT ft1_c9_check
================================

ALTER FOREIGN TABLE ft1 DROP CONSTRAINT no_const
================================
               -- ERROR
ALTER FOREIGN TABLE ft1 DROP CONSTRAINT IF EXISTS no_const
================================

ALTER FOREIGN TABLE ft1 OWNER TO regress_test_role
================================

ALTER FOREIGN TABLE ft1 OPTIONS (DROP delimiter, SET quote '~', ADD escape '@')
================================

ALTER FOREIGN TABLE ft1 DROP COLUMN no_column
================================
                  -- ERROR
ALTER FOREIGN TABLE ft1 DROP COLUMN IF EXISTS no_column
================================

ALTER FOREIGN TABLE ft1 DROP COLUMN c9
================================

ALTER FOREIGN TABLE ft1 SET SCHEMA foreign_schema
================================

ALTER FOREIGN TABLE ft1 SET TABLESPACE ts
================================
                      -- ERROR
ALTER FOREIGN TABLE foreign_schema.ft1 RENAME c1 TO foreign_column_1
================================

ALTER FOREIGN TABLE foreign_schema.ft1 RENAME TO foreign_table_1
================================

\d foreign_schema.foreign_table_1

-- alter noexisting table
ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ADD COLUMN c4 integer
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ADD COLUMN c6 integer
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ADD COLUMN c7 integer NOT NULL
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ADD COLUMN c8 integer
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ADD COLUMN c9 integer
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ADD COLUMN c10 integer OPTIONS (p1 'v1')
================================


ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ALTER COLUMN c6 SET NOT NULL
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ALTER COLUMN c7 DROP NOT NULL
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ALTER COLUMN c8 TYPE char(10)
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ALTER COLUMN c8 SET DATA TYPE text
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ALTER COLUMN c7 OPTIONS (ADD p1 'v1', ADD p2 'v2'),
                        ALTER COLUMN c8 OPTIONS (ADD p1 'v1', ADD p2 'v2')
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 ALTER COLUMN c8 OPTIONS (SET p2 'V2', DROP p1)
================================


ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 DROP CONSTRAINT IF EXISTS no_const
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 DROP CONSTRAINT ft1_c1_check
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 OWNER TO regress_test_role
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 OPTIONS (DROP delimiter, SET quote '~', ADD escape '@')
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 DROP COLUMN IF EXISTS no_column
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 DROP COLUMN c9
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 SET SCHEMA foreign_schema
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 RENAME c1 TO foreign_column_1
================================

ALTER FOREIGN TABLE IF EXISTS doesnt_exist_ft1 RENAME TO foreign_table_1
================================

ALTER FOREIGN DATA WRAPPER foo VALIDATOR postgresql_fdw_validator
================================

CREATE FOREIGN DATA WRAPPER foobar
================================
                             -- ERROR
ALTER FOREIGN DATA WRAPPER foo OPTIONS (gotcha 'true')
================================
         -- ERROR
ALTER FOREIGN DATA WRAPPER foo OWNER TO regress_unprivileged_role
================================
 -- ERROR
DROP FOREIGN DATA WRAPPER foo
================================
   -- ERROR
CREATE SERVER s9 FOREIGN DATA WRAPPER foo
================================
                                  -- ERROR
ALTER SERVER s4 OWNER TO regress_unprivileged_role
================================
             -- ERROR
DROP SERVER s4
================================

CREATE FOREIGN DATA WRAPPER foobar
================================
                             -- ERROR
ALTER FOREIGN DATA WRAPPER foo OPTIONS (gotcha 'true')
================================
         -- ERROR
DROP FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s9 FOREIGN DATA WRAPPER postgresql
================================
                                  -- ERROR
DROP SERVER s6
================================
   -- ERROR
CREATE SERVER s10 FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s9 FOREIGN DATA WRAPPER foo
================================

CREATE SERVER s10 FOREIGN DATA WRAPPER foo
================================

-- owner of server can see some option fields
\deu+
RESET ROLE
================================

-- superuser can see all option fields
\deu+
-- unprivileged user cannot see any option field
SET ROLE regress_unprivileged_role
================================

\deu+
RESET ROLE
================================


CREATE CONSTRAINT TRIGGER trigtest_constraint AFTER INSERT OR UPDATE OR DELETE
ON foreign_schema.foreign_table_1
FOR EACH ROW
EXECUTE PROCEDURE dummy_trigger()
================================


ALTER FOREIGN TABLE foreign_schema.foreign_table_1
	DISABLE TRIGGER trigtest_before_stmt
================================

ALTER FOREIGN TABLE foreign_schema.foreign_table_1
	ENABLE TRIGGER trigtest_before_stmt
================================


DROP TRIGGER trigtest_before_stmt ON foreign_schema.foreign_table_1
================================

DROP TRIGGER trigtest_before_row ON foreign_schema.foreign_table_1
================================

DROP TRIGGER trigtest_after_stmt ON foreign_schema.foreign_table_1
================================

DROP TRIGGER trigtest_after_row ON foreign_schema.foreign_table_1
================================

CREATE FOREIGN TABLE ft2 () INHERITS (fd_pt1)
  SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

\d+ fd_pt1
\d+ ft2
DROP FOREIGN TABLE ft2
================================

\d+ fd_pt1
CREATE FOREIGN TABLE ft2 (
	c1 integer NOT NULL,
	c2 text,
	c3 date
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

\d+ ft2
ALTER FOREIGN TABLE ft2 INHERIT fd_pt1
================================

\d+ fd_pt1
\d+ ft2
CREATE TABLE ct3() INHERITS(ft2)
================================

CREATE FOREIGN TABLE ft3 (
	c1 integer NOT NULL,
	c2 text,
	c3 date
) INHERITS(ft2)
  SERVER s0
================================

\d+ ft2
\d+ ct3
\d+ ft3

-- add attributes recursively
ALTER TABLE fd_pt1 ADD COLUMN c4 integer
================================

\d+ fd_pt1
\d+ ft2
\d+ ct3
\d+ ft3

-- alter attributes recursively
ALTER TABLE fd_pt1 ALTER COLUMN c4 SET DEFAULT 0
================================

ALTER TABLE fd_pt1 ALTER COLUMN c8 SET STATISTICS -1
================================

\d+ fd_pt1
\d+ ft2

-- drop attributes recursively
ALTER TABLE fd_pt1 DROP COLUMN c4
================================

\d+ fd_pt1
\d+ ft2

-- add constraints recursively
ALTER TABLE fd_pt1 ADD CONSTRAINT fd_pt1chk1 CHECK (c1 > 0) NO INHERIT
================================

-- child does not inherit NO INHERIT constraints
\d+ fd_pt1
\d+ ft2
DROP FOREIGN TABLE ft2
================================
 -- ERROR
DROP FOREIGN TABLE ft2 CASCADE
================================

CREATE FOREIGN TABLE ft2 (
	c1 integer NOT NULL,
	c2 text,
	c3 date
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

-- child must have parent's INHERIT constraints
ALTER FOREIGN TABLE ft2 INHERIT fd_pt1
================================
                            -- ERROR
ALTER FOREIGN TABLE ft2 ADD CONSTRAINT fd_pt1chk2 CHECK (c2 <> '')
================================

ALTER FOREIGN TABLE ft2 INHERIT fd_pt1
================================

-- child does not inherit NO INHERIT constraints
\d+ fd_pt1
\d+ ft2

-- drop constraints recursively
ALTER TABLE fd_pt1 DROP CONSTRAINT fd_pt1chk1 CASCADE
================================

\d+ fd_pt1
\d+ ft2
-- VALIDATE CONSTRAINT need do nothing on foreign tables
ALTER TABLE fd_pt1 VALIDATE CONSTRAINT fd_pt1chk3
================================

\d+ fd_pt1
\d+ ft2

-- changes name of an attribute recursively
ALTER TABLE fd_pt1 RENAME COLUMN c1 TO f1
================================

\d+ fd_pt1
\d+ ft2

-- TRUNCATE doesn't work on foreign tables, either directly or recursively
TRUNCATE ft2
================================


-- IMPORT FOREIGN SCHEMA
IMPORT FOREIGN SCHEMA s1 FROM SERVER s9 INTO public
================================
 -- ERROR
IMPORT FOREIGN SCHEMA s1 LIMIT TO (t1) FROM SERVER s9 INTO public
================================
 --ERROR
IMPORT FOREIGN SCHEMA s1 EXCEPT (t1) FROM SERVER s9 INTO public
================================
 -- ERROR
IMPORT FOREIGN SCHEMA s1 EXCEPT (t1, t2) FROM SERVER s9 INTO public
OPTIONS (option1 'value1', option2 'value2')
================================
 -- ERROR

-- DROP FOREIGN TABLE
DROP FOREIGN TABLE no_table
================================
                                    -- ERROR
DROP FOREIGN TABLE IF EXISTS no_table
================================

DROP FOREIGN TABLE foreign_schema.foreign_table_1
================================


-- REASSIGN OWNED/DROP OWNED of foreign objects
REASSIGN OWNED BY regress_test_role TO regress_test_role2
================================

DROP OWNED BY regress_test_role2
================================

DROP OWNED BY regress_test_role2 CASCADE
================================

CREATE FOREIGN TABLE fd_pt2_1 PARTITION OF fd_pt2 FOR VALUES IN (1)
  SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

\d+ fd_pt2
\d+ fd_pt2_1

-- partition cannot have additional columns
DROP FOREIGN TABLE fd_pt2_1
================================

CREATE FOREIGN TABLE fd_pt2_1 (
	c1 integer NOT NULL,
	c2 text,
	c3 date,
	c4 char
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

\d+ fd_pt2_1
ALTER TABLE fd_pt2 ATTACH PARTITION fd_pt2_1 FOR VALUES IN (1)
================================
       -- ERROR

DROP FOREIGN TABLE fd_pt2_1
================================

\d+ fd_pt2
CREATE FOREIGN TABLE fd_pt2_1 (
	c1 integer NOT NULL,
	c2 text,
	c3 date
) SERVER s0 OPTIONS (delimiter ',', quote '"', "be quoted" 'value')
================================

\d+ fd_pt2_1
-- no attach partition validation occurs for foreign tables
ALTER TABLE fd_pt2 ATTACH PARTITION fd_pt2_1 FOR VALUES IN (1)
================================

\d+ fd_pt2
\d+ fd_pt2_1

-- cannot add column to a partition
ALTER TABLE fd_pt2_1 ADD c4 char
================================

\d+ fd_pt2
\d+ fd_pt2_1

-- cannot drop inherited NOT NULL constraint from a partition
ALTER TABLE fd_pt2_1 ALTER c1 DROP NOT NULL
================================

\d+ fd_pt2
\d+ fd_pt2_1
ALTER TABLE fd_pt2 ATTACH PARTITION fd_pt2_1 FOR VALUES IN (1)
================================
       -- ERROR
ALTER FOREIGN TABLE fd_pt2_1 ALTER c2 SET NOT NULL
================================

\d+ fd_pt2
\d+ fd_pt2_1
ALTER TABLE fd_pt2 ATTACH PARTITION fd_pt2_1 FOR VALUES IN (1)
================================
       -- ERROR
ALTER FOREIGN TABLE fd_pt2_1 ADD CONSTRAINT fd_pt2chk1 CHECK (c1 > 0)
================================
  -- ERROR

DROP FOREIGN TABLE fd_pt2_1
================================

CREATE FOREIGN TABLE foreign_part PARTITION OF temp_parted DEFAULT
  SERVER s0
================================
  -- ERROR
CREATE FOREIGN TABLE foreign_part (a int) SERVER s0
================================
  -- ERROR
DROP FOREIGN TABLE foreign_part
================================


-- Cleanup
DROP SCHEMA foreign_schema CASCADE
================================

DROP FOREIGN DATA WRAPPER foo CASCADE
================================

DROP FOREIGN DATA WRAPPER postgresql CASCADE
================================

DROP FOREIGN DATA WRAPPER dummy CASCADE
================================

\c
DROP ROLE regress_foreign_data_user
================================


================================



-- ******************testing built-in type bool********************

-- check bool input syntax

SELECT true AS true
================================


SELECT false AS false
================================


SELECT bool 't' AS true
================================


SELECT bool '   f           ' AS false
================================


SELECT bool 'true' AS true
================================


SELECT bool 'false' AS false
================================


SELECT bool 'y' AS true
================================


SELECT bool 'yes' AS true
================================


SELECT bool 'n' AS false
================================


SELECT bool 'no' AS false
================================


SELECT bool 'on' AS true
================================


SELECT bool 'off' AS false
================================


SELECT bool 'of' AS false
================================


SELECT bool '1' AS true
================================


SELECT bool '0' AS false
================================


-- and, or, not in qualifications

SELECT bool 't' or bool 'f' AS true
================================


SELECT bool 't' and bool 'f' AS false
================================


SELECT not bool 'f' AS true
================================


SELECT bool 't' = bool 'f' AS false
================================


SELECT bool 't' <> bool 'f' AS true
================================


SELECT bool 't' > bool 'f' AS true
================================


SELECT bool 't' >= bool 'f' AS true
================================


SELECT bool 'f' < bool 't' AS true
================================


SELECT bool 'f' <= bool 't' AS true
================================


-- explicit casts to/from text
SELECT 'TrUe'::text::boolean AS true, 'fAlse'::text::boolean AS false
================================

SELECT '    true   '::text::boolean AS true,
       '     FALSE'::text::boolean AS false
================================

SELECT true::boolean::text AS true, false::boolean::text AS false
================================

\pset null '(null)'

-- AND expression need to return null if there's any nulls and not all
-- of the value are true
SELECT istrue AND isnul AND istrue FROM booltbl4
================================


================================

        -- In sort output, the above won't match units-suffixed numbers
        ln := regexp_replace(ln, '\m\d+kB', 'NkB', 'g')
================================

        -- Ignore text-mode buffers output because it varies depending
        -- on the system state
        CONTINUE WHEN (ln ~ ' +Buffers: .*')
================================

        -- Ignore text-mode "Planning:" line because whether it's output
        -- varies depending on the system state
        CONTINUE WHEN (ln = 'Planning:')
================================

        return next ln
================================

$$
================================

    ln text
================================

        data := data || ln
================================

    return data::jsonb
================================

$$
================================


create function pg_temp.mysin(float8) returns float8 language plpgsql
as 'begin return sin($1)
================================


================================


-- empty distinct list isn't OK
select distinct from pg_database
================================



--
-- DELETE

-- missing relation name (this had better not wildcard!)
delete from
================================



--
-- DROP

-- missing relation name (this had better not wildcard!)
drop table
================================



--
-- ALTER TABLE

-- relation renaming

-- missing relation name
alter table rename
================================



--
-- CREATE AGGREGATE

-- sfunc/finalfunc type disagreement
create aggregate newavg2 (sfunc = int4pl,
			  basetype = int4,
			  stype = int4,
			  finalfunc = int2um,
			  initcond = '0')
================================


-- left out basetype
create aggregate newcnt1 (sfunc = int4inc,
			  stype = int4,
			  initcond = '0')
================================



--
-- DROP INDEX

-- missing index name
drop index
================================


-- bad index name
drop index 314159
================================



--
-- DROP AGGREGATE

-- missing aggregate name
drop aggregate
================================


-- missing aggregate type
drop aggregate newcnt1
================================


-- bad aggregate name
drop aggregate 314159 (int)
================================


-- bad aggregate type
drop aggregate newcnt (nonesuch)
================================


-- no such aggregate
drop aggregate nonesuch (int4)
================================


-- no such aggregate for type
drop aggregate newcnt (float4)
================================



--
-- DROP FUNCTION

-- missing function name
drop function ()
================================


-- bad function name
drop function 314159()
================================



--
-- DROP TYPE

-- missing type name
drop type
================================


-- bad type name
drop type 314159
================================


-- no such type
drop type nonesuch
================================



--
-- DROP OPERATOR

-- missing everything
drop operator
================================


-- bad operator name
drop operator equals
================================


-- missing type list
drop operator ===
================================


-- missing parentheses
drop operator int4, int4
================================


-- missing operator name
drop operator (int4, int4)
================================


-- missing type list contents
drop operator === ()
================================


-- no such operator
drop operator === (int4)
================================


-- no such operator by that name
drop operator === (int4, int4)
================================


-- no such type1
drop operator = (nonesuch)
================================


-- no such type1
drop operator = ( , int4)
================================


-- no such type1
drop operator = (nonesuch, int4)
================================


-- no such type2
drop operator = (int4, nonesuch)
================================


-- no such type2
drop operator = (int4, )
================================



--
-- DROP RULE

-- missing rule name
drop rule
================================


-- bad rule name
drop rule 314159
================================


-- no such rule
drop rule nonesuch on noplace
================================


-- these postquel variants are no longer supported
drop tuple rule nonesuch
================================

drop instance rule nonesuch on noplace
================================

drop rewrite rule nonesuch
================================



--
-- Test psql's reporting of syntax error location
--

xxx
================================


CREATE foo
================================


CREATE TABLE 
================================


CREATE TABLE
\g

INSERT INTO foo VALUES(123) foo
================================


INSERT INTO 123
VALUES(123)
================================


-- with a tab
CREATE TABLE foo
  (id INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY,
	id3 INTEGER NOT NUL,
   id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL)
================================


-- long line to be truncated on the left
CREATE TABLE foo(id INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY, id3 INTEGER NOT NUL,
id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL)
================================


-- long line to be truncated on the right
CREATE TABLE foo(
id3 INTEGER NOT NUL, id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL, id INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY)
================================


-- long line to be truncated both ways
CREATE TABLE foo(id INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY, id3 INTEGER NOT NUL, id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL)
================================


-- long line to be truncated on the left, many lines
CREATE
TEMPORARY
TABLE
foo(id INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY, id3 INTEGER NOT NUL,
id4 INT4
UNIQUE
NOT
NULL,
id5 TEXT
UNIQUE
NOT
NULL)

================================


-- long line to be truncated on the right, many lines
CREATE
TEMPORARY
TABLE
foo(
id3 INTEGER NOT NUL, id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL, id INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY)

================================


-- long line to be truncated both ways, many lines
CREATE
TEMPORARY
TABLE
foo
(id
INT4
UNIQUE NOT NULL, idx INT4 UNIQUE NOT NULL, idy INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY, id3 INTEGER NOT NUL, id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL,
idz INT4 UNIQUE NOT NULL,
idv INT4 UNIQUE NOT NULL)
================================


-- more than 10 lines...
CREATE
TEMPORARY
TABLE
foo
(id
INT4
UNIQUE
NOT
NULL
,
idm
INT4
UNIQUE
NOT
NULL,
idx INT4 UNIQUE NOT NULL, idy INT4 UNIQUE NOT NULL, id2 TEXT NOT NULL PRIMARY KEY, id3 INTEGER NOT NUL, id4 INT4 UNIQUE NOT NULL, id5 TEXT UNIQUE NOT NULL,
idz INT4 UNIQUE NOT NULL,
idv
INT4
UNIQUE
NOT
NULL)
================================


================================
 -- ok
EXECUTE test
================================
 -- ok
CREATE TABLE test AS SELECT * FROM writetest
================================

	DECLARE c CURSOR FOR SELECT unique2 FROM tenk1 ORDER BY unique2
================================

		FETCH 10 FROM c
================================

		FETCH 10 FROM c
================================

	FETCH 10 FROM c
================================

	CLOSE c
================================

	DECLARE c CURSOR FOR SELECT unique2/0 FROM tenk1 ORDER BY unique2
================================

		FETCH 10 FROM c
================================

	-- c is now dead to the world ...
		FETCH 10 FROM c
================================

	FETCH 10 FROM c
================================


-- Now the same test with plpgsql (since it depends on SPI which is different)
create or replace function max_xacttest() returns smallint language plpgsql as
'begin return max(a) from xacttest
================================


create or replace function max_xacttest() returns smallint language plpgsql as
'begin return max(a) from xacttest
================================

  return 1::float8/$1
================================

exception
  when division_by_zero then return 0
================================

end$$ language plpgsql volatile
================================

declare foo cursor for select * from abc
================================

fetch from foo
================================


-- should fail
fetch from foo
================================

declare foo cursor for select * from abc
================================


fetch from foo
================================

fetch from foo
================================


fetch from foo
================================

  -- case of interest is that we fail while holding an open
  -- relcache reference to new_table
  INSERT INTO new_table SELECT invert(0.0)
================================

  RETURN 'foo'
================================

DECLARE ok CURSOR FOR SELECT * FROM int8_tbl
================================

DECLARE ctt CURSOR FOR SELECT create_temp_tab()
================================

FETCH ok
================================

FETCH ok
================================
  -- should work
FETCH ctt
================================

FETCH ok
================================
  -- should work
FETCH ctt
================================


-- psql will show only the last result in a multi-statement Query
SELECT 1\
================================
 SELECT 2\
================================


-- this implicitly commits:
insert into i_table values(1)\
================================

-- 1/0 error will cause rolling back the whole implicit transaction
insert into i_table values(2)\
================================
 insert into i_table values(3)\
================================
 insert into i_table values(4)\
================================
  -- we are not in a transaction at this point

-- begin converts implicit transaction into a regular one that
-- can extend past the end of the Query
select 1\
================================

select 1\
================================


-- commit in implicit-transaction state commits but issues a warning.
insert into i_table values(7)\
================================
 insert into i_table values(8)\
================================

-- similarly, rollback aborts but issues a warning.
insert into i_table values(9)\
================================
  -- we are not in a transaction at this point

-- implicit transaction block is still a transaction block, for e.g. VACUUM
SELECT 1\
================================

SELECT 1\
================================


-- we disallow savepoint-related commands in implicit-transaction state
SELECT 1\
================================

SELECT 1\
================================

SELECT 2\
================================


-- but this is OK, because the BEGIN converts it to a regular xact
SELECT 1\
================================


-- COMMIT/ROLLBACK + COMMIT/ROLLBACK AND CHAIN
INSERT INTO abc VALUES (7)\
================================
 INSERT INTO abc VALUES (8)\
================================
  -- 7 commit, 8 error
INSERT INTO abc VALUES (9)\
================================
 INSERT INTO abc VALUES (10)\
================================
  -- 9 rollback, 10 error

-- COMMIT/ROLLBACK AND CHAIN + COMMIT/ROLLBACK
INSERT INTO abc VALUES (11)\
================================
 INSERT INTO abc VALUES (12)\
================================
  -- 11 error, 12 not reached
INSERT INTO abc VALUES (13)\
================================
 INSERT INTO abc VALUES (14)\
================================
 INSERT INTO abc VALUES (15)\
================================
 INSERT INTO abc VALUES (16)\
================================
 INSERT INTO abc VALUES (17)\
================================
 INSERT INTO abc VALUES (18)\
================================
 INSERT INTO abc VALUES (19)\
================================
 INSERT INTO abc VALUES (20)\
================================


-- DO NOT ADD ANYTHING HERE.

================================

INSERT INTO clstr_tst_s (b) SELECT b FROM clstr_tst_s
================================

INSERT INTO clstr_tst_s (b) SELECT b FROM clstr_tst_s
================================

INSERT INTO clstr_tst_s (b) SELECT b FROM clstr_tst_s
================================

INSERT INTO clstr_tst_s (b) SELECT b FROM clstr_tst_s
================================

INSERT INTO clstr_tst_s (b) SELECT b FROM clstr_tst_s
================================


CLUSTER clstr_tst_c ON clstr_tst
================================


-- "CLUSTER <tablename>" on a table that hasn't been clustered
CLUSTER clstr_2
================================


CLUSTER clstr_1_pkey ON clstr_1
================================

CLUSTER clstr_2 USING clstr_2_pkey
================================

CLUSTER
================================

CLUSTER clstr_1
================================

CLUSTER clustertest_pkey ON clustertest
================================

cluster clstr_temp using clstr_temp_pkey
================================

CLUSTER clustertest USING clustertest_pkey
================================

CLUSTER clustertest
================================

CLUSTER clstrpart USING clstrpart_idx
================================


-- Test CLUSTER with external tuplesorting

create table clstr_4 as select * from tenk1
================================

cluster clstr_4 using cluster_sort
================================

INSERT INTO clstr_expression(a, b) SELECT g.i % 42, 'prefix'||g.i FROM generate_series(1, 133) g(i)
================================


-- and after clustering on clstr_expression_minus_a
CLUSTER clstr_expression USING clstr_expression_minus_a
================================


-- and after clustering on clstr_expression_upper_b
CLUSTER clstr_expression USING clstr_expression_upper_b
================================


================================

INSERT INTO base_tbl SELECT i, 'Row ' || i FROM generate_series(-2, 2) g(i)
================================

ALTER VIEW rw_view15 ALTER COLUMN upper SET DEFAULT 'NOT SET'
================================

INSERT INTO base_tbl SELECT i, 'Row ' || i FROM generate_series(-2, 2) g(i)
================================

INSERT INTO base_tbl SELECT i, 'Row ' || i FROM generate_series(-2, 2) g(i)
================================

INSERT INTO base_tbl SELECT i, 'Row ' || i FROM generate_series(-2, 2) g(i)
================================

INSERT INTO base_tbl SELECT i, 'Row ' || i FROM generate_series(-2, 2) g(i)
================================

    RETURN NEW
================================

  ELSIF TG_OP = 'UPDATE' THEN
    UPDATE base_tbl SET b=NEW.b WHERE a=OLD.a
================================

    RETURN NEW
================================

  ELSIF TG_OP = 'DELETE' THEN
    DELETE FROM base_tbl WHERE a=OLD.a
================================

    RETURN OLD
================================

$$
LANGUAGE plpgsql
================================

INSERT INTO base_tbl SELECT i, 'Row ' || i FROM generate_series(-2, 2) g(i)
================================

ALTER VIEW rw_view1 ALTER COLUMN bb SET DEFAULT 'View default'
================================

    RETURN NULL
================================

  RETURN NULL
================================

$$
LANGUAGE plpgsql
================================

DROP TRIGGER rw_view1_ins_trig on base_tbl
================================


UPDATE rw_view1 SET arr[1] = 42, arr[2] = 77 WHERE a = 3
================================

INSERT INTO base_tbl SELECT i/10.0 FROM generate_series(1,10) g(i)
================================

INSERT INTO base_tbl_parent SELECT * FROM generate_series(-8, -1)
================================

INSERT INTO base_tbl_child SELECT * FROM generate_series(1, 8)
================================

\d+ rw_view1
SELECT * FROM information_schema.views WHERE table_name = 'rw_view1'
================================
 -- implicitly cascaded
\d+ rw_view2
SELECT * FROM information_schema.views WHERE table_name = 'rw_view2'
================================

\d+ rw_view2
SELECT * FROM information_schema.views WHERE table_name = 'rw_view2'
================================


ALTER VIEW rw_view1 SET (check_option=here)
================================
 -- invalid
ALTER VIEW rw_view1 SET (check_option=local)
================================
 -- should fail

ALTER VIEW rw_view2 RESET (check_option)
================================

\d+ rw_view2
SELECT * FROM information_schema.views WHERE table_name = 'rw_view2'
================================
 -- should fail

UPDATE rw_view1 SET b[2] = -b[2] WHERE a = 1
================================
 -- ok
UPDATE rw_view1 SET b[1] = -b[1] WHERE a = 1
================================

INSERT INTO ref_tbl SELECT * FROM generate_series(1,10)
================================

  RETURN NEW
================================

$$
LANGUAGE plpgsql
================================

    RETURN NEW
================================

  ELSIF TG_OP = 'UPDATE' THEN
    UPDATE base_tbl SET a=NEW.a WHERE a=OLD.a
================================

    RETURN NEW
================================

  ELSIF TG_OP = 'DELETE' THEN
    DELETE FROM base_tbl WHERE a=OLD.a
================================

    RETURN OLD
================================

$$
LANGUAGE plpgsql
================================


-- Check option won't cascade down to base view with INSTEAD OF triggers

ALTER VIEW rw_view2 SET (check_option=cascaded)
================================


-- Neither local nor cascaded check options work with INSTEAD rules

DROP TRIGGER rw_view1_trig ON rw_view1
================================

  RETURN true
================================

$$
LANGUAGE plpgsql COST 0.000001
================================

$$
LANGUAGE plpgsql STRICT IMMUTABLE LEAKPROOF
================================


ALTER VIEW rw_view1 SET (security_barrier = true)
================================

INSERT INTO t1
SELECT i,i,'t1' FROM generate_series(1,10) g(i)
================================

INSERT INTO t11
SELECT i,i,'t11','t11d' FROM generate_series(1,10) g(i)
================================

INSERT INTO t12
SELECT i,i,'t12','{1,2}'::int[] FROM generate_series(1,10) g(i)
================================

INSERT INTO t111
SELECT i,i,'t111','t111d','{1,1,1}'::int[] FROM generate_series(1,10) g(i)
================================


alter view uv_iocu_view alter column bb set default 'view default'
================================

alter view base_tab_def_view alter b set default 'View default'
================================

alter view base_tab_def_view alter d set default 'View default'
================================

  return new
================================

$$
language plpgsql
================================


-- Using an unconditional DO INSTEAD rule should also cause NULLs to be
-- inserted where there are no view defaults.
drop trigger base_tab_def_view_instrig on base_tab_def_view
================================


-- A DO ALSO rule should cause each row to be inserted twice. The first
-- insert should behave the same as an auto-updatable view (using table
-- defaults, unless overridden by view defaults). The second insert should
-- behave the same as a rule-updatable view (inserting NULLs where there are
-- no view defaults).
drop rule base_tab_def_view_ins_rule on base_tab_def_view
================================

alter view base_tab_view alter column c set default 'View default'
================================

insert into base_tab_view (b[1], b[2], c, b[5], b[4], a, b[3])
values (1, 2, default, 5, 4, default, 3), (10, 11, 'C value', 14, 13, 100, 12)
================================


================================

CREATE CONVERSION myconv FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

--
-- cannot make same name conversion in same schema
--
CREATE CONVERSION myconv FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

--
-- create default conversion with qualified name
--
CREATE DEFAULT CONVERSION public.mydef FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

--
-- cannot make default conversion with same schema/for_encoding/to_encoding
--
CREATE DEFAULT CONVERSION public.mydef2 FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8
================================

-- test comments
COMMENT ON CONVERSION myconv_bad IS 'foo'
================================

COMMENT ON CONVERSION myconv IS 'bar'
================================

COMMENT ON CONVERSION myconv IS NULL
================================

--
-- drop user defined conversion
--
DROP CONVERSION myconv
================================

DROP CONVERSION mydef
================================


--
-- Test built-in conversion functions.
--

-- Helper function to test a conversion. Uses the test_enc_conversion function
-- that was created in the create_function_0 test.
create or replace function test_conv(
  input IN bytea,
  src_encoding IN text,
  dst_encoding IN text,

  result OUT bytea,
  errorat OUT bytea,
  error OUT text)
language plpgsql as
$$
declare
  validlen int
================================

    errorat = NULL
================================

    error := NULL
================================

  exception when others then
    error := sqlerrm
================================

    errorat = substr(input, validlen + 1)
================================

  return
================================

$$
================================


-- Test UTF-8 verification
select description, (test_conv(inbytes, 'utf8', 'utf8')).* from utf8_inputs
================================

-- Test conversions from UTF-8
select description, inbytes, (test_conv(inbytes, 'utf8', 'euc_jis_2004')).* from utf8_inputs
================================

select description, inbytes, (test_conv(inbytes, 'utf8', 'latin1')).* from utf8_inputs
================================

select description, inbytes, (test_conv(inbytes, 'utf8', 'latin2')).* from utf8_inputs
================================

select description, inbytes, (test_conv(inbytes, 'utf8', 'latin5')).* from utf8_inputs
================================

select description, inbytes, (test_conv(inbytes, 'utf8', 'koi8r')).* from utf8_inputs
================================

select description, inbytes, (test_conv(inbytes, 'utf8', 'gb18030')).* from utf8_inputs
================================


-- Test EUC_JIS_2004 verification
select description, inbytes, (test_conv(inbytes, 'euc_jis_2004', 'euc_jis_2004')).* from euc_jis_2004_inputs
================================

-- Test conversions from EUC_JIS_2004
select description, inbytes, (test_conv(inbytes, 'euc_jis_2004', 'utf8')).* from euc_jis_2004_inputs
================================


-- Test SHIFT-JIS-2004 verification
select description, inbytes, (test_conv(inbytes, 'shiftjis2004', 'shiftjis2004')).* from shiftjis2004_inputs
================================

-- Test conversions from SHIFT-JIS-2004
select description, inbytes, (test_conv(inbytes, 'shiftjis2004', 'utf8')).* from shiftjis2004_inputs
================================

select description, inbytes, (test_conv(inbytes, 'shiftjis2004', 'euc_jis_2004')).* from shiftjis2004_inputs
================================


-- Test GB18030 verification
select description, inbytes, (test_conv(inbytes, 'gb18030', 'gb18030')).* from gb18030_inputs
================================

-- Test conversions from GB18030
select description, inbytes, (test_conv(inbytes, 'gb18030', 'utf8')).* from gb18030_inputs
================================


-- Test ISO-8859-5 verification
select description, inbytes, (test_conv(inbytes, 'iso8859-5', 'iso8859-5')).* from iso8859_5_inputs
================================

-- Test conversions from ISO-8859-5
select description, inbytes, (test_conv(inbytes, 'iso8859-5', 'utf8')).* from iso8859_5_inputs
================================

select description, inbytes, (test_conv(inbytes, 'iso8859-5', 'koi8r')).* from iso8859_5_inputs
================================

select description, inbytes, (test_conv(inbytes, 'iso8859_5', 'mule_internal')).* from iso8859_5_inputs
================================


-- Test Big5 verification
select description, inbytes, (test_conv(inbytes, 'big5', 'big5')).* from big5_inputs
================================

-- Test conversions from Big5
select description, inbytes, (test_conv(inbytes, 'big5', 'utf8')).* from big5_inputs
================================

select description, inbytes, (test_conv(inbytes, 'big5', 'mule_internal')).* from big5_inputs
================================


-- Test MULE_INTERNAL verification
select description, inbytes, (test_conv(inbytes, 'mule_internal', 'mule_internal')).* from mic_inputs
================================

-- Test conversions from MULE_INTERNAL
select description, inbytes, (test_conv(inbytes, 'mule_internal', 'koi8r')).* from mic_inputs
================================

select description, inbytes, (test_conv(inbytes, 'mule_internal', 'iso8859-5')).* from mic_inputs
================================

select description, inbytes, (test_conv(inbytes, 'mule_internal', 'sjis')).* from mic_inputs
================================

select description, inbytes, (test_conv(inbytes, 'mule_internal', 'big5')).* from mic_inputs
================================

select description, inbytes, (test_conv(inbytes, 'mule_internal', 'euc_jp')).* from mic_inputs
================================


================================


================================


--
-- Test multiple-set-clause syntax
--

INSERT INTO update_test SELECT a,b+1,c FROM update_test
================================


UPDATE update_test SET (c,b,a) = ('bugle', b+11, DEFAULT) WHERE c = 'foo'
================================

UPDATE update_test SET (c,b) = ('car', a+b), a = a + 1 WHERE a = 10
================================

-- fail, multi assignment to same column:
UPDATE update_test SET (c,b) = ('car', a+b), b = a + 1 WHERE a = 10
================================


-- uncorrelated sub-select:
UPDATE update_test
  SET (b,a) = (select a,b from update_test where b = 41 and c = 'car')
  WHERE a = 100 AND b = 20
================================

-- correlated sub-select:
UPDATE update_test o
  SET (b,a) = (select a+1,b from update_test i
               where i.a=o.a and i.b=o.b and i.c is not distinct from o.c)
================================

-- fail, multiple rows supplied:
UPDATE update_test SET (b,a) = (select a+1,b from update_test)
================================

-- set to null if no rows supplied:
UPDATE update_test SET (b,a) = (select a+1,b from update_test where a = 1000)
  WHERE a = 11
================================

-- *-expansion should work in this context:
UPDATE update_test SET (a,b) = ROW(v.*) FROM (VALUES(21, 100)) AS v(i, j)
  WHERE update_test.a = v.i
================================

-- you might expect this to work, but syntactically it's not a RowExpr:
UPDATE update_test SET (a,b) = (v.*) FROM (VALUES(21, 101)) AS v(i, j)
  WHERE update_test.a = v.i
================================


-- Check multi-assignment with a Result node to handle a one-time filter.
EXPLAIN (VERBOSE, COSTS OFF)
UPDATE update_test t
  SET (a, b) = (SELECT b, a FROM update_test s WHERE s.a = t.a)
  WHERE CURRENT_USER = SESSION_USER
================================

UPDATE update_test t
  SET (a, b) = (SELECT b, a FROM update_test s WHERE s.a = t.a)
  WHERE CURRENT_USER = SESSION_USER
================================


\set init_range_parted 'truncate range_parted
================================
 insert into range_parted VALUES (''a'', 1, 1, 1), (''a'', 10, 200, 1), (''b'', 12, 96, 1), (''b'', 13, 97, 2), (''b'', 15, 105, 16), (''b'', 17, 105, 19)'
\set show_data 'select tableoid::regclass::text COLLATE "C" partname, * from range_parted ORDER BY 1, 2, 3, 4, 5, 6'
:init_range_parted
================================

:show_data
================================

:show_data
================================


:show_data
================================


:show_data
================================


-- RETURNING having whole-row vars.
:init_range_parted
================================

:show_data
================================



-- Transition tables with update row movement
:init_range_parted
================================

    return null
================================

$$
================================

:show_data
================================

:init_range_parted
================================

:show_data
================================

DROP TRIGGER trans_deletetrig ON range_parted
================================

DROP TRIGGER trans_inserttrig ON range_parted
================================

   return NEW
================================

:init_range_parted
================================

:show_data
================================

:init_range_parted
================================

:show_data
================================


-- Case where per-partition tuple conversion map array is allocated, but the
-- map is not required for the particular tuple that is routed, thanks to
-- matching table attributes of the partition and the target table.
:init_range_parted
================================

:show_data
================================


DROP TRIGGER trans_updatetrig ON range_parted
================================

DROP TRIGGER trig_c1_100 ON part_c_1_100
================================

DROP TRIGGER trig_d1_15 ON part_d_1_15
================================

DROP TRIGGER trig_d15_20 ON part_d_15_20
================================

CREATE POLICY seeall ON range_parted AS PERMISSIVE FOR SELECT USING (true)
================================

CREATE POLICY policy_range_parted ON range_parted for UPDATE USING (true) WITH CHECK (c % 2 = 0)
================================


:init_range_parted
================================
 -- Make even numbers odd, or vice versa
   return NEW
================================


:init_range_parted
================================

:init_range_parted
================================

DROP TRIGGER trig_d_1_15 ON part_d_1_15
================================

:init_range_parted
================================

CREATE POLICY policy_range_parted_subplan on range_parted
    AS RESTRICTIVE for UPDATE USING (true)
    WITH CHECK ((SELECT range_parted.c <= c1 FROM mintab))
================================

:init_range_parted
================================

CREATE POLICY policy_range_parted_wholerow on range_parted AS RESTRICTIVE for UPDATE USING (true)
   WITH CHECK (range_parted = row('b', 10, 112, 1, NULL)::range_parted)
================================

:init_range_parted
================================

DROP POLICY policy_range_parted ON range_parted
================================

DROP POLICY policy_range_parted_subplan ON range_parted
================================

DROP POLICY policy_range_parted_wholerow ON range_parted
================================



-- statement triggers with update row movement
---------------------------------------------------

:init_range_parted
================================

    return null
================================

$$
================================

:show_data
================================


DROP TRIGGER parent_delete_trig ON range_parted
================================

DROP TRIGGER parent_update_trig ON range_parted
================================

DROP TRIGGER parent_insert_trig ON range_parted
================================

DROP TRIGGER c1_delete_trig ON part_c_1_100
================================

DROP TRIGGER c1_update_trig ON part_c_1_100
================================

DROP TRIGGER c1_insert_trig ON part_c_1_100
================================

DROP TRIGGER d1_delete_trig ON part_d_1_15
================================

DROP TRIGGER d1_update_trig ON part_d_1_15
================================

DROP TRIGGER d1_insert_trig ON part_d_1_15
================================

DROP TRIGGER d15_delete_trig ON part_d_15_20
================================

DROP TRIGGER d15_update_trig ON part_d_15_20
================================

DROP TRIGGER d15_insert_trig ON part_d_15_20
================================



-- Creating default partition for range
:init_range_parted
================================

\d+ part_def
insert into range_parted values ('c', 9)
================================


:show_data
================================


-- Update row movement from non-default to default partition.
-- fail, default partition is not under part_a_10_a_20
================================

:show_data
================================

:show_data
================================
 -- This is changing partition key column.
   return NEW
================================


DROP TRIGGER parted_mod_b ON sub_part1
================================

   return NULL
================================

-- Drop the trigger. Now the row should be moved.
DROP TRIGGER trig_skip_delete ON sub_part2
================================
 $$ language 'plpgsql' immutable
================================

create operator class custom_opclass for type int4 using hash as
operator 1 = , function 2 dummy_hashint4(int4, int8)
================================

drop operator class custom_opclass using hash
================================


================================

    tmp text[]
================================

    first_row bool := true
================================

            tmp := regexp_match(ln, 'rows=(\d*) .* rows=(\d*)')
================================

            return query select tmp[1]::int, tmp[2]::int
================================

$$
================================

CREATE STATISTICS tst
================================

CREATE STATISTICS tst ON a, b
================================

CREATE STATISTICS tst FROM sometab
================================

CREATE STATISTICS tst ON a, b FROM nonexistent
================================

CREATE STATISTICS tst ON a, b FROM ext_stats_test
================================

CREATE STATISTICS tst ON x, x, y FROM ext_stats_test
================================

CREATE STATISTICS tst ON x, x, y, x, x, y, x, x, y FROM ext_stats_test
================================

CREATE STATISTICS tst ON x, x, y, x, x, (x || 'x'), (y + 1), (x || 'x'), (x || 'x'), (y + 1) FROM ext_stats_test
================================

CREATE STATISTICS tst ON (x || 'x'), (x || 'x'), (y + 1), (x || 'x'), (x || 'x'), (y + 1), (x || 'x'), (x || 'x'), (y + 1) FROM ext_stats_test
================================

CREATE STATISTICS tst ON (x || 'x'), (x || 'x'), y FROM ext_stats_test
================================

CREATE STATISTICS tst (unrecognized) ON x, y FROM ext_stats_test
================================

-- incorrect expressions
CREATE STATISTICS tst ON (y) FROM ext_stats_test
================================
 -- single column reference
CREATE STATISTICS tst ON y + z FROM ext_stats_test
================================
 -- missing parentheses
CREATE STATISTICS tst ON (x, y) FROM ext_stats_test
================================

CREATE STATISTICS IF NOT EXISTS ab1_a_b_stats ON a, b FROM ab1
================================

COMMENT ON STATISTICS ab1_a_b_stats IS 'new comment'
================================

COMMENT ON STATISTICS ab1_a_b_stats IS 'changed comment'
================================

DROP STATISTICS ab1_a_b_stats
================================

ALTER STATISTICS ab1_a_b_stats RENAME TO ab1_a_b_stats_new
================================


CREATE STATISTICS IF NOT EXISTS ab1_a_b_stats ON a, b FROM ab1
================================

DROP STATISTICS ab1_a_b_stats
================================


CREATE SCHEMA regress_schema_2
================================

CREATE STATISTICS regress_schema_2.ab1_a_b_stats ON a, b FROM ab1
================================


DROP STATISTICS regress_schema_2.ab1_a_b_stats
================================


-- Ensure statistics are dropped when columns are
CREATE STATISTICS ab1_b_c_stats ON b, c FROM ab1
================================

CREATE STATISTICS ab1_a_b_c_stats ON a, b, c FROM ab1
================================

CREATE STATISTICS ab1_b_a_stats ON b, a FROM ab1
================================

\d ab1
-- Ensure statistics are dropped when table is
SELECT stxname FROM pg_statistic_ext WHERE stxname LIKE 'ab1%'
================================

INSERT INTO ab1 SELECT a, a%23 FROM generate_series(1, 1000) a
================================

CREATE STATISTICS ab1_a_b_stats ON a, b FROM ab1
================================

ALTER TABLE ab1 ALTER a SET STATISTICS -1
================================

-- setting statistics target 0 skips the statistics, without printing any message, so check catalog
ALTER STATISTICS ab1_a_b_stats SET STATISTICS 0
================================

\d ab1
ANALYZE ab1
================================

ALTER STATISTICS ab1_a_b_stats SET STATISTICS -1
================================

\d+ ab1
-- partial analyze doesn't build stats either
ANALYZE ab1 (a)
================================

ALTER STATISTICS ab1_a_b_stats SET STATISTICS 0
================================

ALTER STATISTICS IF EXISTS ab1_a_b_stats SET STATISTICS 0
================================

CREATE STATISTICS ab1_a_b_stats ON a, b FROM ab1
================================


-- expression stats may be built on a single expression column
CREATE STATISTICS ab1_exprstat_1 ON (a+b) FROM ab1
================================


-- with a single expression, we only enable expression statistics
CREATE STATISTICS ab1_exprstat_2 ON (a+b) FROM ab1
================================


-- adding anything to the expression builds all statistics kinds
CREATE STATISTICS ab1_exprstat_3 ON (a+b), a FROM ab1
================================


-- date_trunc on timestamptz is not immutable, but that should not matter
CREATE STATISTICS ab1_exprstat_4 ON date_trunc('day', d) FROM ab1
================================


-- date_trunc on timestamp is immutable
CREATE STATISTICS ab1_exprstat_5 ON date_trunc('day', c) FROM ab1
================================


-- insert some data and run analyze, to test that these cases build properly
INSERT INTO ab1
SELECT
    generate_series(1,10),
    generate_series(1,10),
    generate_series('2020-10-01'::timestamp, '2020-10-10'::timestamp, interval '1 day'),
    generate_series('2020-10-01'::timestamptz, '2020-10-10'::timestamptz, interval '1 day')
================================


-- Verify supported object types for extended statistics
CREATE schema tststats
================================

CREATE MATERIALIZED VIEW tststats.mv AS SELECT * FROM tststats.t
================================

CREATE TYPE tststats.ty AS (a int, b int, c text)
================================

CREATE FOREIGN DATA WRAPPER extstats_dummy_fdw
================================

CREATE SERVER extstats_dummy_srv FOREIGN DATA WRAPPER extstats_dummy_fdw
================================

CREATE FOREIGN TABLE tststats.f (a int, b int, c text) SERVER extstats_dummy_srv
================================


CREATE STATISTICS tststats.s1 ON a, b FROM tststats.t
================================

CREATE STATISTICS tststats.s2 ON a, b FROM tststats.ti
================================

CREATE STATISTICS tststats.s3 ON a, b FROM tststats.s
================================

CREATE STATISTICS tststats.s4 ON a, b FROM tststats.v
================================

CREATE STATISTICS tststats.s5 ON a, b FROM tststats.mv
================================

CREATE STATISTICS tststats.s6 ON a, b FROM tststats.ty
================================

CREATE STATISTICS tststats.s7 ON a, b FROM tststats.f
================================

CREATE STATISTICS tststats.s8 ON a, b FROM tststats.pt
================================

CREATE STATISTICS tststats.s9 ON a, b FROM tststats.pt1
================================

DO $$
DECLARE
	relname text := reltoastrelid::regclass FROM pg_class WHERE oid = 'tststats.t'::regclass
================================

EXCEPTION WHEN wrong_object_type THEN
	RAISE NOTICE 'stats on toast table not created'
================================

$$
================================


DROP SCHEMA tststats CASCADE
================================

DROP FOREIGN DATA WRAPPER extstats_dummy_fdw CASCADE
================================


-- over-estimates when using only per-column statistics
INSERT INTO ndistinct (a, b, c, filler1)
     SELECT i/100, i/100, i/100, cash_words((i/100)::money)
       FROM generate_series(1,1000) s(i)
================================


-- correct command
CREATE STATISTICS s10 ON a, b, c FROM ndistinct
================================


-- under-estimates when using only per-column statistics
INSERT INTO ndistinct (a, b, c, filler1)
     SELECT mod(i,13), mod(i,17), mod(i,19),
            cash_words(mod(i,23)::int::money)
       FROM generate_series(1,1000) s(i)
================================


DROP STATISTICS s10
================================


CREATE STATISTICS s10 (ndistinct) ON (a+1), (b+100), (2*c) FROM ndistinct
================================


DROP STATISTICS s10
================================


CREATE STATISTICS s10 (ndistinct) ON a, b, (2*c) FROM ndistinct
================================


DROP STATISTICS s10
================================


-- two mostly independent groups of columns
INSERT INTO ndistinct (a, b, c, d)
     SELECT mod(i,3), mod(i,9), mod(i,5), mod(i,20)
       FROM generate_series(1,1000) s(i)
================================


-- basic statistics on both attributes (no expressions)
CREATE STATISTICS s11 (ndistinct) ON a, b FROM ndistinct
================================


CREATE STATISTICS s12 (ndistinct) ON c, d FROM ndistinct
================================



-- replace the second statistics by statistics on expressions

DROP STATISTICS s12
================================


CREATE STATISTICS s12 (ndistinct) ON (c * 10), (d - 1) FROM ndistinct
================================



-- replace the second statistics by statistics on both attributes and expressions

DROP STATISTICS s12
================================


CREATE STATISTICS s12 (ndistinct) ON c, d, (c * 10), (d - 1) FROM ndistinct
================================



-- replace the other statistics by statistics on both attributes and expressions

DROP STATISTICS s11
================================


CREATE STATISTICS s11 (ndistinct) ON a, b, (a*5), (b+1) FROM ndistinct
================================



-- replace statistics by somewhat overlapping ones (this expected to get worse estimate
-- because the first statistics shall be applied to 3 columns, and the second one can't
-- be really applied)

DROP STATISTICS s11
================================

DROP STATISTICS s12
================================


CREATE STATISTICS s11 (ndistinct) ON a, b, (a*5), (b+1) FROM ndistinct
================================

CREATE STATISTICS s12 (ndistinct) ON a, (b+1), (c * 10) FROM ndistinct
================================


DROP STATISTICS s11
================================

DROP STATISTICS s12
================================


-- random data (no functional dependencies)
INSERT INTO functional_dependencies (a, b, c, filler1)
     SELECT mod(i, 5), mod(i, 7), mod(i, 11), i FROM generate_series(1,1000) s(i)
================================


-- create statistics
CREATE STATISTICS func_deps_stat (dependencies) ON a, b, c FROM functional_dependencies
================================

DROP STATISTICS func_deps_stat
================================


-- now do the same thing, but with expressions
INSERT INTO functional_dependencies (a, b, c, filler1)
     SELECT i, i, i, i FROM generate_series(1,5000) s(i)
================================


-- create statistics
CREATE STATISTICS func_deps_stat (dependencies) ON (mod(a,11)), (mod(b::int, 13)), (mod(c, 7)) FROM functional_dependencies
================================

DROP STATISTICS func_deps_stat
================================


INSERT INTO functional_dependencies (a, b, c, filler1)
     SELECT mod(i,100), mod(i,50), mod(i,25), i FROM generate_series(1,5000) s(i)
================================


-- create statistics
CREATE STATISTICS func_deps_stat (dependencies) ON a, b, c FROM functional_dependencies
================================


DROP STATISTICS func_deps_stat
================================


-- create statistics on expressions
CREATE STATISTICS func_deps_stat (dependencies) ON (a * 2), upper(b), (c + 1) FROM functional_dependencies
================================


INSERT INTO functional_dependencies_multi (a, b, c, d)
    SELECT
         mod(i,7),
         mod(i,7),
         mod(i,11),
         mod(i,11)
    FROM generate_series(1,5000) s(i)
================================


-- create separate functional dependencies
CREATE STATISTICS functional_dependencies_multi_1 (dependencies) ON a, b FROM functional_dependencies_multi
================================

CREATE STATISTICS functional_dependencies_multi_2 (dependencies) ON c, d FROM functional_dependencies_multi
================================


-- random data (no MCV list)
INSERT INTO mcv_lists (a, b, c, filler1)
     SELECT mod(i,37), mod(i,41), mod(i,43), mod(i,47) FROM generate_series(1,5000) s(i)
================================


-- create statistics
CREATE STATISTICS mcv_lists_stats (mcv) ON a, b, c FROM mcv_lists
================================

DROP STATISTICS mcv_lists_stats
================================


-- random data (no MCV list), but with expression
INSERT INTO mcv_lists (a, b, c, filler1)
     SELECT i, i, i, i FROM generate_series(1,1000) s(i)
================================


-- create statistics
CREATE STATISTICS mcv_lists_stats (mcv) ON (mod(a,7)), (mod(b::int,11)), (mod(c,13)) FROM mcv_lists
================================

DROP STATISTICS mcv_lists_stats
================================


INSERT INTO mcv_lists (a, b, c, filler1)
     SELECT mod(i,100), mod(i,50), mod(i,25), i FROM generate_series(1,5000) s(i)
================================


-- create statistics
CREATE STATISTICS mcv_lists_stats (mcv) ON a, b, c FROM mcv_lists
================================

DROP STATISTICS mcv_lists_stats
================================


INSERT INTO mcv_lists (a, b, c, filler1)
     SELECT i, i, i, i FROM generate_series(1,1000) s(i)
================================


-- create statistics with expressions only (we create three separate stats, in order not to build more complex extended stats)
CREATE STATISTICS mcv_lists_stats_1 ON (mod(a,20)) FROM mcv_lists
================================

CREATE STATISTICS mcv_lists_stats_2 ON (mod(b::int,10)) FROM mcv_lists
================================

CREATE STATISTICS mcv_lists_stats_3 ON (mod(c,5)) FROM mcv_lists
================================


DROP STATISTICS mcv_lists_stats_1
================================

DROP STATISTICS mcv_lists_stats_2
================================

DROP STATISTICS mcv_lists_stats_3
================================


-- create statistics with both MCV and expressions
CREATE STATISTICS mcv_lists_stats (mcv) ON (mod(a,20)), (mod(b::int,10)), (mod(c,5)) FROM mcv_lists
================================

DROP STATISTICS mcv_lists_stats
================================


INSERT INTO mcv_lists (a, b, c, filler1)
     SELECT
         (CASE WHEN mod(i,100) = 1 THEN NULL ELSE mod(i,100) END),
         (CASE WHEN mod(i,50) = 1  THEN NULL ELSE mod(i,50) END),
         (CASE WHEN mod(i,25) = 1  THEN NULL ELSE mod(i,25) END),
         i
     FROM generate_series(1,5000) s(i)
================================


-- create statistics
CREATE STATISTICS mcv_lists_stats (mcv) ON a, b, c FROM mcv_lists
================================

INSERT INTO mcv_lists (a, b, c) SELECT 1, 2, 3 FROM generate_series(1,1000) s(i)
================================

DROP STATISTICS mcv_lists_stats
================================


INSERT INTO mcv_lists (a, b, c, d)
     SELECT
         NULL, -- always NULL
         (CASE WHEN mod(i,2) = 0 THEN NULL ELSE 'x' END),
         (CASE WHEN mod(i,2) = 0 THEN NULL ELSE 0 END),
         (CASE WHEN mod(i,2) = 0 THEN NULL ELSE 'x' END)
     FROM generate_series(1,5000) s(i)
================================


-- create statistics
CREATE STATISTICS mcv_lists_stats (mcv) ON a, b, d FROM mcv_lists
================================


INSERT INTO mcv_lists_uuid (a, b, c)
     SELECT
         md5(mod(i,100)::text)::uuid,
         md5(mod(i,50)::text)::uuid,
         md5(mod(i,25)::text)::uuid
     FROM generate_series(1,5000) s(i)
================================


CREATE STATISTICS mcv_lists_uuid_stats (mcv) ON a, b, c
  FROM mcv_lists_uuid
================================


INSERT INTO mcv_lists_arrays (a, b, c)
     SELECT
         ARRAY[md5((i/100)::text), md5((i/100-1)::text), md5((i/100+1)::text)],
         ARRAY[(i/100-1)::numeric/1000, (i/100)::numeric/1000, (i/100+1)::numeric/1000],
         ARRAY[(i/100-1), i/100, (i/100+1)]
     FROM generate_series(1,5000) s(i)
================================


CREATE STATISTICS mcv_lists_arrays_stats (mcv) ON a, b, c
  FROM mcv_lists_arrays
================================


INSERT INTO mcv_lists_bool (a, b, c)
     SELECT
         (mod(i,2) = 0), (mod(i,4) = 0), (mod(i,8) = 0)
     FROM generate_series(1,10000) s(i)
================================


CREATE STATISTICS mcv_lists_bool_stats (mcv) ON a, b, c
  FROM mcv_lists_bool
================================


-- 10 frequent groups, each with 100 elements
INSERT INTO mcv_lists_partial (a, b, c)
     SELECT
         mod(i,10),
         mod(i,10),
         mod(i,10)
     FROM generate_series(0,999) s(i)
================================


-- 100 groups that will make it to the MCV list (includes the 10 frequent ones)
INSERT INTO mcv_lists_partial (a, b, c)
     SELECT
         i,
         i,
         i
     FROM generate_series(0,99) s(i)
================================


-- 4000 groups in total, most of which won't make it (just a single item)
INSERT INTO mcv_lists_partial (a, b, c)
     SELECT
         i,
         i,
         i
     FROM generate_series(0,3999) s(i)
================================


CREATE STATISTICS mcv_lists_partial_stats (mcv) ON a, b, c
  FROM mcv_lists_partial
================================


INSERT INTO mcv_lists_multi (a, b, c, d)
    SELECT
         mod(i,5),
         mod(i,5),
         mod(i,7),
         mod(i,7)
    FROM generate_series(1,5000) s(i)
================================


-- create separate MCV statistics
CREATE STATISTICS mcv_lists_multi_1 (mcv) ON a, b FROM mcv_lists_multi
================================

CREATE STATISTICS mcv_lists_multi_2 (mcv) ON c, d FROM mcv_lists_multi
================================

INSERT INTO expr_stats SELECT mod(i,10), mod(i,10), mod(i,10) FROM generate_series(1,1000) s(i)
================================


CREATE STATISTICS expr_stats_1 (mcv) ON (a+b), (a-b), (2*a), (3*b) FROM expr_stats
================================


DROP STATISTICS expr_stats_1
================================

INSERT INTO expr_stats SELECT mod(i,10), mod(i,10), mod(i,10) FROM generate_series(1,1000) s(i)
================================


CREATE STATISTICS expr_stats_1 (mcv) ON a, b, (2*a), (3*b), (a+b), (a-b) FROM expr_stats
================================

INSERT INTO expr_stats SELECT mod(i,10), md5(mod(i,10)::text), md5(mod(i,10)::text) FROM generate_series(1,1000) s(i)
================================


CREATE STATISTICS expr_stats_1 (mcv) ON a, b, (b || c), (c || b) FROM expr_stats
================================


CREATE STATISTICS expr_stat_comp_1 ON c0, c1 FROM expr_stats_incompatible_test
================================


SELECT c0 FROM ONLY expr_stats_incompatible_test WHERE
(
  upper('x') LIKE ('x'||('[0,1]'::int4range))
  AND
  (c0 IN (0, 1) OR c1)
)
================================


-- Permission tests. Users should not be able to see specific data values in
-- the extended statistics, if they lack permission to see those values in
-- the underlying table.
--
-- Currently this is only relevant for MCV stats.
CREATE SCHEMA tststats
================================


INSERT INTO tststats.priv_test_tbl
     SELECT mod(i,5), mod(i,10) FROM generate_series(1,100) s(i)
================================


CREATE STATISTICS tststats.priv_test_stats (mcv) ON a, b
  FROM tststats.priv_test_tbl
================================

create statistics stts_1 (ndistinct) on a, b from stts_t1
================================

create statistics stts_2 (ndistinct, dependencies) on a, b from stts_t1
================================

create statistics stts_3 (ndistinct, dependencies, mcv) on a, b from stts_t1
================================

create statistics stts_4 on b, c from stts_t2
================================

create statistics stts_hoge on col1, col2, col3 from stts_t3
================================


create schema stts_s1
================================

create schema stts_s2
================================

create statistics stts_s1.stts_foo on col1, col2 from stts_t3
================================

create statistics stts_s2.stts_yama (dependencies, mcv) on col1, col3 from stts_t3
================================


insert into stts_t1 select i,i from generate_series(1,100) i
================================


\dX
\dX stts_?
\dX *stts_hoge
\dX+
\dX+ stts_?
\dX+ *stts_hoge
\dX+ stts_s2.stts_yama

set search_path to public, stts_s1
================================

\dX

create role regress_stats_ext nosuperuser
================================

\dX
reset role
================================

drop schema stts_s1, stts_s2 cascade
================================
 RETURN $1 < $2
================================

CREATE OPERATOR <<< (procedure = op_leak, leftarg = int, rightarg = int,
                     restrict = scalarltsel)
================================

SELECT * FROM tststats.priv_test_tbl WHERE a <<< 0 AND b <<< 0
================================

SELECT * FROM tststats.priv_test_view WHERE a <<< 0 AND b <<< 0
================================

SELECT * FROM tststats.priv_test_tbl WHERE a <<< 0 AND b <<< 0
================================
 -- Should not leak

-- Tidy up
DROP OPERATOR <<< (int, int)
================================

DROP SCHEMA tststats CASCADE
================================


================================

insert into onerow default values
================================
  -- error
SELECT * FROM (J1_TBL JOIN J2_TBL USING (i) AS x) AS xx WHERE x.i = 1
================================

SELECT ROW(x.*) FROM J1_TBL JOIN J2_TBL USING (i) AS x WHERE J1_TBL.t = 'one'
================================

SELECT row_to_json(x.*) FROM J1_TBL JOIN J2_TBL USING (i) AS x WHERE J1_TBL.t = 'one'
================================


-- Cases with non-nullable expressions in subquery results
================================

insert into tt3 select x, repeat('xyzzy', 100) from generate_series(1,10000) x
================================


create type mycomptype as (id int, v bigint)
================================


--
-- test NULL behavior of whole-row Vars, per bug #5025
--
select t1.q2, count(t2.*)
from int8_tbl t1 left join int8_tbl t2 on (t1.q2 = t2.q1)
group by t1.q2 order by 1
================================


select t1.q2, count(t2.*)
from int8_tbl t1 left join (select * from int8_tbl) t2 on (t1.q2 = t2.q1)
group by t1.q2 order by 1
================================


select t1.q2, count(t2.*)
from int8_tbl t1 left join (select * from int8_tbl offset 0) t2 on (t1.q2 = t2.q1)
group by t1.q2 order by 1
================================


select t1.q2, count(t2.*)
from int8_tbl t1 left join
  (select q1, case when q2=1 then 1 else q2 end as q2 from int8_tbl) t2
  on (t1.q2 = t2.q1)
group by t1.q2 order by 1
================================


--
-- test for ability to use a cartesian join when necessary
--

create temp table q1 as select 1 as q1
================================

create temp table q2 as select 0 as q2
================================
 $$ language plpgsql immutable
================================


--
-- test that quals attached to an outer join have correct semantics,
-- specifically that they don't re-use expressions computed below the join
================================


-- join removal is not possible when the GROUP BY contains a column that is
-- not in the join condition.  (Note: as of 9.6, we notice that b.id is a
-- primary key and so drop b.c_id from the GROUP BY of the resulting plan
================================


-- check behavior of LATERAL in UPDATE/DELETE

create temp table xx1 as select f1 as x1, -f1 as x2 from int4_tbl
================================

insert into fkest select x, x/10, x/10, x/100 from generate_series(1,1000) x
================================


insert into fkest select x/10, x%10, x from generate_series(1,1000) x
================================

insert into fkest1 select x/10, x%10 from generate_series(1,1000) x
================================


-- ... unless it actually is unique
create table j3 as select unique1, tenthous from onek
================================


================================


DECLARE foo13 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 50
================================


DECLARE foo14 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 51
================================


DECLARE foo15 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 52
================================


DECLARE foo16 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 53
================================


DECLARE foo17 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 54
================================


DECLARE foo18 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 55
================================


DECLARE foo19 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 56
================================


DECLARE foo20 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 57
================================


DECLARE foo21 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 58
================================


DECLARE foo22 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 59
================================


DECLARE foo23 CURSOR FOR
   SELECT * FROM onek WHERE unique1 = 60
================================


DECLARE foo24 CURSOR FOR
   SELECT * FROM onek2 WHERE unique1 = 50
================================


DECLARE foo25 CURSOR FOR
   SELECT * FROM onek2 WHERE unique1 = 60
================================


FETCH all in foo13
================================


FETCH all in foo14
================================


FETCH all in foo15
================================


FETCH all in foo16
================================


FETCH all in foo17
================================


FETCH all in foo18
================================


FETCH all in foo19
================================


FETCH all in foo20
================================


FETCH all in foo21
================================


FETCH all in foo22
================================


FETCH all in foo23
================================


FETCH all in foo24
================================


FETCH all in foo25
================================


CLOSE foo13
================================


CLOSE foo14
================================


CLOSE foo15
================================


CLOSE foo16
================================


CLOSE foo17
================================


CLOSE foo18
================================


CLOSE foo19
================================


CLOSE foo20
================================


CLOSE foo21
================================


CLOSE foo22
================================


CLOSE foo23
================================


CLOSE foo24
================================


CLOSE foo25
================================


================================

    RETURN NEW
================================

END$$
================================


================================

SELECT num_nonnulls(1, 2, NULL::text, NULL::point, '', int8 '9', 1.0 / NULL)
================================

SELECT num_nulls(1, 2, NULL::text, NULL::point, '', int8 '9', 1.0 / NULL)
================================


--
-- Test some built-in SRFs
--
-- The outputs of these are variable, so we can't just print their results
-- directly, but we can at least verify that the code doesn't fail.
--
select setting as segsize
from pg_settings where name = 'wal_segment_size'
\gset

select count(*) > 0 as ok from pg_ls_waldir()
================================

select (w).size = :segsize as ok
from (select pg_ls_waldir() w) ss where length((w).name) = 24 limit 1
================================


================================


-- Test shorthand input values
-- We can't just "select" the results since they aren't constants
================================
 test for
-- equality instead.  We can do that by running the test inside a transaction
-- block, within which the value of 'now' shouldn't change, and so these
-- related values shouldn't either.

BEGIN
================================


SELECT count(*) AS One FROM TIMESTAMP_TBL WHERE d1 = timestamp without time zone 'today'
================================

SELECT count(*) AS Three FROM TIMESTAMP_TBL WHERE d1 = timestamp without time zone 'tomorrow'
================================

SELECT count(*) AS One FROM TIMESTAMP_TBL WHERE d1 = timestamp without time zone 'yesterday'
================================

SELECT count(*) AS two FROM TIMESTAMP_TBL WHERE d1 = timestamp(2) without time zone 'now'
================================
  -- out of range

-- Demonstrate functions and operators
SELECT d1 FROM TIMESTAMP_TBL
   WHERE d1 > timestamp without time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMP_TBL
   WHERE d1 < timestamp without time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMP_TBL
   WHERE d1 = timestamp without time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMP_TBL
   WHERE d1 != timestamp without time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMP_TBL
   WHERE d1 <= timestamp without time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMP_TBL
   WHERE d1 >= timestamp without time zone '1997-01-02'
================================


SELECT date_trunc( 'week', timestamp '2004-02-29 15:44:17.71393' ) AS week_trunc
================================


-- verify date_bin behaves the same as date_trunc for relevant intervals

-- case 1: AD dates, origin < input
SELECT
  str,
  interval,
  date_trunc(str, ts) = date_bin(interval::interval, ts, timestamp '2001-01-01') AS equal
FROM (
  VALUES
  ('week', '7 d'),
  ('day', '1 d'),
  ('hour', '1 h'),
  ('minute', '1 m'),
  ('second', '1 s'),
  ('millisecond', '1 ms'),
  ('microsecond', '1 us')
) intervals (str, interval),
(VALUES (timestamp '2020-02-29 15:44:17.71393')) ts (ts)
================================


-- case 2: BC dates, origin < input
SELECT
  str,
  interval,
  date_trunc(str, ts) = date_bin(interval::interval, ts, timestamp '2000-01-01 BC') AS equal
FROM (
  VALUES
  ('week', '7 d'),
  ('day', '1 d'),
  ('hour', '1 h'),
  ('minute', '1 m'),
  ('second', '1 s'),
  ('millisecond', '1 ms'),
  ('microsecond', '1 us')
) intervals (str, interval),
(VALUES (timestamp '0055-6-10 15:44:17.71393 BC')) ts (ts)
================================


-- case 3: AD dates, origin > input
SELECT
  str,
  interval,
  date_trunc(str, ts) = date_bin(interval::interval, ts, timestamp '2020-03-02') AS equal
FROM (
  VALUES
  ('week', '7 d'),
  ('day', '1 d'),
  ('hour', '1 h'),
  ('minute', '1 m'),
  ('second', '1 s'),
  ('millisecond', '1 ms'),
  ('microsecond', '1 us')
) intervals (str, interval),
(VALUES (timestamp '2020-02-29 15:44:17.71393')) ts (ts)
================================


-- case 4: BC dates, origin > input
SELECT
  str,
  interval,
  date_trunc(str, ts) = date_bin(interval::interval, ts, timestamp '0055-06-17 BC') AS equal
FROM (
  VALUES
  ('week', '7 d'),
  ('day', '1 d'),
  ('hour', '1 h'),
  ('minute', '1 m'),
  ('second', '1 s'),
  ('millisecond', '1 ms'),
  ('microsecond', '1 us')
) intervals (str, interval),
(VALUES (timestamp '0055-6-10 15:44:17.71393 BC')) ts (ts)
================================


-- shift bins using the origin parameter:
SELECT date_bin('5 min'::interval, timestamp '2020-02-01 01:01:01', timestamp '2020-02-01 00:02:30')
================================


-- disallow intervals with months or years
SELECT date_bin('5 months'::interval, timestamp '2020-02-01 01:01:01', timestamp '2001-01-01')
================================

SELECT date_bin('5 years'::interval,  timestamp '2020-02-01 01:01:01', timestamp '2001-01-01')
================================


-- disallow zero intervals
SELECT date_bin('0 days'::interval, timestamp '1970-01-01 01:00:00' , timestamp '1970-01-01 00:00:00')
================================


-- disallow negative intervals
SELECT date_bin('-2 days'::interval, timestamp '1970-01-01 01:00:00' , timestamp '1970-01-01 00:00:00')
================================


SELECT d1 as "timestamp",
   date_part( 'quarter', d1) AS quarter, date_part( 'msec', d1) AS msec,
   date_part( 'usec', d1) AS usec
   FROM TIMESTAMP_TBL
================================


SELECT d1 as "timestamp",
   date_part( 'isoyear', d1) AS isoyear, date_part( 'week', d1) AS week,
   date_part( 'isodow', d1) AS isodow, date_part( 'dow', d1) AS dow,
   date_part( 'doy', d1) AS doy
   FROM TIMESTAMP_TBL
================================


-- Roman months, with upper and lower case.
SELECT i,
       to_char(i * interval '1mon', 'rm'),
       to_char(i * interval '1mon', 'RM')
    FROM generate_series(-13, 13) i
================================


================================


================================

select jsonb_path_query('[1]', 'strict $[1]', silent => true)
================================


select jsonb_path_exists('[{"a": 1}, {"a": 2}, 3]', 'lax $[*].a', silent => false)
================================

select jsonb_path_exists('[{"a": 1}, {"a": 2}, 3]', 'lax $[*].a', silent => true)
================================

select jsonb_path_exists('[{"a": 1}, {"a": 2}, 3]', 'strict $[*].a', silent => false)
================================

select jsonb_path_exists('[{"a": 1}, {"a": 2}, 3]', 'strict $[*].a', silent => true)
================================

select jsonb_path_query('1', 'strict $.a', silent => true)
================================

select jsonb_path_query('1', 'strict $.*', silent => true)
================================

select jsonb_path_query('[]', 'strict $.a', silent => true)
================================

select jsonb_path_query('{}', 'strict $.a', silent => true)
================================

select jsonb_path_query('1', 'strict $[1]', silent => true)
================================

select jsonb_path_query('1', 'strict $[*]', silent => true)
================================

select jsonb_path_query('[]', 'strict $[1]', silent => true)
================================

select jsonb_path_query('[]', 'strict $["a"]', silent => true)
================================

select jsonb_path_query('[1,2,3]', 'strict $[*].a', silent => true)
================================

select jsonb_path_query('[]', 'strict $[last]', silent => true)
================================

select jsonb_path_query('[1,2,3]', '$[last ? (@.type() == "string")]', silent => true)
================================

select jsonb_path_query('1', '$ + "2"', silent => true)
================================

select jsonb_path_query('[1, 2]', '3 * $', silent => true)
================================

select jsonb_path_query('"a"', '-$', silent => true)
================================

select jsonb_path_query('[1,"2",3]', '+$', silent => true)
================================

select jsonb_path_query('{"a": [1, 2]}', 'lax $.a * 3', silent => true)
================================


select jsonb_path_match('[{"a": 1}, {"a": 2}, 3]', 'lax exists($[*].a)', silent => false)
================================

select jsonb_path_match('[{"a": 1}, {"a": 2}, 3]', 'lax exists($[*].a)', silent => true)
================================

select jsonb_path_match('[{"a": 1}, {"a": 2}, 3]', 'strict exists($[*].a)', silent => false)
================================

select jsonb_path_match('[{"a": 1}, {"a": 2}, 3]', 'strict exists($[*].a)', silent => true)
================================

select jsonb_path_query('[1,null,true,"11",[],[1],[1,2,3],{},{"a":1,"b":2}]', 'strict $[*].size()', silent => true)
================================

select jsonb_path_query('[{},1]', '$[*].keyvalue()', silent => true)
================================

select jsonb_path_query('null', '$.double()', silent => true)
================================

select jsonb_path_query('true', '$.double()', silent => true)
================================

select jsonb_path_query('[]', 'strict $.double()', silent => true)
================================

select jsonb_path_query('{}', '$.double()', silent => true)
================================

select jsonb_path_query('"inf"', '$.double()', silent => true)
================================

select jsonb_path_query('"-inf"', '$.double()', silent => true)
================================

select jsonb_path_query('{}', '$.abs()', silent => true)
================================

select jsonb_path_query('true', '$.floor()', silent => true)
================================

select jsonb_path_query('"1.2"', '$.ceiling()', silent => true)
================================

SELECT jsonb_path_query_array('[{"a": 1}, {"a": 2}, {"a": 3}, {"a": 5}]', '$[*].a ? (@ > $min && @ < $max)', vars => '{"min": 1, "max": 4}')
================================

SELECT jsonb_path_query_array('[{"a": 1}, {"a": 2}, {"a": 3}, {"a": 5}]', '$[*].a ? (@ > $min && @ < $max)', vars => '{"min": 3, "max": 4}')
================================

SELECT jsonb_path_query_first('[{"a": 1}, {"a": 2}, {}]', 'strict $[*].a', silent => true)
================================

SELECT jsonb_path_query_first('[{"a": 1}, {"a": 2}, {"a": 3}, {"a": 5}]', '$[*].a ? (@ > $min && @ < $max)', vars => '{"min": 1, "max": 4}')
================================

SELECT jsonb_path_query_first('[{"a": 1}, {"a": 2}, {"a": 3}, {"a": 5}]', '$[*].a ? (@ > $min && @ < $max)', vars => '{"min": 3, "max": 4}')
================================

SELECT jsonb_path_exists('[{"a": 1}, {"a": 2}, {"a": 3}, {"a": 5}]', '$[*] ? (@.a > $min && @.a < $max)', vars => '{"min": 1, "max": 4}')
================================

SELECT jsonb_path_exists('[{"a": 1}, {"a": 2}, {"a": 3}, {"a": 5}]', '$[*] ? (@.a > $min && @.a < $max)', vars => '{"min": 3, "max": 4}')
================================


SELECT jsonb_path_match('true', '$', silent => false)
================================

SELECT jsonb_path_match('false', '$', silent => false)
================================

SELECT jsonb_path_match('null', '$', silent => false)
================================

SELECT jsonb_path_match('1', '$', silent => true)
================================

SELECT jsonb_path_match('1', '$', silent => false)
================================

SELECT jsonb_path_match('"a"', '$', silent => false)
================================

SELECT jsonb_path_match('{}', '$', silent => false)
================================

SELECT jsonb_path_match('[true]', '$', silent => false)
================================

SELECT jsonb_path_match('{}', 'lax $.a', silent => false)
================================

SELECT jsonb_path_match('{}', 'strict $.a', silent => false)
================================

SELECT jsonb_path_match('{}', 'strict $.a', silent => true)
================================

SELECT jsonb_path_match('[true, true]', '$[*]', silent => false)
================================


-- test string comparison (Unicode codepoint collation)
WITH str(j, num) AS
(
	SELECT jsonb_build_object('s', s), num
	FROM unnest('{"", "a", "ab", "abc", "abcd", "b", "A", "AB", "ABC", "ABc", "ABcD", "B"}'::text[]) WITH ORDINALITY AS a(s, num)
)
SELECT
	s1.j, s2.j,
	jsonb_path_query_first(s1.j, '$.s < $s', vars => s2.j) lt,
	jsonb_path_query_first(s1.j, '$.s <= $s', vars => s2.j) le,
	jsonb_path_query_first(s1.j, '$.s == $s', vars => s2.j) eq,
	jsonb_path_query_first(s1.j, '$.s >= $s', vars => s2.j) ge,
	jsonb_path_query_first(s1.j, '$.s > $s', vars => s2.j) gt
FROM str s1, str s2
ORDER BY s1.num, s2.num
================================


================================


--   group w/o existing GROUP BY target under ambiguous condition
--   into a table
CREATE TABLE test_missing_target2 AS
SELECT count(*)
FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY x.b ORDER BY x.b
================================


--   group w/o existing GROUP BY target under ambiguous condition
--   into a table
CREATE TABLE test_missing_target3 AS
SELECT count(x.b)
FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY x.b/2 ORDER BY x.b/2
================================


================================

-- Test comments
COMMENT ON RULE rtest_v1_bad ON rtest_v1 IS 'bad rule'
================================

COMMENT ON RULE rtest_v1_del ON rtest_v1 IS 'delete rule'
================================

COMMENT ON RULE rtest_v1_del ON rtest_v1 IS NULL
================================


create rule rtest_sys_upd as on update to rtest_system do also (
	update rtest_interface set sysname = new.sysname
		where sysname = old.sysname
================================


create rule rtest_sys_del as on delete to rtest_system do also (
	delete from rtest_interface where sysname = old.sysname
================================

	)
================================

** Remember the delete rule on rtest_v1: It says
** DO INSTEAD DELETE FROM rtest_t1 WHERE a = old.a
** So this time both rows with a = 2 must get deleted
\p
\r
delete from rtest_v1 where b = 12
================================


-- insert select
insert into rtest_v1 select * from rtest_t2
================================


-- same with swapped targetlist
insert into rtest_v1 (b, a) select b, a from rtest_t2
================================


-- now with only one target attribute
insert into rtest_v1 (a) select a from rtest_t3
================================

insert into rtest_v1 select rtest_t2.a, rtest_t3.b
    from rtest_t2, rtest_t3
    where rtest_t2.a = rtest_t3.a
================================

insert into rtest_v1 select * from rtest_t3
================================

insert into rtest_emp select * from rtest_empmass
================================


insert into rtest_t4 select * from rtest_t9 where a < 20
================================


insert into rtest_t4 select * from rtest_t9 where b ~ 'and t8'
================================


insert into rtest_t4 select a + 1, b from rtest_t9 where a in (20, 30, 40)
================================


insert into rtest_nothn1 select * from rtest_nothn4
================================


insert into rtest_nothn2 select * from rtest_nothn4
================================


insert into rtest_view3 select * from rtest_vview1 where a < 7
================================


insert into rtest_view3 select * from rtest_vview2 where a != 5 and b !~ '2'
================================


insert into rtest_view3 select * from rtest_vview3
================================


insert into rtest_view4 select * from rtest_vview4 where 3 > refcount
================================


insert into rtest_view4 select * from rtest_vview5 where a > 2 and refcount = 0
================================


insert into shoelace_ok select * from shoelace_arrive
================================


drop rule rules_foorule on rules_foo
================================


drop rule rules_foorule on rules_foo
================================


create rule rrule as
  on update to vview do instead
(
  insert into cchild (pid, descrip)
    select old.pid, new.descrip where old.descrip isnull
================================

)
================================


drop rule rrule on vview
================================



--
-- Check that ruleutils are working
--

-- temporarily disable fancy output, so view changes create less diff noise
\a\t

SELECT viewname, definition FROM pg_views
WHERE schemaname IN ('pg_catalog', 'public')
ORDER BY viewname
================================


-- restore normal output mode
\a\t

--
-- CREATE OR REPLACE RULE
--

CREATE TABLE ruletest_tbl (a int, b int)
================================

drop rule "_RETURN" on rules_fooview
================================


insert into t1 select * from generate_series(5,19,1) g
================================

create rule r1 as on update to rules_src do also
  insert into rules_log values(old.*, 'old'), (new.*, 'new')
================================

create rule r2 as on update to rules_src do also
  values(old.*, 'old'), (new.*, 'new')
================================

\d+ rules_src

--
-- Ensure an aliased target relation for insert is correctly deparsed.
--
create rule r4 as on insert to rules_src do instead insert into rules_log AS trgt SELECT NEW.* RETURNING trgt.f1, trgt.f2
================================

\d+ rules_src

--
-- Also check multiassignment deparsing.
--
create table rule_t1(f1 int, f2 int)
================================

create rule rr as on update to rule_t1 do instead UPDATE rule_dest trgt
  SET (f2[1], f1, tag) = (SELECT new.f2, new.f1, 'updated'::varchar)
  WHERE trgt.f1 = new.f1 RETURNING new.*
================================

\d+ rule_t1
drop table rule_t1, rule_dest
================================


ALTER RULE InsertRule ON rule_v1 RENAME to NewInsertRule
================================


\d+ rule_v1

--
-- error conditions for alter rename rule
--
ALTER RULE InsertRule ON rule_v1 RENAME TO NewInsertRule
================================
 -- doesn't exist
ALTER RULE NewInsertRule ON rule_v1 RENAME TO "_RETURN"
================================
 -- already exists
ALTER RULE "_RETURN" ON rule_v1 RENAME TO abc
================================

\d+ rule_v1
alter table rule_v1 rename column column2 to q2
================================

\d+ rule_v1
drop view rule_v1
================================

\d+ rule_v1
drop view rule_v1
================================

\d+ rule_v1
drop view rule_v1
================================

\d+ rule_v1
drop view rule_v1
================================

DROP RULE hat_nosert ON hats
================================

DROP RULE hat_nosert_all ON hats
================================


-- ensure upserting into a rule, with a CTE (different offsets!) works
WITH data(hat_name, hat_color) AS MATERIALIZED (
    VALUES ('h8', 'green'),
        ('h9', 'blue'),
        ('h7', 'forbidden')
)
INSERT INTO hats
    SELECT * FROM data
RETURNING *
================================

EXPLAIN (costs off)
WITH data(hat_name, hat_color) AS MATERIALIZED (
    VALUES ('h8', 'green'),
        ('h9', 'blue'),
        ('h7', 'forbidden')
)
INSERT INTO hats
    SELECT * FROM data
RETURNING *
================================


DROP RULE hat_upsert ON hats
================================


-- test for pg_get_functiondef properly regurgitating SET parameters
-- Note that the function is kept around to stress pg_dump.
CREATE FUNCTION func_with_set_params() RETURNS integer
    AS 'select 1
================================
'
    LANGUAGE SQL
    SET search_path TO PG_CATALOG
    SET extra_float_digits TO 2
    SET work_mem TO '4MB'
    SET datestyle to iso, mdy
    SET local_preload_libraries TO "Mixed/Case", 'c:/''a"/path', '', '0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789'
    IMMUTABLE STRICT
================================

CREATE RULE rules_parted_table_insert AS ON INSERT to rules_parted_table
    DO INSTEAD INSERT INTO rules_parted_table_1 VALUES (NEW.*)
================================

ALTER RULE rules_parted_table_insert ON rules_parted_table RENAME TO rules_parted_table_insert_redirect
================================


CREATE RULE rule1 AS ON INSERT TO ruletest1
    DO INSTEAD INSERT INTO ruletest2 VALUES (NEW.*)
================================


================================
--
-- Tests for the planner's "equivalence class" mechanism
--

-- One thing that's not tested well during normal querying is the logic
-- for handling "broken" ECs.  This is because an EC can only become broken
-- if its underlying btree operator family doesn't include a complete set
-- of cross-type equality operators.  There are not (and should not be)
-- any such families built into Postgres
================================
 so we have to hack things up
-- to create one.  We do this by making two alias types that are really
-- int8 (so we need no new C code) and adding only some operators for them
-- into the standard integer_ops opfamily.

create type int8alias1
================================

create type int8alias1 (
    input = int8alias1in,
    output = int8alias1out,
    like = int8
)
================================


create type int8alias2
================================

create type int8alias2 (
    input = int8alias2in,
    output = int8alias2out,
    like = int8
)
================================


create cast (int8 as int8alias1) without function
================================

create cast (int8 as int8alias2) without function
================================

create cast (int8alias1 as int8) without function
================================

create cast (int8alias2 as int8) without function
================================

create operator = (
    procedure = int8alias1eq,
    leftarg = int8alias1, rightarg = int8alias1,
    commutator = =,
    restrict = eqsel, join = eqjoinsel,
    merges
)
================================

alter operator family integer_ops using btree add
  operator 3 = (int8alias1, int8alias1)
================================

create operator = (
    procedure = int8alias2eq,
    leftarg = int8alias2, rightarg = int8alias2,
    commutator = =,
    restrict = eqsel, join = eqjoinsel,
    merges
)
================================

alter operator family integer_ops using btree add
  operator 3 = (int8alias2, int8alias2)
================================

create operator = (
    procedure = int8alias1eq,
    leftarg = int8, rightarg = int8alias1,
    restrict = eqsel, join = eqjoinsel,
    merges
)
================================

alter operator family integer_ops using btree add
  operator 3 = (int8, int8alias1)
================================

create operator = (
    procedure = int8alias1eq,
    leftarg = int8alias1, rightarg = int8alias2,
    restrict = eqsel, join = eqjoinsel,
    merges
)
================================

alter operator family integer_ops using btree add
  operator 3 = (int8alias1, int8alias2)
================================

create operator < (
    procedure = int8alias1lt,
    leftarg = int8alias1, rightarg = int8alias1
)
================================

alter operator family integer_ops using btree add
  operator 1 < (int8alias1, int8alias1)
================================

alter operator family integer_ops using btree add
  function 1 int8alias1cmp (int8, int8alias1)
================================

create policy p1 on ec1 using (f1 < '5'::int8alias1)
================================


-- with RLS active, the non-leakproof a.ff = 43 clause is not treated
-- as a suitable source for an EquivalenceClass
================================
 currently, this is true
-- even though the RLS clause has nothing to do directly with the EC
explain (costs off)
  select * from ec0 a, ec1 b
  where a.ff = b.ff and a.ff = 43::bigint::int8alias1
================================


================================


-- Various partitioning-related functions return empty/NULL if passed relations
-- of types that cannot be part of a partition tree
================================
 for example, views,
-- materialized views, legacy inheritance children or parents, etc.
CREATE VIEW ptif_test_view AS SELECT 1
================================

CREATE MATERIALIZED VIEW ptif_test_matview AS SELECT 1
================================

DROP MATERIALIZED VIEW ptif_test_matview
================================


================================

		return NEW
================================

' LANGUAGE plpgsql
================================

		return NULL
================================

' LANGUAGE plpgsql
================================

9999	\N	\\N	\NN	\N
10000	21	31	41	51
\.

COPY x (b, d) from stdin
================================

1	test_1
\.

COPY x (b, d) from stdin
================================

2	test_2
3	test_3
4	test_4
5	test_5
\.

COPY x (a, b, c, d, e) from stdin
================================

10001	22	32	42	52
10002	23	33	43	53
10003	24	34	44	54
10004	25	35	45	55
10005	26	36	46	56
\.

-- non-existent column in column list: should fail
COPY x (xyz) from stdin
================================


\.
COPY x from stdin
================================

2000	230	23	23
\.
COPY x from stdin
================================

2001	231	\N	\N
\.

-- extra data: should fail
COPY x from stdin
================================

2002	232	40	50	60	70	80
\.

-- various COPY options: delimiters, oids, NULL string, encoding
COPY x (b, c, d, e) from stdin delimiter ',' null 'x'
================================

x,45,80,90
x,\x,\\x,\\\x
x,\,,\\\,,\\
\.

COPY x from stdin WITH DELIMITER AS '
================================
' NULL AS ''
================================

3000
================================

================================
c
================================

================================

\.

COPY x from stdin WITH DELIMITER AS ':' NULL AS E'\\X' ENCODING 'sql_ascii'
================================

4000:\X:C:\X:\X
4001:1:empty::
4002:2:null:\X:\X
4003:3:Backslash:\\:\\
4004:4:BackslashX:\\X:\\X
4005:5:N:\N:\N
4006:6:BackslashN:\\N:\\N
4007:7:XX:\XX:\XX
4008:8:Delimiter:\::\:
\.

COPY x TO stdout WHERE a = 1
================================

50003	24	34	44	54
50004	25	35	45	55
50005	26	36	46	56
\.

COPY x from stdin WHERE a > 60003
================================

60001	22	32	42	52
60002	23	33	43	53
60003	24	34	44	54
60004	25	35	45	55
60005	26	36	46	56
\.

COPY x from stdin WHERE f > 60003
================================

COPY y TO stdout WITH CSV FORCE QUOTE col2 ESCAPE E'\\' ENCODING 'sql_ascii'
================================

COPY y TO stdout (FORMAT CSV, FORCE_QUOTE (col2), ESCAPE E'\\')
================================


\copy y TO stdout (FORMAT CSV)
\copy y TO stdout (FORMAT CSV, QUOTE '''', DELIMITER '|')
\copy y TO stdout (FORMAT CSV, FORCE_QUOTE (col2), ESCAPE E'\\')
\copy y TO stdout (FORMAT CSV, FORCE_QUOTE *)

--test that we read consecutive LFs properly

CREATE TEMP TABLE testnl (a int, b text, c int)
================================

1,"a field with two LFs

inside",2
\.

-- test end of copy marker
CREATE TEMP TABLE testeoc (a text)
================================

a\.
\.b
c\.d
"\."
\.

COPY testeoc TO stdout CSV
================================


COPY testnull TO stdout WITH NULL AS E'\\0'
================================


COPY testnull FROM stdin WITH NULL AS E'\\0'
================================

42	\\0
\0	\0
\.

SELECT * FROM testnull
================================

a0
b
\.
COMMIT
================================

a1
b
\.
SELECT * FROM vistest
================================

d1
e
\.
SELECT * FROM vistest
================================

a2
b
\.
SELECT * FROM vistest
================================

d2
e
\.
SELECT * FROM vistest
================================

x
y
\.
SELECT * FROM vistest
================================

p
g
\.
BEGIN
================================

m
k
\.
COMMIT
================================

d3
e
\.
COMMIT
================================

EXCEPTION
  WHEN OTHERS THEN
	INSERT INTO vistest VALUES ('subxact failure')
================================

$$ language plpgsql
================================

d4
e
\.
SELECT * FROM vistest
================================

\pset null NULL
-- should succeed with no effect ("b" remains an empty string, "c" remains NULL)
BEGIN
================================

1,,""
\.
COMMIT
================================

2,'a',,""
\.
COMMIT
================================

3,,""
\.
ROLLBACK
================================

\pset null ''

-- test case with whole-row Var in a check constraint
create table check_con_tbl (f1 int)
================================

  return $1.f1 > 0
================================

alter table check_con_tbl add check (check_con_function(check_con_tbl.*))
================================

\d+ check_con_tbl
copy check_con_tbl from stdin
================================

1
\N
\.
copy check_con_tbl from stdin
================================

0
\.
select * from check_con_tbl
================================

1	4	1
2	3	2
3	2	3
4	1	4
\.

CREATE POLICY p1 ON rls_t1 FOR SELECT USING (a % 2 = 0)
================================
 -- fail
test1
\.

CREATE FUNCTION fun_instead_of_insert_tbl() RETURNS trigger AS $$
BEGIN
  INSERT INTO instead_of_insert_tbl (name) VALUES (NEW.str)
================================

  RETURN NULL
================================

$$ LANGUAGE plpgsql
================================

test1
\.

SELECT * FROM instead_of_insert_tbl
================================

test1
\.

SELECT * FROM instead_of_insert_tbl
================================


================================


-- insert enough tuples to fill at least two pages
INSERT INTO tidrangescan SELECT i,repeat('x', 100) FROM generate_series(1,200) AS s(i)
================================

DECLARE c SCROLL CURSOR FOR SELECT ctid FROM tidrangescan WHERE ctid < '(1,0)'
================================

FETCH NEXT c
================================

FETCH NEXT c
================================

FETCH PRIOR c
================================

FETCH FIRST c
================================

FETCH LAST c
================================


================================

  child json
================================

  else
    for child in select json_array_elements(node->'Plans')
    loop
      x := find_hash(child)
================================

      if x is not null then
        return x
================================

    return null
================================

$$
================================

  hash_node json
================================

    original := hash_node->>'Original Hash Batches'
================================

    final := hash_node->>'Hash Batches'
================================

    return next
================================

$$
================================


-- Make a simple relation with well distributed keys and correctly
-- estimated size.
create table simple as
  select generate_series(1, 20000) AS id, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
================================


-- Make a relation whose size we will under-estimate.  We want stats
-- to say 1000 rows, but actually there are 20,000 rows.
create table bigger_than_it_looks as
  select generate_series(1, 20000) as id, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
================================

insert into extremely_skewed
  select 42 as id, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
  from generate_series(1, 20000)
================================


-- Make a relation with a couple of enormous tuples.
create table wide as select generate_series(1, 2) as id, rpad('', 320000, 'x') as t
================================


-- The "optimal" case: the hash table fits in memory
================================
 we plan for 1
-- batch, we stick to that number, and peak memory usage stays within
-- our work_mem budget

-- non-parallel
savepoint settings
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================


-- The "good" case: batches required, but we plan the right number
================================
 we
-- plan for some number of batches, and we stick to that number, and
-- peak memory usage says within our work_mem budget

-- non-parallel
savepoint settings
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================


-- The "bad" case: during execution we need to increase number of
-- batches
================================
 in this case we plan for 1 batch, and increase at least a
-- couple of times, and peak memory usage stays within our work_mem
-- budget

-- non-parallel
savepoint settings
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) FROM simple r JOIN bigger_than_it_looks s USING (id)
================================

$$)
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join bigger_than_it_looks s using (id)
================================

$$)
================================

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join bigger_than_it_looks s using (id)
================================

$$)
================================


-- The "ugly" case: increasing the number of batches during execution
-- doesn't help, so stop trying to fit in work_mem and hope for the
-- best
================================
 in this case we plan for 1 batch, increases just once and
-- then stop increasing because that didn't help at all, so we blow
-- right through the work_mem budget and hope for the best...

-- non-parallel
savepoint settings
================================

select * from hash_join_batches(
$$
  select count(*) from simple r join extremely_skewed s using (id)
================================

$$)
================================

select * from hash_join_batches(
$$
  select count(*) from simple r join extremely_skewed s using (id)
================================

$$)
================================

select * from hash_join_batches(
$$
  select count(*) from simple r join extremely_skewed s using (id)
================================

$$)
================================

select * from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id)
================================

$$)
================================


-- Exercise rescans.  We'll turn off parallel_leader_participation so
-- that we can check that instrumentation comes back correctly.

create table join_foo as select generate_series(1, 3) as id, 'xxxxx'::text as t
================================

create table join_bar as select generate_series(1, 10000) as id, 'xxxxx'::text as t
================================

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1
================================

$$)
================================

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1
================================

$$)
================================

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1
================================

$$)
================================

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1
================================

$$)
================================

select final > 1 as multibatch
  from hash_join_batches(
$$
  select length(max(s.t))
  from wide left join (select id, coalesce(t, '') || '' as t from wide) s using (id)
================================

$$)
================================
 --  fails hjtest_1.a <> hjtest_2.b
================================


================================
  -- not allowed

--
-- TIME simple math
--
-- We now make a distinction between time and intervals,
-- and adding two times together makes no sense at all.
-- Leave in one query to show that it is rejected,
-- and do the rest of the testing in horology.sql
-- where we do mixed-type arithmetic. - thomas 2000-12-02

SELECT f1 + time '00:01' AS "Illegal" FROM TIME_TBL
================================


--
-- test EXTRACT
--
SELECT EXTRACT(MICROSECOND FROM TIME '2020-05-26 13:30:25.575401')
================================

SELECT EXTRACT(MILLISECOND FROM TIME '2020-05-26 13:30:25.575401')
================================

SELECT EXTRACT(SECOND      FROM TIME '2020-05-26 13:30:25.575401')
================================

SELECT EXTRACT(MINUTE      FROM TIME '2020-05-26 13:30:25.575401')
================================

SELECT EXTRACT(HOUR        FROM TIME '2020-05-26 13:30:25.575401')
================================

SELECT EXTRACT(DAY         FROM TIME '2020-05-26 13:30:25.575401')
================================
  -- error
SELECT EXTRACT(FORTNIGHT   FROM TIME '2020-05-26 13:30:25.575401')
================================
  -- error
SELECT EXTRACT(TIMEZONE    FROM TIME '2020-05-26 13:30:25.575401')
================================
  -- error
SELECT EXTRACT(EPOCH       FROM TIME '2020-05-26 13:30:25.575401')
================================


-- date_part implementation is mostly the same as extract, so only
-- test a few cases for additional coverage.
SELECT date_part('microsecond', TIME '2020-05-26 13:30:25.575401')
================================

SELECT date_part('millisecond', TIME '2020-05-26 13:30:25.575401')
================================

SELECT date_part('second',      TIME '2020-05-26 13:30:25.575401')
================================

SELECT date_part('epoch',       TIME '2020-05-26 13:30:25.575401')
================================


================================


CREATE FUNCTION rngfunct(int) returns setof rngfunc2 as 'SELECT * FROM rngfunc2 WHERE rngfuncid = $1 ORDER BY f2
================================
' LANGUAGE SQL
================================

select row_to_json(s.*) from generate_series(11,14) with ordinality s
================================


-- multiple functions
select * from rows from(rngfunct(1),rngfunct(2)) with ordinality as z(a,b,c,d,ord)
================================

create temporary view vw_ord as select * from (values (1)) v(n) join rows from(rngfunct(1),rngfunct(2)) with ordinality as z(a,b,c,d,ord) on (n=ord)
================================

select * from rows from(unnest(array[10,20],array['foo','bar'],array[1.0])) with ordinality as z(a,b,c,ord)
================================

select * from rows from(unnest(array[10,20],array['foo','bar']), generate_series(101,102)) with ordinality as z(a,b,c,ord)
================================

create temporary view vw_ord as select * from rows from(unnest(array[10,20],array['foo','bar'],array[1.0])) as z(a,b,c)
================================

create temporary view vw_ord as select * from rows from(unnest(array[10,20],array['foo','bar']), generate_series(1,2)) as z(a,b,c)
================================

declare rf_cur scroll cursor for select * from rows from(generate_series(1,5),generate_series(1,2)) with ordinality as g(i,j,o)
================================

fetch all from rf_cur
================================

fetch backward all from rf_cur
================================

fetch all from rf_cur
================================

fetch next from rf_cur
================================

fetch next from rf_cur
================================

fetch prior from rf_cur
================================

fetch absolute 1 from rf_cur
================================

fetch next from rf_cur
================================

fetch next from rf_cur
================================

fetch next from rf_cur
================================

fetch prior from rf_cur
================================

fetch prior from rf_cur
================================

fetch prior from rf_cur
================================


-- sql, proretset = f, prorettype = b
CREATE FUNCTION getrngfunc1(int) RETURNS int AS 'SELECT $1
================================
' LANGUAGE SQL
================================


-- sql, proretset = t, prorettype = b
CREATE FUNCTION getrngfunc2(int) RETURNS setof int AS 'SELECT rngfuncid FROM rngfunc WHERE rngfuncid = $1
================================
' LANGUAGE SQL
================================


-- sql, proretset = t, prorettype = b
CREATE FUNCTION getrngfunc3(int) RETURNS setof text AS 'SELECT rngfuncname FROM rngfunc WHERE rngfuncid = $1
================================
' LANGUAGE SQL
================================


-- sql, proretset = f, prorettype = c
CREATE FUNCTION getrngfunc4(int) RETURNS rngfunc AS 'SELECT * FROM rngfunc WHERE rngfuncid = $1
================================
' LANGUAGE SQL
================================


-- sql, proretset = t, prorettype = c
CREATE FUNCTION getrngfunc5(int) RETURNS setof rngfunc AS 'SELECT * FROM rngfunc WHERE rngfuncid = $1
================================
' LANGUAGE SQL
================================


-- sql, proretset = f, prorettype = record
CREATE FUNCTION getrngfunc6(int) RETURNS RECORD AS 'SELECT * FROM rngfunc WHERE rngfuncid = $1
================================
' LANGUAGE SQL
================================


-- sql, proretset = t, prorettype = record
CREATE FUNCTION getrngfunc7(int) RETURNS setof record AS 'SELECT * FROM rngfunc WHERE rngfuncid = $1
================================
' LANGUAGE SQL
================================


-- plpgsql, proretset = f, prorettype = b
CREATE FUNCTION getrngfunc8(int) RETURNS int AS 'DECLARE rngfuncint int
================================
 RETURN rngfuncint
================================
' LANGUAGE plpgsql
================================


-- plpgsql, proretset = f, prorettype = c
CREATE FUNCTION getrngfunc9(int) RETURNS rngfunc AS 'DECLARE rngfunctup rngfunc%ROWTYPE
================================
 RETURN rngfunctup
================================
' LANGUAGE plpgsql
================================


-- mix 'n match kinds, to exercise expandRTE and related logic

select * from rows from(getrngfunc1(1),getrngfunc2(1),getrngfunc3(1),getrngfunc4(1),getrngfunc5(1),
                    getrngfunc6(1) AS (rngfuncid int, rngfuncsubid int, rngfuncname text),
                    getrngfunc7(1) AS (rngfuncid int, rngfuncsubid int, rngfuncname text),
                    getrngfunc8(1),getrngfunc9(1))
              with ordinality as t1(a,b,c,d,e,f,g,h,i,j,k,l,m,o,p,q,r,s,t,u)
================================

select * from rows from(getrngfunc9(1),getrngfunc8(1),
                    getrngfunc7(1) AS (rngfuncid int, rngfuncsubid int, rngfuncname text),
                    getrngfunc6(1) AS (rngfuncid int, rngfuncsubid int, rngfuncname text),
                    getrngfunc5(1),getrngfunc4(1),getrngfunc3(1),getrngfunc2(1),getrngfunc1(1))
              with ordinality as t1(a,b,c,d,e,f,g,h,i,j,k,l,m,o,p,q,r,s,t,u)
================================


create temporary view vw_rngfunc as
  select * from rows from(getrngfunc9(1),
                      getrngfunc7(1) AS (rngfuncid int, rngfuncsubid int, rngfuncname text),
                      getrngfunc1(1))
                with ordinality as t1(a,b,c,d,e,f,g,n)
================================

CREATE TYPE rngfunc_rescan_t AS (i integer, s bigint)
================================
' LANGUAGE SQL
================================
' LANGUAGE plpgsql
================================

SELECT * FROM (VALUES (1),(2),(3)) v(r) LEFT JOIN ROWS FROM( rngfunc_sql(11,13), rngfunc_mat(11,13) ) WITH ORDINALITY AS f(i1,s1,i2,s2,o) ON (r+i1+i2)<100
================================

SELECT * FROM (VALUES (1),(2),(3)) v(r), ROWS FROM( rngfunc_sql(11,11), rngfunc_mat(10+r,13) )
================================

SELECT * FROM (VALUES (1),(2),(3)) v(r), ROWS FROM( rngfunc_sql(10+r,13), rngfunc_mat(11,11) )
================================

SELECT * FROM (VALUES (1),(2),(3)) v(r), ROWS FROM( rngfunc_sql(10+r,13), rngfunc_mat(10+r,13) )
================================

SELECT * FROM generate_series(1,2) r1, generate_series(r1,3) r2, ROWS FROM( rngfunc_sql(10+r1,13), rngfunc_mat(10+r2,13) )
================================

SELECT dup(int4range(4,7))
================================

SELECT dup(numrange(4,7))
================================

  return null
================================


create rule insert_tt_rule as on insert to tt do also
  insert into tt_log values(new.*)
================================

-- and if it doesn't work, you get a compile-time not run-time error
select * from array_to_set(array['one', 'two']) as t(f1 point,f2 text)
================================

select * from array_to_set(array['one', 'two']) as t(f1 point,f2 text)
================================

$$ language sql
================================

$$ language sql
================================


-- Check that typmod imposed by a composite type is honored
create type rngfunc_type as (f1 numeric(35,6), f2 numeric(35,2))
================================

$$ language sql immutable
================================

$$ language sql volatile
================================

$$ language sql immutable
================================

$$ language sql volatile
================================

$$ language sql immutable
================================
  -- fail, scalar result type

drop type rngfunc_type cascade
================================
 $$
language sql stable
================================
 $$
language sql stable
================================
   -- make sure ordinality copes

-- multiple functions vs. dropped columns
SELECT * FROM ROWS FROM(generate_series(10,11), get_users()) WITH ORDINALITY
================================

SELECT * FROM ROWS FROM(get_users(), generate_series(10,11)) WITH ORDINALITY
================================


-- check that we can cope with post-parsing changes in rowtypes
create temp view usersview as
SELECT * FROM ROWS FROM(get_users(), generate_series(10,11)) WITH ORDINALITY
================================
 $$
language sql stable
================================


-- check handling of nulls in SRF results (bug #7808)

create type rngfunc2 as (a integer, b text)
================================


drop type rngfunc2
================================


================================


-- don't include the hash_ovfl_heap stuff in the distribution
-- the data set is too large for what it's worth
--
-- CREATE TABLE hash_ovfl_heap (
--	x			int4,
--	y			int4
-- )
================================


CREATE TYPE unknown_comptype AS (
	u unknown    -- fail
)
================================


-- invalid: non-lowercase quoted reloptions identifiers
CREATE TABLE tas_case WITH ("Fillfactor" = 10) AS SELECT 1 a
================================

REINDEX INDEX unlogged1_pkey
================================

REINDEX INDEX unlogged2_pkey
================================


CREATE TABLE as_select1 AS SELECT * FROM pg_class WHERE relkind = 'r'
================================

CREATE TABLE as_select1 AS SELECT * FROM pg_class WHERE relkind = 'r'
================================

CREATE TABLE IF NOT EXISTS as_select1 AS SELECT * FROM pg_class WHERE relkind = 'r'
================================

CREATE TABLE as_select1 AS EXECUTE select1
================================

CREATE TABLE as_select1 AS EXECUTE select1
================================

CREATE TABLE IF NOT EXISTS as_select1 AS EXECUTE select1
================================


-- create an extra wide table to test for issues related to that
-- (temporarily hide query, to avoid the long CREATE TABLE stmt)
\set ECHO none
SELECT 'CREATE TABLE extra_wide_table(firstc text, '|| array_to_string(array_agg('c'||i||' bool'),',')||', lastc text)
================================
'
FROM generate_series(1, 1100) g(i)
\gexec
\set ECHO all
INSERT INTO extra_wide_table(firstc, lastc) VALUES('first col', 'last col')
================================


-- check that tables with oids cannot be created anymore
CREATE TABLE withoid() WITH OIDS
================================
 $$ LANGUAGE SQL IMMUTABLE
================================
 $$ LANGUAGE SQL IMMUTABLE
================================
 $$ LANGUAGE SQL
================================
 $$ LANGUAGE SQL
================================


-- Partition key in describe output
\d partitioned
\d+ partitioned2

INSERT INTO partitioned2 VALUES (1, 'hello')
================================

\d+ part2_1

DROP TABLE partitioned, partitioned2
================================

\d+ partitioned1
drop table partitioned
================================

\d+ list_parted

-- forbidden expressions for partition bound with list partitioned table
CREATE TABLE part_bogus_expr_fail PARTITION OF list_parted FOR VALUES IN (somename)
================================


-- syntax does not allow empty list of values for list partitions
CREATE TABLE fail_part PARTITION OF list_parted FOR VALUES IN ()
================================

-- note that while b's default is overriden, a's default is preserved
\d parted_notnull_inh_test1
drop table parted_notnull_inh_test
================================

-- ok
================================
 partition collation silently overrides the default collation of type 'name'
create table test_part_coll_cast2 partition of test_part_coll_posix for values from (name 's') to ('z')
================================


-- Partition bound in describe output
\d+ part_b

-- Both partition bound and partition key in describe output
\d+ part_c

-- a level-2 partition's constraint will include the parent's expressions
\d+ part_c_1_10

-- Show partition count in the parent's describe output
-- Tempted to include \d+ output listing partitions with bound info but
-- output could vary depending on the order in which partition oids are
-- returned.
\d parted
\d hash_parted

-- check that we get the expected partition constraints
CREATE TABLE range_parted4 (a int, b int, c int) PARTITION BY RANGE (abs(a), abs(b), c)
================================

\d+ unbounded_range_part
DROP TABLE unbounded_range_part
================================

\d+ range_parted4_1
CREATE TABLE range_parted4_2 PARTITION OF range_parted4 FOR VALUES FROM (3, 4, 5) TO (6, 7, MAXVALUE)
================================

\d+ range_parted4_2
CREATE TABLE range_parted4_3 PARTITION OF range_parted4 FOR VALUES FROM (6, 8, MINVALUE) TO (9, MAXVALUE, MAXVALUE)
================================

\d+ range_parted4_3
DROP TABLE range_parted4
================================
 $$
================================

CREATE OPERATOR CLASS test_int4_ops FOR TYPE int4 USING btree AS
  OPERATOR 1 < (int4,int4), OPERATOR 2 <= (int4,int4),
  OPERATOR 3 = (int4,int4), OPERATOR 4 >= (int4,int4),
  OPERATOR 5 > (int4,int4), FUNCTION 1 my_int4_sort(int4,int4)
================================

DROP OPERATOR CLASS test_int4_ops USING btree
================================

COMMENT ON TABLE parted_col_comment IS 'Am partitioned table'
================================

COMMENT ON COLUMN parted_col_comment.a IS 'Partition key'
================================

\d+ parted_col_comment
DROP TABLE parted_col_comment
================================

\d+ arrlp12
DROP TABLE arrlp
================================

\d+ boolspart
drop table boolspart
================================

    return null
================================

\d part_column_drop
\d part_column_drop_1_10
drop table part_column_drop
================================


================================


-- Test shorthand input values
-- We can't just "select" the results since they aren't constants
================================
 test for
-- equality instead.  We can do that by running the test inside a transaction
-- block, within which the value of 'now' shouldn't change, and so these
-- related values shouldn't either.

BEGIN
================================


SELECT count(*) AS One FROM TIMESTAMPTZ_TBL WHERE d1 = timestamp with time zone 'today'
================================

SELECT count(*) AS One FROM TIMESTAMPTZ_TBL WHERE d1 = timestamp with time zone 'tomorrow'
================================

SELECT count(*) AS One FROM TIMESTAMPTZ_TBL WHERE d1 = timestamp with time zone 'yesterday'
================================

SELECT count(*) AS One FROM TIMESTAMPTZ_TBL WHERE d1 = timestamp with time zone 'tomorrow EST'
================================

SELECT count(*) AS One FROM TIMESTAMPTZ_TBL WHERE d1 = timestamp with time zone 'tomorrow zulu'
================================

SELECT count(*) AS two FROM TIMESTAMPTZ_TBL WHERE d1 = timestamp(2) with time zone 'now'
================================
  -- out of range

-- Demonstrate functions and operators
SELECT d1 FROM TIMESTAMPTZ_TBL
   WHERE d1 > timestamp with time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMPTZ_TBL
   WHERE d1 < timestamp with time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMPTZ_TBL
   WHERE d1 = timestamp with time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMPTZ_TBL
   WHERE d1 != timestamp with time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMPTZ_TBL
   WHERE d1 <= timestamp with time zone '1997-01-02'
================================


SELECT d1 FROM TIMESTAMPTZ_TBL
   WHERE d1 >= timestamp with time zone '1997-01-02'
================================


SELECT date_trunc( 'week', timestamp with time zone '2004-02-29 15:44:17.71393' ) AS week_trunc
================================


SELECT date_trunc('day', timestamp with time zone '2001-02-16 20:38:40+00', 'Australia/Sydney') as sydney_trunc
================================
  -- zone name
SELECT date_trunc('day', timestamp with time zone '2001-02-16 20:38:40+00', 'GMT') as gmt_trunc
================================
  -- fixed-offset abbreviation
SELECT date_trunc('day', timestamp with time zone '2001-02-16 20:38:40+00', 'VET') as vet_trunc
================================
  -- variable-offset abbreviation

-- verify date_bin behaves the same as date_trunc for relevant intervals
SELECT
  str,
  interval,
  date_trunc(str, ts, 'Australia/Sydney') = date_bin(interval::interval, ts, timestamp with time zone '2001-01-01+11') AS equal
FROM (
  VALUES
  ('day', '1 d'),
  ('hour', '1 h'),
  ('minute', '1 m'),
  ('second', '1 s'),
  ('millisecond', '1 ms'),
  ('microsecond', '1 us')
) intervals (str, interval),
(VALUES (timestamptz '2020-02-29 15:44:17.71393+00')) ts (ts)
================================


-- disallow intervals with months or years
SELECT date_bin('5 months'::interval, timestamp with time zone '2020-02-01 01:01:01+00', timestamp with time zone '2001-01-01+00')
================================

SELECT date_bin('5 years'::interval,  timestamp with time zone '2020-02-01 01:01:01+00', timestamp with time zone '2001-01-01+00')
================================


-- disallow zero intervals
SELECT date_bin('0 days'::interval, timestamp with time zone '1970-01-01 01:00:00+00' , timestamp with time zone '1970-01-01 00:00:00+00')
================================


-- disallow negative intervals
SELECT date_bin('-2 days'::interval, timestamp with time zone '1970-01-01 01:00:00+00' , timestamp with time zone '1970-01-01 00:00:00+00')
================================


SELECT d1 as timestamptz,
   date_part( 'quarter', d1) AS quarter, date_part( 'msec', d1) AS msec,
   date_part( 'usec', d1) AS usec
   FROM TIMESTAMPTZ_TBL
================================


SELECT d1 as timestamptz,
   date_part( 'isoyear', d1) AS isoyear, date_part( 'week', d1) AS week,
   date_part( 'isodow', d1) AS isodow, date_part( 'dow', d1) AS dow,
   date_part( 'doy', d1) AS doy
   FROM TIMESTAMPTZ_TBL
================================


================================
--
-- Tests to exercise the plan caching/invalidation mechanism
--

CREATE TEMP TABLE pcachetest AS SELECT * FROM int8_tbl
================================


EXECUTE prepstmt
================================


EXECUTE prepstmt
================================


-- recreate the temp table (this demonstrates that the raw plan is
-- purely textual and doesn't depend on OIDs, for instance)
CREATE TEMP TABLE pcachetest AS SELECT * FROM int8_tbl ORDER BY 2
================================


EXECUTE prepstmt
================================


EXECUTE prepstmt
================================


EXECUTE prepstmt
================================


EXECUTE vprep
================================


EXECUTE vprep
================================

	return total
================================

end$$ language plpgsql
================================


--- Check that change of search_path is honored when re-using cached plan

create schema s1
  create table abc (f1 int)
================================


create schema s2
  create table abc (f1 int)
================================


execute p1
================================


execute p1
================================
   -- force replan

execute p1
================================


drop schema s1 cascade
================================

drop schema s2 cascade
================================


execute p2
================================


execute p2
================================

  create temp table temptable as select * from generate_series(1,3) as f1
================================

  for r in select * from vv loop
    raise notice '%', r
================================

end$$ language plpgsql
================================

insert into test_mode select 1 from generate_series(1,1000) union all select 2
================================


================================
  -- invalid chr code

================================
--
-- SELECT_DISTINCT
--

--
-- awk '{print $3
================================
}' onek.data | sort -n | uniq
--
SELECT DISTINCT two FROM tmp ORDER BY 1
================================


--
-- awk '{print $5
================================
}' onek.data | sort -n | uniq
--
SELECT DISTINCT ten FROM tmp ORDER BY 1
================================


--
-- awk '{print $16
================================
}' onek.data | sort -d | uniq
--
SELECT DISTINCT string4 FROM tmp ORDER BY 1
================================


--
-- awk '{print $3,$16,$5
================================
}' onek.data | sort -d | uniq |
-- sort +0n -1 +1d -2 +2n -3
--
SELECT DISTINCT two, string4, ten
   FROM tmp
   ORDER BY two using <, string4 using <, ten using <
================================


--
-- awk '{print $2
================================
}' person.data |
-- awk '{if(NF!=1){print $2
================================
}else{print
================================
}}' - emp.data |
-- awk '{if(NF!=1){print $2
================================
}else{print
================================
}}' - student.data |
-- awk 'BEGIN{FS="      "
================================
}{if(NF!=1){print $5
================================
}else{print
================================
}}' - stud_emp.data |
-- sort -n -r | uniq
--
SELECT DISTINCT p.age FROM person* p ORDER BY age using >
================================


CREATE TABLE distinct_group_1 AS
SELECT DISTINCT g%1000 FROM generate_series(0,9999) g
================================


CREATE TABLE distinct_group_2 AS
SELECT DISTINCT (g%1000)::text FROM generate_series(0,9999) g
================================


CREATE TABLE distinct_hash_1 AS
SELECT DISTINCT g%1000 FROM generate_series(0,9999) g
================================


CREATE TABLE distinct_hash_2 AS
SELECT DISTINCT (g%1000)::text FROM generate_series(0,9999) g
================================

$$ LANGUAGE plpgsql PARALLEL UNSAFE
================================

$$ LANGUAGE plpgsql PARALLEL SAFE
================================


================================


select q1, float8(count(*)) / (select count(*) from int8_tbl)
from int8_tbl group by q1 order by q1
================================


--
-- Check EXISTS simplification with LIMIT
--
explain (costs off)
select * from int4_tbl o where exists
  (select 1 from int4_tbl i where i.f1=o.f1 limit null)
================================

select (select (a.*)::text) from view_a a
================================


create operator = (procedure=bogus_int8_text_eq, leftarg=int8, rightarg=text)
================================

insert into exists_tbl select x, x/2, x+1 from generate_series(0,10) x
================================


--
-- Check that volatile quals aren't pushed down past a set-returning function
================================

  return x > y
================================

end$$
================================

        return next ln
================================

$$
================================


declare c1 scroll cursor for
 select * from generate_series(1,4) i
  where i <> all (values (2),(3))
================================


move forward all in c1
================================

fetch backward all in c1
================================


================================

-- use fillfactor so we don't have to load too much data to get multiple pages

INSERT INTO test_tablesample
  SELECT i, repeat(i::text, 200) FROM generate_series(0, 9) s(i)
================================

\d+ test_tablesample_v1
\d+ test_tablesample_v2

-- check a sampled query doesn't affect cursor in progress
BEGIN
================================

DECLARE tablesample_cur SCROLL CURSOR FOR
  SELECT id FROM test_tablesample TABLESAMPLE SYSTEM (50) REPEATABLE (0)
================================


FETCH FIRST FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================


FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================


FETCH FIRST FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================

FETCH NEXT FROM tablesample_cur
================================


CLOSE tablesample_cur
================================


================================


-- data modification
CREATE TABLE fewmore AS SELECT generate_series(1,3) AS data
================================

-- SRFs are not allowed in LIMIT.
SELECT 1 LIMIT generate_series(1,3)
================================


-- tSRF in correlated subquery, referencing table outside
SELECT (SELECT generate_series(1,3) LIMIT 1 OFFSET few.id) FROM few
================================

-- tSRF in correlated subquery, referencing SRF outside
SELECT (SELECT generate_series(1,3) LIMIT 1 OFFSET g.i) FROM generate_series(0,3) g(i)
================================


-- Operators can return sets too
CREATE OPERATOR |@| (PROCEDURE = unnest, RIGHTARG = ANYARRAY)
================================


================================
-- should fail, return type mismatch
create event trigger regress_event_trigger
   on ddl_command_start
   execute procedure pg_backend_pid()
================================


-- should fail, no elephant_bootstrap entry point
create event trigger regress_event_trigger on elephant_bootstrap
   execute procedure test_event_trigger()
================================


-- OK
create event trigger regress_event_trigger on ddl_command_start
   execute procedure test_event_trigger()
================================


-- OK
create event trigger regress_event_trigger_end on ddl_command_end
   execute function test_event_trigger()
================================


-- should fail, food is not a valid filter variable
create event trigger regress_event_trigger2 on ddl_command_start
   when food in ('sandwich')
   execute procedure test_event_trigger()
================================


-- should fail, sandwich is not a valid command tag
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('sandwich')
   execute procedure test_event_trigger()
================================


-- should fail, create skunkcabbage is not a valid command tag
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('create table', 'create skunkcabbage')
   execute procedure test_event_trigger()
================================


-- should fail, can't have event triggers on event triggers
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('DROP EVENT TRIGGER')
   execute procedure test_event_trigger()
================================


-- should fail, can't have event triggers on global objects
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('CREATE ROLE')
   execute procedure test_event_trigger()
================================


-- should fail, can't have event triggers on global objects
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('CREATE DATABASE')
   execute procedure test_event_trigger()
================================


-- should fail, can't have event triggers on global objects
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('CREATE TABLESPACE')
   execute procedure test_event_trigger()
================================


-- should fail, can't have same filter variable twice
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('create table') and tag in ('CREATE FUNCTION')
   execute procedure test_event_trigger()
================================


-- should fail, can't have arguments
create event trigger regress_event_trigger2 on ddl_command_start
   execute procedure test_event_trigger('argument not allowed')
================================


-- OK
create event trigger regress_event_trigger2 on ddl_command_start
   when tag in ('create table', 'CREATE FUNCTION')
   execute procedure test_event_trigger()
================================


-- OK
comment on event trigger regress_event_trigger is 'test comment'
================================

create event trigger regress_event_trigger_noperms on ddl_command_start
   execute procedure test_event_trigger()
================================


-- test enabling and disabling
alter event trigger regress_event_trigger disable
================================

alter event trigger regress_event_trigger enable
================================

alter event trigger regress_event_trigger enable replica
================================

alter event trigger regress_event_trigger enable always
================================

  return 0
================================


-- clean up
alter event trigger regress_event_trigger disable
================================

drop routine f1(), p1()
================================

comment on table event_trigger_fire1 is 'here is a comment'
================================

create foreign data wrapper useless
================================

create server useless_server foreign data wrapper useless
================================

alter default privileges for role regress_evt_user
 revoke delete on tables from regress_evt_user
================================


-- alter owner to non-superuser should fail
alter event trigger regress_event_trigger owner to regress_evt_user
================================

alter event trigger regress_event_trigger owner to regress_evt_user
================================


-- should fail, name collision
alter event trigger regress_event_trigger rename to regress_event_trigger2
================================


-- OK
alter event trigger regress_event_trigger rename to regress_event_trigger3
================================


-- should fail, doesn't exist any more
drop event trigger regress_event_trigger
================================


-- cleanup before next test
-- these are all OK
================================
 the second one should emit a NOTICE
drop event trigger if exists regress_event_trigger2
================================

drop event trigger if exists regress_event_trigger2
================================

drop event trigger regress_event_trigger3
================================

drop event trigger regress_event_trigger_end
================================


-- test support for dropped objects
CREATE SCHEMA schema_one authorization regress_evt_user
================================

CREATE SCHEMA schema_two authorization regress_evt_user
================================

CREATE SCHEMA audit_tbls authorization regress_evt_user
================================
 $$
================================

CREATE AGGREGATE schema_two.newton
  (BASETYPE = int, SFUNC = schema_two.add, STYPE = int)
================================


-- This tests errors raised within event triggers
================================
 the one in audit_tbls
-- uses 2nd-level recursive invocation via test_evtrig_dropped_objects().
CREATE OR REPLACE FUNCTION undroppable() RETURNS event_trigger
LANGUAGE plpgsql AS $$
DECLARE
	obj record
================================

	IF NOT FOUND THEN
		RAISE NOTICE 'table undroppable_objs not found, skipping'
================================

		RETURN
================================

	FOR obj IN
		SELECT * FROM pg_event_trigger_dropped_objects() JOIN
			undroppable_objs USING (object_type, object_identity)
	LOOP
		RAISE EXCEPTION 'object % of type % cannot be dropped',
			obj.object_identity, obj.object_type
================================

$$
================================


CREATE EVENT TRIGGER undroppable ON sql_drop
	EXECUTE PROCEDURE undroppable()
================================


CREATE EVENT TRIGGER regress_event_trigger_drop_objects ON sql_drop
	WHEN TAG IN ('drop table', 'drop function', 'drop view',
		'drop owned', 'drop schema', 'alter table')
	EXECUTE PROCEDURE test_evtrig_dropped_objects()
================================

DROP SCHEMA schema_one, schema_two CASCADE
================================

DROP SCHEMA schema_one, schema_two CASCADE
================================

DROP SCHEMA schema_one, schema_two CASCADE
================================


DROP OWNED BY regress_evt_user
================================


DROP EVENT TRIGGER regress_event_trigger_drop_objects
================================

DROP EVENT TRIGGER undroppable
================================

    RAISE NOTICE 'NORMAL: orig=% normal=% istemp=% type=% identity=% name=% args=%',
        r.original, r.normal, r.is_temporary, r.object_type,
        r.object_identity, r.address_names, r.address_args
================================
 $$
================================

CREATE EVENT TRIGGER regress_event_trigger_report_dropped ON sql_drop
    EXECUTE PROCEDURE event_trigger_report_dropped()
================================
 $$
================================

CREATE EVENT TRIGGER regress_event_trigger_report_end ON ddl_command_end
  EXECUTE PROCEDURE event_trigger_report_end()
================================


CREATE SCHEMA evttrig
	CREATE TABLE one (col_a SERIAL PRIMARY KEY, col_b text DEFAULT 'forty two', col_c SERIAL)
	CREATE INDEX one_idx ON one (col_b)
	CREATE TABLE two (col_c INTEGER CHECK (col_c > 0) REFERENCES one DEFAULT 42)
	CREATE TABLE id (col_d int NOT NULL GENERATED ALWAYS AS IDENTITY)
================================

DROP SCHEMA evttrig CASCADE
================================


DROP EVENT TRIGGER regress_event_trigger_report_dropped
================================

DROP EVENT TRIGGER regress_event_trigger_report_end
================================

$$
================================


create event trigger no_rewrite_allowed on table_rewrite
  execute procedure test_evtrig_no_rewrite()
================================

insert into rewriteme
     select x * 1.001 from generate_series(1, 500) as t(x)
================================

$$
================================

$$
================================


create type rewritetype as (a int)
================================

alter type rewritetype alter attribute a type text cascade
================================

alter type rewritetype alter attribute a type varchar cascade
================================

drop event trigger no_rewrite_allowed
================================

$$ LANGUAGE plpgsql
================================

$$ LANGUAGE plpgsql
================================

$$ LANGUAGE plpgsql
================================


CREATE EVENT TRIGGER start_rls_command ON ddl_command_start
    WHEN TAG IN ('CREATE POLICY', 'ALTER POLICY', 'DROP POLICY') EXECUTE PROCEDURE start_command()
================================


CREATE EVENT TRIGGER end_rls_command ON ddl_command_end
    WHEN TAG IN ('CREATE POLICY', 'ALTER POLICY', 'DROP POLICY') EXECUTE PROCEDURE end_command()
================================


CREATE EVENT TRIGGER sql_drop_command ON sql_drop
    WHEN TAG IN ('DROP POLICY') EXECUTE PROCEDURE drop_sql_command()
================================


CREATE POLICY p1 ON event_trigger_test USING (FALSE)
================================

ALTER POLICY p1 ON event_trigger_test USING (TRUE)
================================

ALTER POLICY p1 ON event_trigger_test RENAME TO p2
================================

DROP POLICY p2 ON event_trigger_test
================================


DROP EVENT TRIGGER start_rls_command
================================

DROP EVENT TRIGGER end_rls_command
================================

DROP EVENT TRIGGER sql_drop_command
================================


================================
--
-- TYPE_SANITY
-- Sanity checks for common errors in making type-related system tables:
-- pg_type, pg_class, pg_attribute, pg_range.
--
-- None of the SELECTs here should ever find any matching entries,
-- so the expected output is easy to maintain 
================================
-).
-- A test failure indicates someone messed up an entry in the system tables.
--
-- NB: we assume the oidjoins test will have caught any dangling links,
-- that is OID or REGPROC fields that are not zero and do not match some
-- row in the linked-to table.  However, if we want to enforce that a link
-- field can't be 0, we have to check it here.

-- **************** pg_type ****************

-- Look for illegal values in pg_type fields.

SELECT p1.oid, p1.typname
FROM pg_type as p1
WHERE p1.typnamespace = 0 OR
    (p1.typlen <= 0 AND p1.typlen != -1 AND p1.typlen != -2) OR
    (p1.typtype not in ('b', 'c', 'd', 'e', 'p', 'r', 'm')) OR
    NOT p1.typisdefined OR
    (p1.typalign not in ('c', 's', 'i', 'd')) OR
    (p1.typstorage not in ('p', 'x', 'e', 'm'))
================================


-- Check for bogus typinput routines

SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typinput = p2.oid AND NOT
    ((p2.pronargs = 1 AND p2.proargtypes[0] = 'cstring'::regtype) OR
     (p2.pronargs = 2 AND p2.proargtypes[0] = 'cstring'::regtype AND
      p2.proargtypes[1] = 'oid'::regtype) OR
     (p2.pronargs = 3 AND p2.proargtypes[0] = 'cstring'::regtype AND
      p2.proargtypes[1] = 'oid'::regtype AND
      p2.proargtypes[2] = 'int4'::regtype))
================================


-- Check for type of the variadic array parameter's elements.
-- provariadic should be ANYOID if the type of the last element is ANYOID,
-- ANYELEMENTOID if the type of the last element is ANYARRAYOID,
-- ANYCOMPATIBLEOID if the type of the last element is ANYCOMPATIBLEARRAYOID,
-- and otherwise the element type corresponding to the array type.

SELECT oid::regprocedure, provariadic::regtype, proargtypes::regtype[]
FROM pg_proc
WHERE provariadic != 0
AND case proargtypes[array_length(proargtypes, 1)-1]
	WHEN '"any"'::regtype THEN '"any"'::regtype
	WHEN 'anyarray'::regtype THEN 'anyelement'::regtype
	WHEN 'anycompatiblearray'::regtype THEN 'anycompatible'::regtype
	ELSE (SELECT t.oid
		  FROM pg_type t
		  WHERE t.typarray = proargtypes[array_length(proargtypes, 1)-1])
	END  != provariadic
================================


-- Check for bogus typoutput routines

-- As of 8.0, this check finds refcursor, which is borrowing
-- other types' I/O routines
SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typoutput = p2.oid AND p1.typtype in ('b', 'p') AND NOT
    (p2.pronargs = 1 AND
     (p2.proargtypes[0] = p1.oid OR
      (p2.oid = 'array_out'::regproc AND
       p1.typelem != 0 AND p1.typlen = -1)))
ORDER BY 1
================================


-- Check for bogus typreceive routines

SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typreceive = p2.oid AND NOT
    ((p2.pronargs = 1 AND p2.proargtypes[0] = 'internal'::regtype) OR
     (p2.pronargs = 2 AND p2.proargtypes[0] = 'internal'::regtype AND
      p2.proargtypes[1] = 'oid'::regtype) OR
     (p2.pronargs = 3 AND p2.proargtypes[0] = 'internal'::regtype AND
      p2.proargtypes[1] = 'oid'::regtype AND
      p2.proargtypes[2] = 'int4'::regtype))
================================


-- Check for bogus typsend routines

-- As of 7.4, this check finds refcursor, which is borrowing
-- other types' I/O routines
SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typsend = p2.oid AND p1.typtype in ('b', 'p') AND NOT
    (p2.pronargs = 1 AND
     (p2.proargtypes[0] = p1.oid OR
      (p2.oid = 'array_send'::regproc AND
       p1.typelem != 0 AND p1.typlen = -1)))
ORDER BY 1
================================


-- Check for bogus typmodin routines

SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typmodin = p2.oid AND NOT
    (p2.pronargs = 1 AND
     p2.proargtypes[0] = 'cstring[]'::regtype AND
     p2.prorettype = 'int4'::regtype AND NOT p2.proretset)
================================


-- Check for bogus typmodout routines

SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typmodout = p2.oid AND NOT
    (p2.pronargs = 1 AND
     p2.proargtypes[0] = 'int4'::regtype AND
     p2.prorettype = 'cstring'::regtype AND NOT p2.proretset)
================================


-- Check for bogus typanalyze routines

SELECT p1.oid, p1.typname, p2.oid, p2.proname
FROM pg_type AS p1, pg_proc AS p2
WHERE p1.typanalyze = p2.oid AND NOT
    (p2.pronargs = 1 AND
     p2.proargtypes[0] = 'internal'::regtype AND
     p2.prorettype = 'bool'::regtype AND NOT p2.proretset)
================================


-- Cross-check against pg_type entry
-- NOTE: we allow attstorage to be 'plain' even when typstorage is not
================================


-- canonical function, if any, had better match the range type

SELECT p1.rngtypid, p1.rngsubtype, p.proname
FROM pg_range p1 JOIN pg_proc p ON p.oid = p1.rngcanonical
WHERE pronargs != 1 OR proargtypes[0] != rngtypid OR prorettype != rngtypid
================================


-- subdiff function, if any, had better match the subtype

SELECT p1.rngtypid, p1.rngsubtype, p.proname
FROM pg_range p1 JOIN pg_proc p ON p.oid = p1.rngsubdiff
WHERE pronargs != 2
    OR proargtypes[0] != rngsubtype OR proargtypes[1] != rngsubtype
    OR prorettype != 'pg_catalog.float8'::regtype
================================


-- Create a table that holds all the known in-core data types and leave it
-- around so as pg_upgrade is able to test their binary compatibility.
CREATE TABLE tab_core_types AS SELECT
  '(11,12)'::point,
  '(1,1),(2,2)'::line,
  '((11,11),(12,12))'::lseg,
  '((11,11),(13,13))'::box,
  '((11,12),(13,13),(14,14))'::path AS openedpath,
  '[(11,12),(13,13),(14,14)]'::path AS closedpath,
  '((11,12),(13,13),(14,14))'::polygon,
  '1,1,1'::circle,
  'today'::date,
  'now'::time,
  'now'::timestamp,
  'now'::timetz,
  'now'::timestamptz,
  '12 seconds'::interval,
  '{"reason":"because"}'::json,
  '{"when":"now"}'::jsonb,
  '$.a[*] ? (@ > 2)'::jsonpath,
  '127.0.0.1'::inet,
  '127.0.0.0/8'::cidr,
  '00:01:03:86:1c:ba'::macaddr8,
  '00:01:03:86:1c:ba'::macaddr,
  2::int2, 4::int4, 8::int8,
  4::float4, '8'::float8, pi()::numeric,
  'foo'::"char",
  'c'::bpchar,
  'abc'::varchar,
  'name'::name,
  'txt'::text,
  true::bool,
  E'\\xDEADBEEF'::bytea,
  B'10001'::bit,
  B'10001'::varbit AS varbit,
  '12.34'::money,
  'abc'::refcursor,
  '1 2'::int2vector,
  '1 2'::oidvector,
  format('%I=UC/%I', USER, USER)::aclitem AS aclitem,
  'a fat cat sat on a mat and ate a fat rat'::tsvector,
  'fat & rat'::tsquery,
  'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11'::uuid,
  '11'::xid8,
  'pg_class'::regclass,
  'regtype'::regtype type,
  'pg_monitor'::regrole,
  'pg_class'::regclass::oid,
  '(1,1)'::tid, '2'::xid, '3'::cid,
  '10:20:10,14,15'::txid_snapshot,
  '10:20:10,14,15'::pg_snapshot,
  '16/B374D848'::pg_lsn,
  1::information_schema.cardinal_number,
  'l'::information_schema.character_data,
  'n'::information_schema.sql_identifier,
  'now'::information_schema.time_stamp,
  'YES'::information_schema.yes_or_no,
  'venus'::planets,
  'i16'::insenum,
  '(1,2)'::int4range, '{(1,2)}'::int4multirange,
  '(3,4)'::int8range, '{(3,4)}'::int8multirange,
  '(1,2)'::float8range, '{(1,2)}'::float8multirange,
  '(3,4)'::numrange, '{(3,4)}'::nummultirange,
  '(a,b)'::textrange, '{(a,b)}'::textmultirange,
  '(12.34, 56.78)'::cashrange, '{(12.34, 56.78)}'::cashmultirange,
  '(2020-01-02, 2021-02-03)'::daterange,
  '{(2020-01-02, 2021-02-03)}'::datemultirange,
  '(2020-01-02 03:04:05, 2021-02-03 06:07:08)'::tsrange,
  '{(2020-01-02 03:04:05, 2021-02-03 06:07:08)}'::tsmultirange,
  '(2020-01-02 03:04:05, 2021-02-03 06:07:08)'::tstzrange,
  '{(2020-01-02 03:04:05, 2021-02-03 06:07:08)}'::tstzmultirange,
  arrayrange(ARRAY[1,2], ARRAY[2,1]),
  arraymultirange(arrayrange(ARRAY[1,2], ARRAY[2,1]))
================================


================================
--
-- Verify system catalog foreign key relationships
--
DO $doblock$
declare
  fk record
================================

  nkeys integer
================================

  cmd text
================================

  err record
================================

    nkeys := array_length(fk.fkcols, 1)
================================

    cmd := 'SELECT ctid'
================================

    for i in 1 .. nkeys loop
      cmd := cmd || ', ' || quote_ident(fk.fkcols[i])
================================

    if fk.is_array then
      cmd := cmd || ' FROM (SELECT ctid'
================================

      for i in 1 .. nkeys-1 loop
        cmd := cmd || ', ' || quote_ident(fk.fkcols[i])
================================

      cmd := cmd || ', unnest(' || quote_ident(fk.fkcols[nkeys])
================================

      cmd := cmd || ') as ' || quote_ident(fk.fkcols[nkeys])
================================

      cmd := cmd || ' FROM ' || fk.fktable::text || ') fk WHERE '
================================

    else
      cmd := cmd || ' FROM ' || fk.fktable::text || ' fk WHERE '
================================

    if fk.is_opt then
      for i in 1 .. nkeys loop
        cmd := cmd || quote_ident(fk.fkcols[i]) || ' != 0 AND '
================================

    cmd := cmd || 'NOT EXISTS(SELECT 1 FROM ' || fk.pktable::text || ' pk WHERE '
================================

    for i in 1 .. nkeys loop
      if i > 1 then cmd := cmd || ' AND '
================================

      cmd := cmd || 'pk.' || quote_ident(fk.pkcols[i])
================================

      cmd := cmd || ' = fk.' || quote_ident(fk.fkcols[i])
================================

    cmd := cmd || ')'
================================

    -- raise notice 'cmd = %', cmd
================================

    for err in execute cmd loop
      raise warning 'FK VIOLATION IN %(%): %', fk.fktable, fk.fkcols, err
================================


================================

SELECT normalize(U&'\0061\0308\24D1c') = U&'\00E4\24D1c' COLLATE "C" AS test_default
================================

SELECT normalize(U&'\0061\0308\24D1c', NFC) = U&'\00E4\24D1c' COLLATE "C" AS test_nfc
================================

SELECT normalize(U&'\00E4bc', NFC) = U&'\00E4bc' COLLATE "C" AS test_nfc_idem
================================

SELECT normalize(U&'\00E4\24D1c', NFD) = U&'\0061\0308\24D1c' COLLATE "C" AS test_nfd
================================

SELECT normalize(U&'\0061\0308\24D1c', NFKC) = U&'\00E4bc' COLLATE "C" AS test_nfkc
================================

SELECT normalize(U&'\00E4\24D1c', NFKD) = U&'\0061\0308bc' COLLATE "C" AS test_nfkd
================================
  -- run-time error

SELECT U&'\00E4\24D1c' IS NORMALIZED AS test_default
================================

SELECT U&'\00E4\24D1c' IS NFC NORMALIZED AS test_nfc
================================


SELECT num, val,
    val IS NFC NORMALIZED AS NFC,
    val IS NFD NORMALIZED AS NFD,
    val IS NFKC NORMALIZED AS NFKC,
    val IS NFKD NORMALIZED AS NFKD
FROM
  (VALUES (1, U&'\00E4bc'),
          (2, U&'\0061\0308bc'),
          (3, U&'\00E4\24D1c'),
          (4, U&'\0061\0308\24D1c'),
          (5, '')) vals (num, val)
ORDER BY num
================================
  -- run-time error

================================


SELECT v as value, hash_aclitem(v)::bit(32) as standard,
       hash_aclitem_extended(v, 0)::bit(32) as extended0,
       hash_aclitem_extended(v, 1)::bit(32) as extended1
FROM   (SELECT DISTINCT(relacl[1]) FROM pg_class LIMIT 10) x(v)
WHERE  hash_aclitem(v)::bit(32) != hash_aclitem_extended(v, 0)::bit(32)
       OR hash_aclitem(v)::bit(32) = hash_aclitem_extended(v, 1)::bit(32)
================================


SELECT v as value, hashmacaddr(v)::bit(32) as standard,
       hashmacaddrextended(v, 0)::bit(32) as extended0,
       hashmacaddrextended(v, 1)::bit(32) as extended1
FROM   (VALUES (NULL::macaddr), ('08:00:2b:01:02:04'), ('08:00:2b:01:02:04'),
        ('e2:7f:51:3e:70:49'), ('d6:a9:4a:78:1c:d5'),
        ('ea:29:b1:5e:1f:a5')) x(v)
WHERE  hashmacaddr(v)::bit(32) != hashmacaddrextended(v, 0)::bit(32)
       OR hashmacaddr(v)::bit(32) = hashmacaddrextended(v, 1)::bit(32)
================================


SELECT v as value, hashinet(v)::bit(32) as standard,
       hashinetextended(v, 0)::bit(32) as extended0,
       hashinetextended(v, 1)::bit(32) as extended1
FROM   (VALUES (NULL::inet), ('192.168.100.128/25'), ('192.168.100.0/8'),
        ('172.168.10.126/16'), ('172.18.103.126/24'), ('192.188.13.16/32')) x(v)
WHERE  hashinet(v)::bit(32) != hashinetextended(v, 0)::bit(32)
       OR hashinet(v)::bit(32) = hashinetextended(v, 1)::bit(32)
================================


SELECT v as value, hashmacaddr8(v)::bit(32) as standard,
       hashmacaddr8extended(v, 0)::bit(32) as extended0,
       hashmacaddr8extended(v, 1)::bit(32) as extended1
FROM   (VALUES (NULL::macaddr8), ('08:00:2b:01:02:04:36:49'),
        ('08:00:2b:01:02:04:f0:e8'), ('e2:7f:51:3e:70:49:16:29'),
        ('d6:a9:4a:78:1c:d5:47:32'), ('ea:29:b1:5e:1f:a5')) x(v)
WHERE  hashmacaddr8(v)::bit(32) != hashmacaddr8extended(v, 0)::bit(32)
       OR hashmacaddr8(v)::bit(32) = hashmacaddr8extended(v, 1)::bit(32)
================================


-- array hashing with non-hashable element type
SELECT v as value, hash_array(v)::bit(32) as standard
FROM   (VALUES ('{0}'::money[])) x(v)
================================

SELECT v as value, hash_array_extended(v, 0)::bit(32) as extended0
FROM   (VALUES ('{0}'::money[])) x(v)
================================


CREATE TYPE mood AS ENUM ('sad', 'ok', 'happy')
================================

DROP TYPE mood
================================


SELECT v as value, hash_range(v)::bit(32) as standard,
       hash_range_extended(v, 0)::bit(32) as extended0,
       hash_range_extended(v, 1)::bit(32) as extended1
FROM   (VALUES (int4range(10, 20)), (int4range(23, 43)),
        (int4range(5675, 550273)),
        (int4range(550274, 1550274)), (int4range(1550275, 208112489))) x(v)
WHERE  hash_range(v)::bit(32) != hash_range_extended(v, 0)::bit(32)
       OR hash_range(v)::bit(32) = hash_range_extended(v, 1)::bit(32)
================================


CREATE TYPE hash_test_t1 AS (a int, b text)
================================

DROP TYPE hash_test_t1
================================


-- record hashing with non-hashable field type
CREATE TYPE hash_test_t2 AS (a money, b text)
================================

DROP TYPE hash_test_t2
================================


================================

PREPARE TRANSACTION 'foo1'
================================

PREPARE TRANSACTION 'foo2'
================================

PREPARE TRANSACTION 'foo3'
================================


-- This should fail, because the gid foo3 is already in use
PREPARE TRANSACTION 'foo3'
================================

PREPARE TRANSACTION 'foo4'
================================

PREPARE TRANSACTION 'foo5'
================================

PREPARE TRANSACTION 'foo6'
================================

PREPARE TRANSACTION 'regress-one'
================================

  DECLARE foo CURSOR FOR SELECT * FROM pxtest4
================================

  -- Fetch 1 tuple, keeping the cursor open
  FETCH 1 FROM foo
================================

PREPARE TRANSACTION 'regress-two'
================================


-- No such cursor
FETCH 1 FROM foo
================================

lock table pxtest3 in access share mode nowait
================================


-- Disconnect, we will continue testing in a different backend
\c -

-- There should still be two prepared transactions
SELECT gid FROM pg_prepared_xacts
================================

lock table pxtest3 in access share mode nowait
================================

\d pxtest2
SELECT * FROM pxtest2
================================


================================


--
-- Test that wholerow references to ON CONFLICT's EXCLUDED work
--
create unique index plain on insertconflicttest(key)
================================


drop index plain
================================

create view insertconflictv as
  select * from insertconflict with cascaded check option
================================

insert into capitals values ('Sacramento', 4664.E+5, 30, 'CA') on conflict (name) do update set population = excluded.population
================================


================================


================================


================================


================================

create table parted_conflict_test_1 partition of parted_conflict_test (b unique) for values in (1, 2)
================================

insert into parted_conflict values (50, 'cincuenta', 2)
  on conflict (a, b) do update set (a, b, c) = row(excluded.*)
  where parted_conflict = (50, text 'cincuenta', 1) and
        excluded = (50, text 'cincuenta', 2)
================================

 return new
================================

$$ language plpgsql
================================


================================


select polyf(42) as int, polyf(4.5) as num
================================

select polyf(point(3,4))
================================


select polyf(42) as int, polyf(4.5) as num
================================


select polyf(array[2,4]) as int, polyf(array[4.5, 7.7]) as num
================================


select polyf(array[2,4]) as int, polyf(array[4.5, 7.7]) as num
================================


select polyf(int4range(42, 49)) as int, polyf(float8range(4.5, 7.8)) as num
================================


select polyf(2, 4) as int, polyf(2, 4.5) as num
================================


select polyf(int4range(42, 49), 11, 2::smallint) as int, polyf(float8range(4.5, 7.8), 7.8, 11::real) as num
================================


select polyf(int4range(42, 49), 11, 4.5) as fail
================================


select polyf(multirange(int4range(42, 49)), 11, 2::smallint) as int, polyf(multirange(float8range(4.5, 7.8)), 7.8, 11::real) as num
================================


select polyf(multirange(int4range(42, 49)), 11, 4.5) as fail
================================


select polyf(int4range(42, 49), array[11]) as int, polyf(float8range(4.5, 7.8), array[7]) as num
================================


select polyf(multirange(int4range(42, 49)), array[11]) as int, polyf(multirange(float8range(4.5, 7.8)), array[7]) as num
================================

select x, pg_typeof(x), y, pg_typeof(y)
  from polyf(11, array[1, 2], point(1,2), point(3,4))
================================

select x, pg_typeof(x), y, pg_typeof(y)
  from polyf(11, '{1,2}', point(1,2), '(3,4)')
================================


create function polyf(anyrange) returns anymultirange
as 'select multirange($1)
================================
' language sql
================================


select polyf(int4range(1,10))
================================


create function polyf(anymultirange) returns anyelement
as 'select lower($1)
================================
' language sql
================================


select polyf(int4multirange(int4range(1,10), int4range(20,30)))
================================


create function polyf(anycompatiblerange) returns anycompatiblemultirange
as 'select multirange($1)
================================
' language sql
================================


select polyf(int4range(1,10))
================================


create function polyf(anymultirange) returns anyrange
as 'select range_merge($1)
================================
' language sql
================================


select polyf(int4multirange(int4range(1,10), int4range(20,30)))
================================


create function polyf(anycompatiblemultirange) returns anycompatiblerange
as 'select range_merge($1)
================================
' language sql
================================


select polyf(int4multirange(int4range(1,10), int4range(20,30)))
================================


create function polyf(anycompatiblemultirange) returns anycompatible
as 'select lower($1)
================================
' language sql
================================


select polyf(int4multirange(int4range(1,10), int4range(20,30)))
================================


-- Try to cover all the possible states:
--
-- Note: in Cases 1 & 2, we are trying to return P. Therefore, if the transfn
-- is stfnp, tfnp, or tf2p, we must use ffp as finalfn, because stfnp, tfnp,
-- and tf2p do not return P. Conversely, in Cases 3 & 4, we are trying to
-- return N. Therefore, if the transfn is stfp, tfp, or tf1p, we must use ffnp
-- as finalfn, because stfp, tfp, and tf1p do not return N.
--
--     Case1 (R = P) && (B = A)
--     ------------------------
--     S    tf1
--     -------
--     N    N
-- should CREATE
CREATE AGGREGATE myaggp01a(*) (SFUNC = stfnp, STYPE = int4[],
  FINALFUNC = ffp, INITCOND = '{}')
================================


--     P    N
-- should ERROR: stfnp(anyarray) not matched by stfnp(int[])
CREATE AGGREGATE myaggp02a(*) (SFUNC = stfnp, STYPE = anyarray,
  FINALFUNC = ffp, INITCOND = '{}')
================================


--     N    P
-- should CREATE
CREATE AGGREGATE myaggp03a(*) (SFUNC = stfp, STYPE = int4[],
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp03b(*) (SFUNC = stfp, STYPE = int4[],
  INITCOND = '{}')
================================


--     P    P
-- should ERROR: we have no way to resolve S
CREATE AGGREGATE myaggp04a(*) (SFUNC = stfp, STYPE = anyarray,
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp04b(*) (SFUNC = stfp, STYPE = anyarray,
  INITCOND = '{}')
================================



--    Case2 (R = P) && ((B = P) || (B = N))
--    -------------------------------------
--    S    tf1      B    tf2
--    -----------------------
--    N    N        N    N
-- should CREATE
CREATE AGGREGATE myaggp05a(BASETYPE = int, SFUNC = tfnp, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================


--    N    N        N    P
-- should CREATE
CREATE AGGREGATE myaggp06a(BASETYPE = int, SFUNC = tf2p, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================


--    N    N        P    N
-- should ERROR: tfnp(int[], anyelement) not matched by tfnp(int[], int)
CREATE AGGREGATE myaggp07a(BASETYPE = anyelement, SFUNC = tfnp, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================


--    N    N        P    P
-- should CREATE
CREATE AGGREGATE myaggp08a(BASETYPE = anyelement, SFUNC = tf2p, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================


--    N    P        N    N
-- should CREATE
CREATE AGGREGATE myaggp09a(BASETYPE = int, SFUNC = tf1p, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp09b(BASETYPE = int, SFUNC = tf1p, STYPE = int[],
  INITCOND = '{}')
================================


--    N    P        N    P
-- should CREATE
CREATE AGGREGATE myaggp10a(BASETYPE = int, SFUNC = tfp, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp10b(BASETYPE = int, SFUNC = tfp, STYPE = int[],
  INITCOND = '{}')
================================


--    N    P        P    N
-- should ERROR: tf1p(int[],anyelement) not matched by tf1p(anyarray,int)
CREATE AGGREGATE myaggp11a(BASETYPE = anyelement, SFUNC = tf1p, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp11b(BASETYPE = anyelement, SFUNC = tf1p, STYPE = int[],
  INITCOND = '{}')
================================


--    N    P        P    P
-- should ERROR: tfp(int[],anyelement) not matched by tfp(anyarray,anyelement)
CREATE AGGREGATE myaggp12a(BASETYPE = anyelement, SFUNC = tfp, STYPE = int[],
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp12b(BASETYPE = anyelement, SFUNC = tfp, STYPE = int[],
  INITCOND = '{}')
================================


--    P    N        N    N
-- should ERROR: tfnp(anyarray, int) not matched by tfnp(int[],int)
CREATE AGGREGATE myaggp13a(BASETYPE = int, SFUNC = tfnp, STYPE = anyarray,
  FINALFUNC = ffp, INITCOND = '{}')
================================


--    P    N        N    P
-- should ERROR: tf2p(anyarray, int) not matched by tf2p(int[],anyelement)
CREATE AGGREGATE myaggp14a(BASETYPE = int, SFUNC = tf2p, STYPE = anyarray,
  FINALFUNC = ffp, INITCOND = '{}')
================================


--    P    N        P    N
-- should ERROR: tfnp(anyarray, anyelement) not matched by tfnp(int[],int)
CREATE AGGREGATE myaggp15a(BASETYPE = anyelement, SFUNC = tfnp,
  STYPE = anyarray, FINALFUNC = ffp, INITCOND = '{}')
================================


--    P    N        P    P
-- should ERROR: tf2p(anyarray, anyelement) not matched by tf2p(int[],anyelement)
CREATE AGGREGATE myaggp16a(BASETYPE = anyelement, SFUNC = tf2p,
  STYPE = anyarray, FINALFUNC = ffp, INITCOND = '{}')
================================


--    P    P        N    N
-- should ERROR: we have no way to resolve S
CREATE AGGREGATE myaggp17a(BASETYPE = int, SFUNC = tf1p, STYPE = anyarray,
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp17b(BASETYPE = int, SFUNC = tf1p, STYPE = anyarray,
  INITCOND = '{}')
================================


--    P    P        N    P
-- should ERROR: tfp(anyarray, int) not matched by tfp(anyarray, anyelement)
CREATE AGGREGATE myaggp18a(BASETYPE = int, SFUNC = tfp, STYPE = anyarray,
  FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp18b(BASETYPE = int, SFUNC = tfp, STYPE = anyarray,
  INITCOND = '{}')
================================


--    P    P        P    N
-- should ERROR: tf1p(anyarray, anyelement) not matched by tf1p(anyarray, int)
CREATE AGGREGATE myaggp19a(BASETYPE = anyelement, SFUNC = tf1p,
  STYPE = anyarray, FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp19b(BASETYPE = anyelement, SFUNC = tf1p,
  STYPE = anyarray, INITCOND = '{}')
================================


--    P    P        P    P
-- should CREATE
CREATE AGGREGATE myaggp20a(BASETYPE = anyelement, SFUNC = tfp,
  STYPE = anyarray, FINALFUNC = ffp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggp20b(BASETYPE = anyelement, SFUNC = tfp,
  STYPE = anyarray, INITCOND = '{}')
================================


--     Case3 (R = N) && (B = A)
--     ------------------------
--     S    tf1
--     -------
--     N    N
-- should CREATE
CREATE AGGREGATE myaggn01a(*) (SFUNC = stfnp, STYPE = int4[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn01b(*) (SFUNC = stfnp, STYPE = int4[],
  INITCOND = '{}')
================================


--     P    N
-- should ERROR: stfnp(anyarray) not matched by stfnp(int[])
CREATE AGGREGATE myaggn02a(*) (SFUNC = stfnp, STYPE = anyarray,
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn02b(*) (SFUNC = stfnp, STYPE = anyarray,
  INITCOND = '{}')
================================


--     N    P
-- should CREATE
CREATE AGGREGATE myaggn03a(*) (SFUNC = stfp, STYPE = int4[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--     P    P
-- should ERROR: ffnp(anyarray) not matched by ffnp(int[])
CREATE AGGREGATE myaggn04a(*) (SFUNC = stfp, STYPE = anyarray,
  FINALFUNC = ffnp, INITCOND = '{}')
================================



--    Case4 (R = N) && ((B = P) || (B = N))
--    -------------------------------------
--    S    tf1      B    tf2
--    -----------------------
--    N    N        N    N
-- should CREATE
CREATE AGGREGATE myaggn05a(BASETYPE = int, SFUNC = tfnp, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn05b(BASETYPE = int, SFUNC = tfnp, STYPE = int[],
  INITCOND = '{}')
================================


--    N    N        N    P
-- should CREATE
CREATE AGGREGATE myaggn06a(BASETYPE = int, SFUNC = tf2p, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn06b(BASETYPE = int, SFUNC = tf2p, STYPE = int[],
  INITCOND = '{}')
================================


--    N    N        P    N
-- should ERROR: tfnp(int[], anyelement) not matched by tfnp(int[], int)
CREATE AGGREGATE myaggn07a(BASETYPE = anyelement, SFUNC = tfnp, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn07b(BASETYPE = anyelement, SFUNC = tfnp, STYPE = int[],
  INITCOND = '{}')
================================


--    N    N        P    P
-- should CREATE
CREATE AGGREGATE myaggn08a(BASETYPE = anyelement, SFUNC = tf2p, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn08b(BASETYPE = anyelement, SFUNC = tf2p, STYPE = int[],
  INITCOND = '{}')
================================


--    N    P        N    N
-- should CREATE
CREATE AGGREGATE myaggn09a(BASETYPE = int, SFUNC = tf1p, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--    N    P        N    P
-- should CREATE
CREATE AGGREGATE myaggn10a(BASETYPE = int, SFUNC = tfp, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--    N    P        P    N
-- should ERROR: tf1p(int[],anyelement) not matched by tf1p(anyarray,int)
CREATE AGGREGATE myaggn11a(BASETYPE = anyelement, SFUNC = tf1p, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--    N    P        P    P
-- should ERROR: tfp(int[],anyelement) not matched by tfp(anyarray,anyelement)
CREATE AGGREGATE myaggn12a(BASETYPE = anyelement, SFUNC = tfp, STYPE = int[],
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--    P    N        N    N
-- should ERROR: tfnp(anyarray, int) not matched by tfnp(int[],int)
CREATE AGGREGATE myaggn13a(BASETYPE = int, SFUNC = tfnp, STYPE = anyarray,
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn13b(BASETYPE = int, SFUNC = tfnp, STYPE = anyarray,
  INITCOND = '{}')
================================


--    P    N        N    P
-- should ERROR: tf2p(anyarray, int) not matched by tf2p(int[],anyelement)
CREATE AGGREGATE myaggn14a(BASETYPE = int, SFUNC = tf2p, STYPE = anyarray,
  FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn14b(BASETYPE = int, SFUNC = tf2p, STYPE = anyarray,
  INITCOND = '{}')
================================


--    P    N        P    N
-- should ERROR: tfnp(anyarray, anyelement) not matched by tfnp(int[],int)
CREATE AGGREGATE myaggn15a(BASETYPE = anyelement, SFUNC = tfnp,
  STYPE = anyarray, FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn15b(BASETYPE = anyelement, SFUNC = tfnp,
  STYPE = anyarray, INITCOND = '{}')
================================


--    P    N        P    P
-- should ERROR: tf2p(anyarray, anyelement) not matched by tf2p(int[],anyelement)
CREATE AGGREGATE myaggn16a(BASETYPE = anyelement, SFUNC = tf2p,
  STYPE = anyarray, FINALFUNC = ffnp, INITCOND = '{}')
================================

CREATE AGGREGATE myaggn16b(BASETYPE = anyelement, SFUNC = tf2p,
  STYPE = anyarray, INITCOND = '{}')
================================


--    P    P        N    N
-- should ERROR: ffnp(anyarray) not matched by ffnp(int[])
CREATE AGGREGATE myaggn17a(BASETYPE = int, SFUNC = tf1p, STYPE = anyarray,
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--    P    P        N    P
-- should ERROR: tfp(anyarray, int) not matched by tfp(anyarray, anyelement)
CREATE AGGREGATE myaggn18a(BASETYPE = int, SFUNC = tfp, STYPE = anyarray,
  FINALFUNC = ffnp, INITCOND = '{}')
================================


--    P    P        P    N
-- should ERROR: tf1p(anyarray, anyelement) not matched by tf1p(anyarray, int)
CREATE AGGREGATE myaggn19a(BASETYPE = anyelement, SFUNC = tf1p,
  STYPE = anyarray, FINALFUNC = ffnp, INITCOND = '{}')
================================


--    P    P        P    P
-- should ERROR: ffnp(anyarray) not matched by ffnp(int[])
CREATE AGGREGATE myaggn20a(BASETYPE = anyelement, SFUNC = tfp,
  STYPE = anyarray, FINALFUNC = ffnp, INITCOND = '{}')
================================


-- multi-arg polymorphic
CREATE AGGREGATE mysum2(anyelement,anyelement) (SFUNC = sum3,
  STYPE = anyelement, INITCOND = '0')
================================

  return $1
================================

end$$ language plpgsql
================================


-- another sort of polymorphic aggregate

CREATE AGGREGATE array_larger_accum (anyarray)
(
    sfunc = array_larger,
    stype = anyarray,
    initcond = '{}'
)
================================

  if array_upper(grp, 1) < size then
    return grp || ad
================================

  return grp
================================

$$
  language plpgsql immutable
================================


create aggregate build_group(anyelement, integer) (
  SFUNC = add_group,
  STYPE = anyarray
)
================================


-- this should fail because stype isn't compatible with arg
create aggregate build_group(int8, integer) (
  SFUNC = add_group,
  STYPE = int2[]
)
================================


-- but we can make a non-poly agg from a poly sfunc if types are OK
create aggregate build_group(int8, integer) (
  SFUNC = add_group,
  STYPE = int8[]
)
================================


create aggregate first_el_agg_f8(float8) (
  SFUNC = array_append,
  STYPE = float8[],
  FINALFUNC = first_el
)
================================


create aggregate first_el_agg_any(anyelement) (
  SFUNC = first_el_transfn,
  STYPE = anyarray,
  FINALFUNC = first_el
)
================================

$$ language sql immutable strict
================================

$$ language sql immutable strict
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================


-- verify it lists properly
\df dfunc

drop function dfunc(int, int)
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================


\df dfunc

drop function dfunc(a variadic int[])
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================

$$ language sql
================================


select (dfunc(10,20,30)).*
================================

select (dfunc(a := 10, b := 20, c := 30)).*
================================

$$ language sql
================================


select (dfunc('Hello World', 20, '2009-07-25'::date)).*
================================

$$ language sql
================================


select (dfunc()).*
================================

$$ language sql
================================

$$ language sql
================================
$$ language sql
================================
$$ language sql
================================
$$ language sql
================================
$$ language sql
================================
$$ language sql
================================
$$ language sql
================================

$$ language sql
================================
 -- mixed notation

-- ansi/sql syntax
select dfunc(a => 1, b => 2)
================================

select dfunc(a => 'a'::text, b => 'b')
================================

select dfunc(a => 'a'::text, b => 'b', flag => false)
================================
 -- named notation

select dfunc(b => 'b'::text, a => 'a')
================================
 -- named notation with default
select dfunc(a => 'a'::text, flag => true)
================================
 -- named notation with default
select dfunc(a => 'a'::text, flag => false)
================================
 -- named notation with default
select dfunc(b => 'b'::text, a => 'a', flag => true)
================================
 -- full positional notation
select dfunc('a'::text, 'b', flag => false)
================================
 -- full positional notation
select dfunc('a'::text, 'b', flag => true)
================================
 -- mixed notation

-- this tests lexer edge cases around =>
select dfunc(a =>-1)
================================

select dfunc(a =>+1)
================================

select dfunc(a =>/**/1)
================================

select dfunc(a =>--comment to be removed by psql
  1)
================================

-- need DO to protect the -- from psql
do $$
  declare r integer
================================

    raise info 'r = %', r
================================

$$
================================


\d+ dfview

drop view dfview
================================

select x, pg_typeof(x) from anyctest(11, point(1,2)) x
================================

select x, pg_typeof(x) from anyctest(11, array[point(1,2)]) x
================================


select x, pg_typeof(x) from anyctest(11, int4range(4,7)) x
================================

select x, pg_typeof(x) from anyctest(11, numrange(4,7)) x
================================
  -- fail
select x, pg_typeof(x) from anyctest(11.2, int4range(4,7)) x
================================


select x, pg_typeof(x) from anyctest(int4range(11,12), int4range(4,7)) x
================================

select x, pg_typeof(x) from anyctest(int4range(11,12), numrange(4,7)) x
================================


select x, pg_typeof(x) from anyctest(11, multirange(int4range(4,7))) x
================================

select x, pg_typeof(x) from anyctest(11, multirange(numrange(4,7))) x
================================
  -- fail
select x, pg_typeof(x) from anyctest(11.2, multirange(int4range(4,7))) x
================================


select x, pg_typeof(x) from anyctest(multirange(int4range(11,12)), multirange(int4range(4,7))) x
================================

select x, pg_typeof(x) from anyctest(multirange(int4range(11,12)), multirange(numrange(4,7))) x
================================

select x, pg_typeof(x) from anyctest(11, array[1, 2], point(1,2), point(3,4)) x
================================

select x, pg_typeof(x) from anyctest(11, '{1,2}', point(1,2), '(3,4)') x
================================


================================

SELECT pg_current_xact_id() \gset
SELECT pg_current_xact_id_if_assigned() IS NOT DISTINCT FROM xid8 :'pg_current_xact_id'
================================

SELECT pg_xact_status(:rolledback::text::xid8) AS rolledback
================================

SELECT pg_xact_status(:inprogress::text::xid8) AS inprogress
================================

  RAISE EXCEPTION 'didn''t ERROR at xid in the future as expected'
================================

EXCEPTION
  WHEN invalid_parameter_value THEN
    RAISE NOTICE 'Got expected error for xid in the future'
================================

$$
================================

SELECT test_future_xid_status((:inprogress + 10000)::text::xid8)
================================


================================


SELECT * FROM INTERVAL_TBL
   WHERE INTERVAL_TBL.f1 <> interval '@ 10 days'
================================


SELECT * FROM INTERVAL_TBL
   WHERE INTERVAL_TBL.f1 <= interval '@ 5 hours'
================================


SELECT * FROM INTERVAL_TBL
   WHERE INTERVAL_TBL.f1 < interval '@ 1 day'
================================


SELECT * FROM INTERVAL_TBL
   WHERE INTERVAL_TBL.f1 = interval '@ 34 years'
================================


SELECT * FROM INTERVAL_TBL
   WHERE INTERVAL_TBL.f1 >= interval '@ 1 month'
================================


SELECT * FROM INTERVAL_TBL
   WHERE INTERVAL_TBL.f1 > interval '@ 3 seconds ago'
================================

41 mon 12 days 360:00
-41 mon -12 days +360:00
-12 days
9 mon -27 days 12:34:56
-3 years 482 days 76:54:32.189
4 mon
14 mon
999 mon 999 days
\.

SELECT span * 0.3 AS product
FROM INTERVAL_MULDIV_TBL
================================


-- test justify_hours() and justify_days()

SELECT justify_hours(interval '6 months 3 days 52 hours 3 minutes 2 seconds') as "6 mons 5 days 4 hours 3 mins 2 seconds"
================================

SELECT justify_days(interval '6 months 36 days 5 hours 4 minutes 3 seconds') as "7 mons 6 days 5 hours 4 mins 3 seconds"
================================


-- test justify_interval()

SELECT justify_interval(interval '1 month -1 hour') as "1 month -1 hour"
================================


-- test casting to restricted precision (bug #14479)
SELECT f1, f1::INTERVAL DAY TO MINUTE AS "minutes",
  (f1 + INTERVAL '1 month')::INTERVAL MONTH::INTERVAL YEAR AS "years"
  FROM interval_tbl
================================


--
-- test EXTRACT
--
SELECT f1,
    EXTRACT(MICROSECOND FROM f1) AS MICROSECOND,
    EXTRACT(MILLISECOND FROM f1) AS MILLISECOND,
    EXTRACT(SECOND FROM f1) AS SECOND,
    EXTRACT(MINUTE FROM f1) AS MINUTE,
    EXTRACT(HOUR FROM f1) AS HOUR,
    EXTRACT(DAY FROM f1) AS DAY,
    EXTRACT(MONTH FROM f1) AS MONTH,
    EXTRACT(QUARTER FROM f1) AS QUARTER,
    EXTRACT(YEAR FROM f1) AS YEAR,
    EXTRACT(DECADE FROM f1) AS DECADE,
    EXTRACT(CENTURY FROM f1) AS CENTURY,
    EXTRACT(MILLENNIUM FROM f1) AS MILLENNIUM,
    EXTRACT(EPOCH FROM f1) AS EPOCH
    FROM INTERVAL_TBL
================================


SELECT EXTRACT(FORTNIGHT FROM INTERVAL '2 days')
================================
  -- error
SELECT EXTRACT(TIMEZONE FROM INTERVAL '2 days')
================================
  -- error

SELECT EXTRACT(DECADE FROM INTERVAL '100 y')
================================

SELECT EXTRACT(DECADE FROM INTERVAL '99 y')
================================

SELECT EXTRACT(DECADE FROM INTERVAL '-99 y')
================================

SELECT EXTRACT(DECADE FROM INTERVAL '-100 y')
================================


SELECT EXTRACT(CENTURY FROM INTERVAL '100 y')
================================

SELECT EXTRACT(CENTURY FROM INTERVAL '99 y')
================================

SELECT EXTRACT(CENTURY FROM INTERVAL '-99 y')
================================

SELECT EXTRACT(CENTURY FROM INTERVAL '-100 y')
================================


-- date_part implementation is mostly the same as extract, so only
-- test a few cases for additional coverage.
SELECT f1,
    date_part('microsecond', f1) AS microsecond,
    date_part('millisecond', f1) AS millisecond,
    date_part('second', f1) AS second,
    date_part('epoch', f1) AS epoch
    FROM INTERVAL_TBL
================================


-- internal overflow test case
SELECT extract(epoch from interval '1000000000 days')
================================


================================

DECLARE c CURSOR FOR
SELECT ctid, * FROM tidscan WHERE ctid = ANY(ARRAY['(0,1)', '(0,2)']::tid[])
================================

FETCH ALL FROM c
================================

FETCH BACKWARD 1 FROM c
================================

FETCH FIRST FROM c
================================

DECLARE c CURSOR FOR SELECT ctid, * FROM tidscan
================================

FETCH NEXT FROM c
================================
 -- skip one row
FETCH NEXT FROM c
================================

-- perform update
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
UPDATE tidscan SET id = -id WHERE CURRENT OF c RETURNING *
================================

FETCH NEXT FROM c
================================

-- perform update
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
UPDATE tidscan SET id = -id WHERE CURRENT OF c RETURNING *
================================

-- position cursor past any rows
FETCH NEXT FROM c
================================

-- should error out
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
UPDATE tidscan SET id = -id WHERE CURRENT OF c RETURNING *
================================


================================


\echo :LAST_ERROR_MESSAGE

================================

$$
================================


\df ptest1
SELECT pg_get_functiondef('ptest1'::regproc)
================================


-- show only normal functions
\dfn public.*test*1

-- show only procedures
\dfp public.*test*1

SELECT ptest1('x')
================================


\df ptest1s
SELECT pg_get_functiondef('ptest1s'::regproc)
================================

$$
================================

$$
================================

$$
================================
  -- error, not supported
$$
================================

$$
================================

CALL ptest5(10, b => 'Hello')
================================

CALL ptest5(b => 'Hello', a => 10)
================================

$$
================================

$$
================================



-- empty body
CREATE PROCEDURE ptest8(x text)
BEGIN ATOMIC
END
================================


\df ptest8
SELECT pg_get_functiondef('ptest8'::regproc)
================================

$$
================================
  -- no error
-- ... and it had better match the type of the parameter
CALL ptest9(1./0.)
================================

CALL ptest10(a => null, b => 8, c => 2)
================================

CALL ptest10(null, 7, c => 2)
================================

CALL ptest10(null, c => 4, b => 11)
================================

CALL ptest10(b => 8, c => 2, a => 0)
================================


\df ptest10

drop procedure ptest10
================================

\df ptest10
drop procedure ptest10(int, int, int)
================================

\df ptest10
drop procedure ptest10(int, int, int)
================================



-- ROUTINE syntax

ALTER ROUTINE cp_testfunc1(int) RENAME TO cp_testfunc1a
================================

ALTER ROUTINE cp_testfunc1a RENAME TO cp_testfunc1
================================


ALTER ROUTINE ptest1(text) RENAME TO ptest1a
================================

ALTER ROUTINE ptest1a RENAME TO ptest1
================================


DROP ROUTINE cp_testfunc1(int)
================================


================================

SELECT array_to_json(array_agg(q),false)
  FROM ( SELECT $$a$$ || x AS b, y AS c,
               ARRAY[ROW(x.*,ARRAY[1,2,3]),
               ROW(y.*,ARRAY[4,5,6])] AS z
         FROM generate_series(1,2) x,
              generate_series(4,5) y) q
================================


SELECT row_to_json(q)
FROM (SELECT $$a$$ || x AS b,
         y AS c,
         ARRAY[ROW(x.*,ARRAY[1,2,3]),
               ROW(y.*,ARRAY[4,5,6])] AS z
      FROM generate_series(1,2) x,
           generate_series(4,5) y) q
================================


SELECT row_to_json(q,true)
FROM (SELECT $$a$$ || x AS b,
         y AS c,
         ARRAY[ROW(x.*,ARRAY[1,2,3]),
               ROW(y.*,ARRAY[4,5,6])] AS z
      FROM generate_series(1,2) x,
           generate_series(4,5) y) q
================================


CREATE TEMP TABLE rows AS
SELECT x, 'txt' || x as y
FROM generate_series(1,3) AS x
================================


-- to_json, timestamps

select to_json(timestamp '2014-05-28 12:22:35.614298')
================================


select to_json(date '2014-05-28')
================================


select to_json(date 'Infinity')
================================

select to_json(date '-Infinity')
================================

select to_json(timestamp 'Infinity')
================================

select to_json(timestamp '-Infinity')
================================


--json_agg

SELECT json_agg(q)
  FROM ( SELECT $$a$$ || x AS b, y AS c,
               ARRAY[ROW(x.*,ARRAY[1,2,3]),
               ROW(y.*,ARRAY[4,5,6])] AS z
         FROM generate_series(1,2) x,
              generate_series(4,5) y) q
================================


-- populate_record
create type jpop as (a text, b int, c timestamp)
================================


create type j_unordered_pair as (x int, y int)
================================


CREATE TYPE jsrec AS (
	i	int,
	ia	_int4,
	ia1	int[],
	ia2	int[][],
	ia3	int[][][],
	ia1d	js_int_array_1d,
	ia2d	js_int_array_2d,
	t	text,
	ta	text[],
	c	char(10),
	ca	char(10)[],
	ts	timestamp,
	js	json,
	jsb	jsonb,
	jsa	json[],
	rec	jpop,
	reca	jpop[]
)
================================


CREATE TYPE jsrec_i_not_null AS (
	i	js_int_not_null
)
================================


create type jpop2 as (a int, b json, c int, d int)
================================


INSERT INTO jspoptest
SELECT '{
	"jsa": [1, "2", null, 4],
	"rec": {"a": "abc", "c": "01.02.2003", "x": 43.2},
	"reca": [{"a": "abc", "b": 456}, null, {"c": "01.02.2003", "x": 43.2}]
}'::json
FROM generate_series(1, 3)
================================


SELECT (json_populate_record(NULL::jsrec, js)).* FROM jspoptest
================================


DROP TYPE jsrec
================================

DROP TYPE jsrec_i_not_null
================================

DROP TYPE j_unordered_pair
================================


select * from json_to_record('{"out": {"key": 1}}') as x(out json)
================================

select * from json_to_record('{"out": [{"key": 1}]}') as x(out json)
================================

select * from json_to_record('{"out": "{\"key\": 1}"}') as x(out json)
================================

select * from json_to_record('{"out": {"key": 1}}') as x(out jsonb)
================================

select * from json_to_record('{"out": [{"key": 1}]}') as x(out jsonb)
================================

select * from json_to_record('{"out": "{\"key\": 1}"}') as x(out jsonb)
================================


-- ts_headline for json
select ts_headline('{"a": "aaa bbb", "b": {"c": "ccc ddd fff", "c1": "ccc1 ddd1"}, "d": ["ggg hhh", "iii jjj"]}'::json, tsquery('bbb & ddd & hhh'))
================================

select ts_headline('english', '{"a": "aaa bbb", "b": {"c": "ccc ddd fff"}, "d": ["ggg hhh", "iii jjj"]}'::json, tsquery('bbb & ddd & hhh'))
================================

select ts_headline('{"a": "aaa bbb", "b": {"c": "ccc ddd fff", "c1": "ccc1 ddd1"}, "d": ["ggg hhh", "iii jjj"]}'::json, tsquery('bbb & ddd & hhh'), 'StartSel = <, StopSel = >')
================================

select ts_headline('english', '{"a": "aaa bbb", "b": {"c": "ccc ddd fff", "c1": "ccc1 ddd1"}, "d": ["ggg hhh", "iii jjj"]}'::json, tsquery('bbb & ddd & hhh'), 'StartSel = <, StopSel = >')
================================


-- corner cases for ts_headline with json
select ts_headline('null'::json, tsquery('aaa & bbb'))
================================

select ts_headline('{}'::json, tsquery('aaa & bbb'))
================================

select ts_headline('[]'::json, tsquery('aaa & bbb'))
================================


================================


CREATE FUNCTION vol(text) returns text as
  'begin return $1
================================


CREATE FUNCTION volfoo(text) returns foodomain as
  'begin return $1::foodomain
================================


CREATE OPERATOR = (procedure = inline_eq,
                   leftarg = foodomain, rightarg = foodomain)
================================


CREATE FUNCTION make_ad(int,int) returns arrdomain as
  'declare x arrdomain
================================

     return x
================================


CREATE FUNCTION ad_eq(arrdomain, arrdomain) returns boolean as
  'begin return array_eq($1, $2)
================================


CREATE OPERATOR = (procedure = ad_eq,
                   leftarg = arrdomain, rightarg = arrdomain)
================================


CREATE TYPE casetestenum AS ENUM ('e', 'f', 'g')
================================


================================


INSERT INTO brintest_bloom SELECT
	repeat(stringu1, 8)::bytea,
	substr(stringu1, 1, 1)::"char",
	stringu1::name, 142857 * tenthous,
	thousand,
	twothousand,
	repeat(stringu1, 8),
	unique1::oid,
	(four + 1.0)/(hundred+1),
	odd::float8 / (tenthous + 1),
	format('%s:00:%s:00:%s:00', to_hex(odd), to_hex(even), to_hex(hundred))::macaddr,
	inet '10.2.3.4/24' + tenthous,
	cidr '10.2.3/24' + tenthous,
	substr(stringu1, 1, 1)::bpchar,
	date '1995-08-15' + tenthous,
	time '01:20:30' + thousand * interval '18.5 second',
	timestamp '1942-07-23 03:05:09' + tenthous * interval '36.38 hours',
	timestamptz '1972-10-10 03:00' + thousand * interval '1 hour',
	justify_days(justify_hours(tenthous * interval '12 minutes')),
	timetz '01:30:20+02' + hundred * interval '15 seconds',
	tenthous::numeric(36,30) * fivethous * even / (hundred + 1),
	format('%s%s-%s-%s-%s-%s%s%s', to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'))::uuid,
	format('%s/%s%s', odd, even, tenthous)::pg_lsn
FROM tenk1 ORDER BY unique2 LIMIT 100
================================


-- throw in some NULL's and different values
INSERT INTO brintest_bloom (inetcol, cidrcol) SELECT
	inet 'fe80::6e40:8ff:fea9:8c46' + tenthous,
	cidr 'fe80::6e40:8ff:fea9:8c46' + tenthous
FROM tenk1 ORDER BY thousand, tenthous LIMIT 25
================================


-- test bloom specific index options
-- ndistinct must be >= -1.0
CREATE INDEX brinidx_bloom ON brintest_bloom USING brin (
	byteacol bytea_bloom_ops(n_distinct_per_range = -1.1)
)
================================


DO $x$
DECLARE
	r record
================================

	r2 record
================================

	cond text
================================

	idx_ctids tid[]
================================

	ss_ctids tid[]
================================

	count int
================================

	plan_ok bool
================================

	plan_line text
================================

		ELSE
			cond := format('%I %s %L::%s', r.colname, r.oper, r.value, r.typ)
================================


		plan_ok := false
================================

		FOR plan_line IN EXECUTE format($y$EXPLAIN SELECT array_agg(ctid) FROM brintest_bloom WHERE %s $y$, cond) LOOP
			IF plan_line LIKE '%Bitmap Heap Scan on brintest_bloom%' THEN
				plan_ok := true
================================

		IF NOT plan_ok THEN
			RAISE WARNING 'did not get bitmap indexscan plan for %', r
================================


		EXECUTE format($y$SELECT array_agg(ctid) FROM brintest_bloom WHERE %s $y$, cond)
			INTO idx_ctids
================================


		plan_ok := false
================================

		FOR plan_line IN EXECUTE format($y$EXPLAIN SELECT array_agg(ctid) FROM brintest_bloom WHERE %s $y$, cond) LOOP
			IF plan_line LIKE '%Seq Scan on brintest_bloom%' THEN
				plan_ok := true
================================

		IF NOT plan_ok THEN
			RAISE WARNING 'did not get seqscan plan for %', r
================================


		EXECUTE format($y$SELECT array_agg(ctid) FROM brintest_bloom WHERE %s $y$, cond)
			INTO ss_ctids
================================


		-- make sure both return the same results
		count := array_length(idx_ctids, 1)
================================


		IF NOT (count = array_length(ss_ctids, 1) AND
				idx_ctids @> ss_ctids AND
				idx_ctids <@ ss_ctids) THEN
			-- report the results of each scan to make the differences obvious
			RAISE WARNING 'something not right in %: count %', r, count
================================

			FOR r2 IN EXECUTE 'SELECT ' || r.colname || ' FROM brintest_bloom WHERE ' || cond LOOP
				RAISE NOTICE 'seqscan: %', r2
================================

			FOR r2 IN EXECUTE 'SELECT ' || r.colname || ' FROM brintest_bloom WHERE ' || cond LOOP
				RAISE NOTICE 'bitmapscan: %', r2
================================


		-- make sure we found expected number of matches
		IF count != r.matches THEN RAISE WARNING 'unexpected number of results % for %', count, r
================================

$x$
================================


INSERT INTO brintest_bloom SELECT
	repeat(stringu1, 42)::bytea,
	substr(stringu1, 1, 1)::"char",
	stringu1::name, 142857 * tenthous,
	thousand,
	twothousand,
	repeat(stringu1, 42),
	unique1::oid,
	(four + 1.0)/(hundred+1),
	odd::float8 / (tenthous + 1),
	format('%s:00:%s:00:%s:00', to_hex(odd), to_hex(even), to_hex(hundred))::macaddr,
	inet '10.2.3.4' + tenthous,
	cidr '10.2.3/24' + tenthous,
	substr(stringu1, 1, 1)::bpchar,
	date '1995-08-15' + tenthous,
	time '01:20:30' + thousand * interval '18.5 second',
	timestamp '1942-07-23 03:05:09' + tenthous * interval '36.38 hours',
	timestamptz '1972-10-10 03:00' + thousand * interval '1 hour',
	justify_days(justify_hours(tenthous * interval '12 minutes')),
	timetz '01:30:20' + hundred * interval '15 seconds',
	tenthous::numeric(36,30) * fivethous * even / (hundred + 1),
	format('%s%s-%s-%s-%s-%s%s%s', to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'))::uuid,
	format('%s/%s%s', odd, even, tenthous)::pg_lsn
FROM tenk1 ORDER BY unique2 LIMIT 5 OFFSET 5
================================

-- Fill a few pages
DO $$
DECLARE curtid tid
================================

    EXIT WHEN curtid > tid '(2, 0)'
================================

$$
================================

INSERT INTO brin_test_bloom SELECT x/100,x%100 FROM generate_series(1,10000) x(x)
================================


================================

INSERT INTO prt1 SELECT i, i % 25, to_char(i, 'FM0000') FROM generate_series(0, 599) i WHERE i % 2 = 0
================================

INSERT INTO prt2 SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(0, 599) i WHERE i % 3 = 0
================================


-- left outer join, with whole-row reference
================================
 partitionwise join does not apply
EXPLAIN (COSTS OFF)
SELECT t1, t2 FROM prt1 t1 LEFT JOIN prt2 t2 ON t1.a = t2.b WHERE t1.b = 0 ORDER BY t1.a, t2.b
================================

INSERT INTO prt1_e SELECT i, i, i % 25 FROM generate_series(0, 599, 2) i
================================

INSERT INTO prt2_e SELECT i, i, i % 25 FROM generate_series(0, 599, 3) i
================================


-- Cases with non-nullable expressions in subquery results
================================


-- merge join when expression with whole-row reference needs to be sorted
================================

INSERT INTO prt1_m SELECT i, i, i % 25 FROM generate_series(0, 599, 2) i
================================

INSERT INTO prt2_m SELECT i, i, i % 25 FROM generate_series(0, 599, 3) i
================================

INSERT INTO plt1 SELECT i, i, to_char(i/50, 'FM0000') FROM generate_series(0, 599, 2) i
================================

INSERT INTO plt2 SELECT i, i, to_char(i/50, 'FM0000') FROM generate_series(0, 599, 3) i
================================

INSERT INTO plt1_e SELECT i, i, 'A' || to_char(i/50, 'FM0000') FROM generate_series(0, 599, 2) i
================================

INSERT INTO pht1 SELECT i, i, to_char(i/50, 'FM0000') FROM generate_series(0, 599, 2) i
================================

INSERT INTO pht2 SELECT i, i, to_char(i/50, 'FM0000') FROM generate_series(0, 599, 3) i
================================

INSERT INTO pht1_e SELECT i, i, 'A' || to_char(i/50, 'FM0000') FROM generate_series(0, 299, 2) i
================================

INSERT INTO prt1_l SELECT i, i % 25, to_char(i % 4, 'FM0000') FROM generate_series(0, 599, 2) i
================================

INSERT INTO prt2_l SELECT i % 25, i, to_char(i % 4, 'FM0000') FROM generate_series(0, 599, 3) i
================================

INSERT INTO prt1_n SELECT i, i, to_char(i, 'FM0000') FROM generate_series(0, 499, 2) i
================================

INSERT INTO prt2_n SELECT i, i, to_char(i/50, 'FM0000') FROM generate_series(0, 599, 2) i
================================

INSERT INTO prt2_n SELECT i, i, to_char(i/50, 'FM0000') FROM generate_series(0, 599, 2) i
================================

INSERT INTO prt4_n SELECT i, i, to_char(i, 'FM0000') FROM generate_series(0, 599, 2) i
================================

insert into prtx1 select 1 + i%30, i, i
  from generate_series(1,1000) i
================================

insert into prtx2 select 1 + i%30, i, i
  from generate_series(1,500) i, generate_series(1,10) j
================================

INSERT INTO prt1_adv SELECT i, i % 25, to_char(i, 'FM0000') FROM generate_series(100, 399) i
================================

INSERT INTO prt2_adv_p1 SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(100, 149) i
================================

INSERT INTO prt2_adv_p2 SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(200, 299) i
================================

INSERT INTO prt2_adv_p3 SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(350, 499) i
================================

INSERT INTO prt2_adv SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(500, 599) i
================================


-- left join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.b, t1.c, t2.a, t2.c FROM prt2_adv t1 LEFT JOIN prt1_adv t2 ON (t1.b = t2.a) WHERE t1.a = 0 ORDER BY t1.b, t2.a
================================


-- anti join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.* FROM prt2_adv t1 WHERE NOT EXISTS (SELECT 1 FROM prt1_adv t2 WHERE t1.b = t2.a) AND t1.a = 0 ORDER BY t1.b
================================


-- full join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.a, t1.c, t2.b, t2.c FROM (SELECT 175 phv, * FROM prt1_adv WHERE prt1_adv.b = 0) t1 FULL JOIN (SELECT 425 phv, * FROM prt2_adv WHERE prt2_adv.a = 0) t2 ON (t1.a = t2.b) WHERE t1.phv = t1.a OR t2.phv = t2.b ORDER BY t1.a, t2.b
================================


-- Test cases where a partition on one side matches multiple partitions on
-- the other side
================================
 we currently can't do partitioned join in such cases
ALTER TABLE prt2_adv DETACH PARTITION prt2_adv_p3
================================

INSERT INTO prt2_adv SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(350, 499) i
================================

INSERT INTO prt3_adv SELECT i, i % 25, to_char(i, 'FM0000') FROM generate_series(200, 399) i
================================

INSERT INTO prt1_adv SELECT i, i % 25, to_char(i, 'FM0000') FROM generate_series(100, 399) i
================================

INSERT INTO prt2_adv SELECT i % 25, i, to_char(i, 'FM0000') FROM generate_series(100, 399) i
================================

INSERT INTO plt1_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (1, 3, 4, 6, 8, 9)
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================


-- left join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.a, t1.c, t2.a, t2.c FROM plt2_adv t1 LEFT JOIN plt1_adv t2 ON (t1.a = t2.a AND t1.c = t2.c) WHERE t1.b < 10 ORDER BY t1.a
================================


-- anti join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.* FROM plt2_adv t1 WHERE NOT EXISTS (SELECT 1 FROM plt1_adv t2 WHERE t1.a = t2.a AND t1.c = t2.c) AND t1.b < 10 ORDER BY t1.a
================================


-- full join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.a, t1.c, t2.a, t2.c FROM plt1_adv t1 FULL JOIN plt2_adv t2 ON (t1.a = t2.a AND t1.c = t2.c) WHERE coalesce(t1.b, 0) < 10 AND coalesce(t2.b, 0) < 10 ORDER BY t1.a, t2.a
================================


-- Test cases where a partition on one side matches multiple partitions on
-- the other side
================================
 we currently can't do partitioned join in such cases
ALTER TABLE plt2_adv DETACH PARTITION plt2_adv_p2
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (4, 6)
================================

INSERT INTO plt1_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (1, 3)
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (7, 9)
================================


-- left join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.a, t1.c, t2.a, t2.c FROM plt1_adv t1 LEFT JOIN plt2_adv t2 ON (t1.a = t2.a AND t1.c = t2.c) WHERE t1.b < 10 ORDER BY t1.a
================================


-- full join
================================
 currently we can't do partitioned join if there are no matched
-- partitions on the nullable side
EXPLAIN (COSTS OFF)
SELECT t1.a, t1.c, t2.a, t2.c FROM plt1_adv t1 FULL JOIN plt2_adv t2 ON (t1.a = t2.a AND t1.c = t2.c) WHERE coalesce(t1.b, 0) < 10 AND coalesce(t2.b, 0) < 10 ORDER BY t1.a, t2.a
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (4, 5, 6)
================================

INSERT INTO plt3_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (4, 6, 7, 9)
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (1, 3)
================================

INSERT INTO plt1_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (1, 2, 3, 4, 5)
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 10, 'FM0000') FROM generate_series(1, 299) i WHERE i % 10 IN (1, 2, 3, 4, 5)
================================

INSERT INTO plt1_adv SELECT i, i, to_char(i % 5, 'FM0000') FROM generate_series(0, 24) i
================================

INSERT INTO plt2_adv SELECT i, i, to_char(i % 5, 'FM0000') FROM generate_series(0, 24) i WHERE i % 5 IN (2, 3, 4)
================================

INSERT INTO plt3_adv SELECT i, i, to_char(i % 5, 'FM0000') FROM generate_series(0, 24) i WHERE i % 5 IN (1, 3, 4)
================================

INSERT INTO alpha_neg SELECT -1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(100, 399) i WHERE i % 10 IN (1, 3, 4, 6, 8, 9)
================================

INSERT INTO alpha_pos SELECT  1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(100, 399) i WHERE i % 10 IN (1, 3, 4, 6, 8, 9)
================================

INSERT INTO beta_neg SELECT -1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(100, 149) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================

INSERT INTO beta_neg SELECT -1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(200, 299) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================

INSERT INTO beta_neg SELECT -1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(350, 499) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================

INSERT INTO beta_pos SELECT  1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(100, 149) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================

INSERT INTO beta_pos SELECT  1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(200, 299) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================

INSERT INTO beta_pos SELECT  1.0, i, to_char(i % 10, 'FM0000') FROM generate_series(350, 499) i WHERE i % 10 IN (2, 3, 4, 6, 7, 9)
================================


================================
--
-- Test the LOCK statement
--

-- Setup
CREATE SCHEMA lock_schema1
================================


-- Try all valid lock options
================================
 also try omitting the optional TABLE keyword.
BEGIN TRANSACTION
================================

LOCK TABLE lock_tbl1 IN ACCESS SHARE MODE
================================

LOCK lock_tbl1 IN ROW SHARE MODE
================================

LOCK TABLE lock_tbl1 IN ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl1 IN SHARE UPDATE EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl1 IN SHARE MODE
================================

LOCK lock_tbl1 IN SHARE ROW EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl1 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl1 IN ACCESS EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl1 IN ACCESS SHARE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN ROW SHARE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN ROW EXCLUSIVE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN SHARE UPDATE EXCLUSIVE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN SHARE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN SHARE ROW EXCLUSIVE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN EXCLUSIVE MODE NOWAIT
================================

LOCK TABLE lock_tbl1 IN ACCESS EXCLUSIVE MODE NOWAIT
================================

LOCK TABLE lock_view1 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view2 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view3 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view4 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view5 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view6 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view2 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_view7 IN EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl1 * IN ACCESS EXCLUSIVE MODE
================================

LOCK TABLE lock_tbl2
================================

LOCK TABLE lock_tbl1 * IN ACCESS EXCLUSIVE MODE
================================

LOCK TABLE ONLY lock_tbl1
================================

DROP SCHEMA lock_schema1 CASCADE
================================


================================
--Test text search dictionaries and configurations

-- Test ISpell dictionary with ispell affix file
CREATE TEXT SEARCH DICTIONARY ispell (
                        Template=ispell,
                        DictFile=ispell_sample,
                        AffFile=ispell_sample
)
================================


-- Test ISpell dictionary with hunspell affix file
CREATE TEXT SEARCH DICTIONARY hunspell (
                        Template=ispell,
                        DictFile=ispell_sample,
                        AffFile=hunspell_sample
)
================================


-- Test ISpell dictionary with hunspell affix file with FLAG long parameter
CREATE TEXT SEARCH DICTIONARY hunspell_long (
                        Template=ispell,
                        DictFile=hunspell_sample_long,
                        AffFile=hunspell_sample_long
)
================================


-- Test ISpell dictionary with hunspell affix file with FLAG num parameter
CREATE TEXT SEARCH DICTIONARY hunspell_num (
                        Template=ispell,
                        DictFile=hunspell_sample_num,
                        AffFile=hunspell_sample_num
)
================================


-- Test suitability of affix and dict files
CREATE TEXT SEARCH DICTIONARY hunspell_err (
						Template=ispell,
						DictFile=ispell_sample,
						AffFile=hunspell_sample_long
)
================================


CREATE TEXT SEARCH DICTIONARY hunspell_err (
						Template=ispell,
						DictFile=ispell_sample,
						AffFile=hunspell_sample_num
)
================================


CREATE TEXT SEARCH DICTIONARY hunspell_invalid_1 (
						Template=ispell,
						DictFile=hunspell_sample_long,
						AffFile=ispell_sample
)
================================


CREATE TEXT SEARCH DICTIONARY hunspell_invalid_2 (
						Template=ispell,
						DictFile=hunspell_sample_long,
						AffFile=hunspell_sample_num
)
================================


CREATE TEXT SEARCH DICTIONARY hunspell_invalid_3 (
						Template=ispell,
						DictFile=hunspell_sample_num,
						AffFile=ispell_sample
)
================================


CREATE TEXT SEARCH DICTIONARY hunspell_err (
						Template=ispell,
						DictFile=hunspell_sample_num,
						AffFile=hunspell_sample_long
)
================================


-- Synonym dictionary
CREATE TEXT SEARCH DICTIONARY synonym (
						Template=synonym,
						Synonyms=synonym_sample
)
================================


ALTER TEXT SEARCH DICTIONARY synonym (CaseSensitive = 1)
================================


ALTER TEXT SEARCH DICTIONARY synonym (CaseSensitive = 2)
================================
  -- fail

ALTER TEXT SEARCH DICTIONARY synonym (CaseSensitive = off)
================================


-- Create and simple test thesaurus dictionary
-- More tests in configuration checks because ts_lexize()
-- cannot pass more than one word to thesaurus.
CREATE TEXT SEARCH DICTIONARY thesaurus (
                        Template=thesaurus,
						DictFile=thesaurus_sample,
						Dictionary=english_stem
)
================================


-- Test ispell dictionary in configuration
CREATE TEXT SEARCH CONFIGURATION ispell_tst (
						COPY=english
)
================================


ALTER TEXT SEARCH CONFIGURATION ispell_tst ALTER MAPPING FOR
	word, numword, asciiword, hword, numhword, asciihword, hword_part, hword_numpart, hword_asciipart
	WITH ispell, english_stem
================================


-- Test ispell dictionary with hunspell affix in configuration
CREATE TEXT SEARCH CONFIGURATION hunspell_tst (
						COPY=ispell_tst
)
================================


ALTER TEXT SEARCH CONFIGURATION hunspell_tst ALTER MAPPING
	REPLACE ispell WITH hunspell
================================


-- Test ispell dictionary with hunspell affix with FLAG long in configuration
ALTER TEXT SEARCH CONFIGURATION hunspell_tst ALTER MAPPING
	REPLACE hunspell WITH hunspell_long
================================


-- Test ispell dictionary with hunspell affix with FLAG num in configuration
ALTER TEXT SEARCH CONFIGURATION hunspell_tst ALTER MAPPING
	REPLACE hunspell_long WITH hunspell_num
================================


-- Test synonym dictionary in configuration
CREATE TEXT SEARCH CONFIGURATION synonym_tst (
						COPY=english
)
================================


ALTER TEXT SEARCH CONFIGURATION synonym_tst ALTER MAPPING FOR
	asciiword, hword_asciipart, asciihword
	WITH synonym, english_stem
================================


-- test thesaurus in configuration
-- see thesaurus_sample.ths to understand 'odd' resulting tsvector
CREATE TEXT SEARCH CONFIGURATION thesaurus_tst (
						COPY=synonym_tst
)
================================


ALTER TEXT SEARCH CONFIGURATION thesaurus_tst ALTER MAPPING FOR
	asciiword, hword_asciipart, asciihword
	WITH synonym, thesaurus, english_stem
================================


-- invalid: non-lowercase quoted identifiers
CREATE TEXT SEARCH DICTIONARY tsdict_case
(
	Template = ispell,
	"DictFile" = ispell_sample,
	"AffFile" = ispell_sample
)
================================


================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val + t2.val
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val + t2.val, 10)
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val - t2.val
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val - t2.val, 40)
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val * t2.val
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val * t2.val, 30)
    FROM num_data t1, num_data t2
================================

INSERT INTO num_result SELECT t1.id, t2.id, t1.val / t2.val
    FROM num_data t1, num_data t2
    WHERE t2.val != '0.0'
================================

INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val / t2.val, 80)
    FROM num_data t1, num_data t2
    WHERE t2.val != '0.0'
================================

INSERT INTO num_result SELECT id, 0, SQRT(ABS(val))
    FROM num_data
================================

INSERT INTO num_result SELECT id, 0, LN(ABS(val))
    FROM num_data
    WHERE val != '0.0'
================================

INSERT INTO num_result SELECT id, 0, LOG('10'::numeric, ABS(val))
    FROM num_data
    WHERE val != '0.0'
================================

INSERT INTO num_result SELECT id, 0, POW(numeric '10', LN(ABS(round(val,1000))))
    FROM num_data
    WHERE val != '0.0'
================================



--
-- Test code path for raising to integer powers
--

-- base less than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of POW():
--
-- for p in {-20..20}
-- do
--   b="0.084738"
--   r=$(bc -ql <<< "scale=500 
================================
 $b^$p" | head -n 1)
--   echo "($b, $p, $r),"
-- done

WITH t(b, p, bc_result) AS (VALUES
(0.084738, -20, 2744326694304960114888.7859130502035257),
(0.084738, -19, 232548755422013710215.4459407000481464),
(0.084738, -18, 19705716436950597776.2364581230406798),
(0.084738, -17, 1669822999434319754.3627249884302211),
(0.084738, -16, 141497461326065387.3451885900696001),
(0.084738, -15, 11990211877848128.7928565907453178),
(0.084738, -14, 1016026574105094.7376490817865767),
(0.084738, -13, 86096059836517.5178789078924309),
(0.084738, -12, 7295607918426.8214300228969888),
(0.084738, -11, 618215223791.6519943372802450),
(0.084738, -10, 52386321633.6570066961524534),
(0.084738, -9, 4439112122.5928274334185666),
(0.084738, -8, 376161483.0442710110530225),
(0.084738, -7, 31875171.7502054369346110),
(0.084738, -6, 2701038.3037689083149651),
(0.084738, -5, 228880.5837847697527935),
(0.084738, -4, 19394.8829087538193122),
(0.084738, -3, 1643.4835879219811409),
(0.084738, -2, 139.2655122733328379),
(0.084738, -1, 11.8010809790176780),
(0.084738, 0, 1),
(0.084738, 1, .084738),
(0.084738, 2, .007180528644),
(0.084738, 3, .0006084636362353),
(0.084738, 4, .0000515599916073),
(0.084738, 5, .0000043690905688),
(0.084738, 6, .0000003702279966),
(0.084738, 7, .0000000313723800),
(0.084738, 8, .0000000026584327),
(0.084738, 9, .0000000002252703),
(0.084738, 10, .0000000000190890),
(0.084738, 11, .0000000000016176),
(0.084738, 12, .0000000000001371),
(0.084738, 13, .0000000000000116),
(0.084738, 14, .0000000000000010),
(0.084738, 15, .0000000000000001),
(0.084738, 16, .0000000000000000),
(0.084738, 17, .0000000000000000),
(0.084738, 18, .0000000000000000),
(0.084738, 19, .0000000000000000),
(0.084738, 20, .0000000000000000))
SELECT b, p, bc_result, b^p AS power, b^p - bc_result AS diff FROM t
================================


-- base greater than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of POW():
--
-- for p in {-20..20}
-- do
--   b="37.821637"
--   r=$(bc -ql <<< "scale=500 
================================
 $b^$p" | head -n 1)
--   echo "($b, $p, $r),"
-- done

WITH t(b, p, bc_result) AS (VALUES
(37.821637, -20, .0000000000000000),
(37.821637, -19, .0000000000000000),
(37.821637, -18, .0000000000000000),
(37.821637, -17, .0000000000000000),
(37.821637, -16, .0000000000000000),
(37.821637, -15, .0000000000000000),
(37.821637, -14, .0000000000000000),
(37.821637, -13, .0000000000000000),
(37.821637, -12, .0000000000000000),
(37.821637, -11, .0000000000000000),
(37.821637, -10, .0000000000000002),
(37.821637, -9, .0000000000000063),
(37.821637, -8, .0000000000002388),
(37.821637, -7, .0000000000090327),
(37.821637, -6, .0000000003416316),
(37.821637, -5, .0000000129210673),
(37.821637, -4, .0000004886959182),
(37.821637, -3, .0000184832796213),
(37.821637, -2, .0006990678924066),
(37.821637, -1, .0264398920649574),
(37.821637, 0, 1),
(37.821637, 1, 37.821637),
(37.821637, 2, 1430.476225359769),
(37.821637, 3, 54102.9525326873775219),
(37.821637, 4, 2046262.2313195326271135),
(37.821637, 5, 77392987.3197773940323425),
(37.821637, 6, 2927129472.7542235178972258),
(37.821637, 7, 110708828370.5116321107718772),
(37.821637, 8, 4187189119324.7924539711577286),
(37.821637, 9, 158366346921451.9852944363360812),
(37.821637, 10, 5989674486279224.5007355092228730),
(37.821637, 11, 226539294168214309.7083246628376531),
(37.821637, 12, 8568086950266418559.9938312759931069),
(37.821637, 13, 324059074417413536066.1494087598581043),
(37.821637, 14, 12256444679171401239980.3109258799733927),
(37.821637, 15, 463558801566202198479885.2069857662592280),
(37.821637, 16, 17532552720991931019508170.1002855156233684),
(37.821637, 17, 663109844696719094948877928.0672523682648687),
(37.821637, 18, 25079899837245684700124994552.6717306599041850),
(37.821637, 19, 948562867640665366544581398598.1275771806665398),
(37.821637, 20, 35876200451584291931921101974730.6901038166532866))
SELECT b, p, bc_result, b^p AS power, b^p - bc_result AS diff FROM t
================================


--
-- Tests for raising to non-integer powers
--

-- base less than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of POW():
--
-- for n in {-20..20}
-- do
--   b="0.06933247"
--   p="$n.342987"
--   r=$(bc -ql <<< "scale=500 
================================
 e($p*l($b))" | head -n 1)
--   echo "($b, $p, $r),"
-- done

WITH t(b, p, bc_result) AS (VALUES
(0.06933247, -20.342987, 379149253615977128356318.39406340),
(0.06933247, -19.342987, 26287354251852125772450.59436685),
(0.06933247, -18.342987, 1822567200045909954554.65766042),
(0.06933247, -17.342987, 126363085720167050546.86216560),
(0.06933247, -16.342987, 8761064849800910427.02880469),
(0.06933247, -15.342987, 607426265866876128.15466179),
(0.06933247, -14.342987, 42114363355427213.14899924),
(0.06933247, -13.342987, 2919892833909256.59283660),
(0.06933247, -12.342987, 202443382310228.51544515),
(0.06933247, -11.342987, 14035899730722.44924025),
(0.06933247, -10.342987, 973143597003.32229028),
(0.06933247, -9.342987, 67470449244.92493259),
(0.06933247, -8.342987, 4677892898.16028054),
(0.06933247, -7.342987, 324329869.02491071),
(0.06933247, -6.342987, 22486590.914273551),
(0.06933247, -5.342987, 1559050.8899661435),
(0.06933247, -4.342987, 108092.84905705095),
(0.06933247, -3.342987, 7494.3442144625131),
(0.06933247, -2.342987, 519.60139541889576),
(0.06933247, -1.342987, 36.025248159838727),
(0.06933247, 0.342987, .40036522320023350),
(0.06933247, 1.342987, .02775830982657349),
(0.06933247, 2.342987, .001924552183301612),
(0.06933247, 3.342987, .0001334339565121935),
(0.06933247, 4.342987, .000009251305786862961),
(0.06933247, 5.342987, .0000006414158809285026),
(0.06933247, 6.342987, .00000004447094732199898),
(0.06933247, 7.342987, .000000003083280621074075),
(0.06933247, 8.342987, .0000000002137714611621997),
(0.06933247, 9.342987, .00000000001482130341788437),
(0.06933247, 10.342987, .000000000001027597574581366),
(0.06933247, 11.342987, .00000000000007124587801173530),
(0.06933247, 12.342987, .000000000000004939652699872298),
(0.06933247, 13.342987, .0000000000000003424783226243151),
(0.06933247, 14.342987, .00000000000000002374486802900065),
(0.06933247, 15.342987, .000000000000000001646290350274646),
(0.06933247, 16.342987, .0000000000000000001141413763217064),
(0.06933247, 17.342987, .000000000000000000007913703549583420),
(0.06933247, 18.342987, .0000000000000000000005486766139403860),
(0.06933247, 19.342987, .00000000000000000000003804110487572339),
(0.06933247, 20.342987, .000000000000000000000002637483762562946))
SELECT b, p, bc_result, b^p AS power, b^p - bc_result AS diff FROM t
================================


-- base greater than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of POW():
--
-- for n in {-20..20}
-- do
--   b="27.234987"
--   p="$n.230957"
--   r=$(bc -ql <<< "scale=500 
================================
 e($p*l($b))" | head -n 1)
--   echo "($b, $p, $r),"
-- done

WITH t(b, p, bc_result) AS (VALUES
(27.234987, -20.230957, .000000000000000000000000000009247064512095633),
(27.234987, -19.230957, .0000000000000000000000000002518436817750859),
(27.234987, -18.230957, .000000000000000000000000006858959399176602),
(27.234987, -17.230957, .0000000000000000000000001868036700701026),
(27.234987, -16.230957, .000000000000000000000005087595525911532),
(27.234987, -15.230957, .0000000000000000000001385605980094587),
(27.234987, -14.230957, .000000000000000000003773696085499835),
(27.234987, -13.230957, .0000000000000000001027765638305389),
(27.234987, -12.230957, .000000000000000002799118379829397),
(27.234987, -11.230957, .00000000000000007623395268611469),
(27.234987, -10.230957, .000000000000002076230710364949),
(27.234987, -9.230957, .00000000000005654611640579014),
(27.234987, -8.230957, .000000000001540032745212181),
(27.234987, -7.230957, .00000000004194277179542807),
(27.234987, -6.230957, .000000001142310844592450),
(27.234987, -5.230957, .00000003111082100243440),
(27.234987, -4.230957, .0000008473028055606278),
(27.234987, -3.230957, .00002307628089450723),
(27.234987, -2.230957, .0006284822101702527),
(27.234987, -1.230957, .01711670482371810),
(27.234987, 0.230957, 2.1451253063142300),
(27.234987, 1.230957, 58.422459830839071),
(27.234987, 2.230957, 1591.1349340009243),
(27.234987, 3.230957, 43334.539242761031),
(27.234987, 4.230957, 1180215.6129275865),
(27.234987, 5.230957, 32143156.875279851),
(27.234987, 6.230957, 875418459.63720737),
(27.234987, 7.230957, 23842010367.779367),
(27.234987, 8.230957, 649336842420.336290),
(27.234987, 9.230957, 17684680461938.907402),
(27.234987, 10.230957, 481642042480060.137900),
(27.234987, 11.230957, 13117514765597885.614921),
(27.234987, 12.230957, 357255344113366461.949871),
(27.234987, 13.230957, 9729844652608062117.440722),
(27.234987, 14.230957, 264992192625800087863.690528),
(27.234987, 15.230957, 7217058921265161257566.469315),
(27.234987, 16.230957, 196556505898890690402726.443417),
(27.234987, 17.230957, 5353213882921711267539279.451015),
(27.234987, 18.230957, 145794710509592328389185797.837767),
(27.234987, 19.230957, 3970717045397510438979206144.696206),
(27.234987, 20.230957, 108142427112079606637962972621.121293))
SELECT b, p, bc_result, b^p AS power, b^p - bc_result AS diff FROM t
================================


--
-- Tests for EXP()
--

-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of EXP():
--
-- for n in {-20..20}
-- do
--   x="$n.29837"
--   r=$(bc -ql <<< "scale=500 
================================
 e($x)" | head -n 1)
--   echo "($x, $r),"
-- done

WITH t(x, bc_result) AS (VALUES
(-20.29837, .000000001529431101152222),
(-19.29837, .000000004157424770142192),
(-18.29837, .00000001130105220586304),
(-17.29837, .00000003071944485366452),
(-16.29837, .00000008350410872606600),
(-15.29837, .0000002269877013517336),
(-14.29837, .0000006170165438681061),
(-13.29837, .000001677224859055276),
(-12.29837, .000004559169856609741),
(-11.29837, .00001239310857408049),
(-10.29837, .00003368796183504298),
(-9.29837, .00009157337449401917),
(-8.29837, .0002489222398577673),
(-7.29837, .0006766408013046928),
(-6.29837, .001839300394580514),
(-5.29837, .004999736839665763),
(-4.29837, .01359069379834070),
(-3.29837, .03694333598818056),
(-2.29837, .1004223988993283),
(-1.29837, .2729763820983097),
(0.29837, 1.3476603299656679),
(1.29837, 3.6633205858807959),
(2.29837, 9.9579377804197108),
(3.29837, 27.068481317440698),
(4.29837, 73.579760889182206),
(5.29837, 200.01052696742555),
(6.29837, 543.68498095607070),
(7.29837, 1477.8890041389891),
(8.29837, 4017.3188244304487),
(9.29837, 10920.204759575742),
(10.29837, 29684.194161006717),
(11.29837, 80690.005580314652),
(12.29837, 219338.17590722828),
(13.29837, 596222.97785597218),
(14.29837, 1620702.0864156289),
(15.29837, 4405525.0308492653),
(16.29837, 11975458.636179032),
(17.29837, 32552671.598188404),
(18.29837, 88487335.673150406),
(19.29837, 240533516.60908059),
(20.29837, 653837887.33381570))
SELECT x, bc_result, exp(x), exp(x)-bc_result AS diff FROM t
================================


--
-- Tests for LN()
--

-- input very small
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..40}
-- do
--   l=$(bc -ql <<< "scale=500 
================================
 l(10^-$p)" | head -n 1)
--   echo "('1.0e-$p', $l),"
-- done

WITH t(x, bc_result) AS (VALUES
('1.0e-1', -2.3025850929940457),
('1.0e-2', -4.6051701859880914),
('1.0e-3', -6.9077552789821371),
('1.0e-4', -9.2103403719761827),
('1.0e-5', -11.512925464970228),
('1.0e-6', -13.815510557964274),
('1.0e-7', -16.118095650958320),
('1.0e-8', -18.420680743952365),
('1.0e-9', -20.723265836946411),
('1.0e-10', -23.025850929940457),
('1.0e-11', -25.328436022934503),
('1.0e-12', -27.631021115928548),
('1.0e-13', -29.933606208922594),
('1.0e-14', -32.236191301916640),
('1.0e-15', -34.5387763949106853),
('1.0e-16', -36.84136148790473094),
('1.0e-17', -39.143946580898776628),
('1.0e-18', -41.4465316738928223123),
('1.0e-19', -43.74911676688686799634),
('1.0e-20', -46.051701859880913680360),
('1.0e-21', -48.3542869528749593643778),
('1.0e-22', -50.65687204586900504839581),
('1.0e-23', -52.959457138863050732413803),
('1.0e-24', -55.2620422318570964164317949),
('1.0e-25', -57.56462732485114210044978637),
('1.0e-26', -59.867212417845187784467777822),
('1.0e-27', -62.1697975108392334684857692765),
('1.0e-28', -64.47238260383327915250376073116),
('1.0e-29', -66.774967696827324836521752185847),
('1.0e-30', -69.0775527898213705205397436405309),
('1.0e-31', -71.38013788281541620455773509521529),
('1.0e-32', -73.682722975809461888575726549899655),
('1.0e-33', -75.9853080688035075725937180045840189),
('1.0e-34', -78.28789316179755325661170945926838306),
('1.0e-35', -80.590478254791598940629700913952747266),
('1.0e-36', -82.8930633477856446246476923686371114736),
('1.0e-37', -85.19564844077969030866568382332147568124),
('1.0e-38', -87.498233533773735992683675278005839888842),
('1.0e-39', -89.8008186267677816767016667326902040964430),
('1.0e-40', -92.10340371976182736071965818737456830404406))
SELECT x, bc_result, ln(x::numeric), ln(x::numeric)-bc_result AS diff FROM t
================================


-- input very close to but smaller than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..40}
-- do
--   l=$(bc -ql <<< "scale=500 
================================
 l(1-10^-$p)" | head -n 1)
--   echo "('1.0e-$p', $l),"
-- done

WITH t(x, bc_result) AS (VALUES
('1.0e-1', -.10536051565782630),
('1.0e-2', -.010050335853501441),
('1.0e-3', -.0010005003335835335),
('1.0e-4', -.00010000500033335834),
('1.0e-5', -.000010000050000333336),
('1.0e-6', -.0000010000005000003333),
('1.0e-7', -.00000010000000500000033),
('1.0e-8', -.000000010000000050000000),
('1.0e-9', -.0000000010000000005000000),
('1.0e-10', -.00000000010000000000500000),
('1.0e-11', -.000000000010000000000050000),
('1.0e-12', -.0000000000010000000000005000),
('1.0e-13', -.00000000000010000000000000500),
('1.0e-14', -.000000000000010000000000000050),
('1.0e-15', -.0000000000000010000000000000005),
('1.0e-16', -.00000000000000010000000000000001),
('1.0e-17', -.000000000000000010000000000000000),
('1.0e-18', -.0000000000000000010000000000000000),
('1.0e-19', -.00000000000000000010000000000000000),
('1.0e-20', -.000000000000000000010000000000000000),
('1.0e-21', -.0000000000000000000010000000000000000),
('1.0e-22', -.00000000000000000000010000000000000000),
('1.0e-23', -.000000000000000000000010000000000000000),
('1.0e-24', -.0000000000000000000000010000000000000000),
('1.0e-25', -.00000000000000000000000010000000000000000),
('1.0e-26', -.000000000000000000000000010000000000000000),
('1.0e-27', -.0000000000000000000000000010000000000000000),
('1.0e-28', -.00000000000000000000000000010000000000000000),
('1.0e-29', -.000000000000000000000000000010000000000000000),
('1.0e-30', -.0000000000000000000000000000010000000000000000),
('1.0e-31', -.00000000000000000000000000000010000000000000000),
('1.0e-32', -.000000000000000000000000000000010000000000000000),
('1.0e-33', -.0000000000000000000000000000000010000000000000000),
('1.0e-34', -.00000000000000000000000000000000010000000000000000),
('1.0e-35', -.000000000000000000000000000000000010000000000000000),
('1.0e-36', -.0000000000000000000000000000000000010000000000000000),
('1.0e-37', -.00000000000000000000000000000000000010000000000000000),
('1.0e-38', -.000000000000000000000000000000000000010000000000000000),
('1.0e-39', -.0000000000000000000000000000000000000010000000000000000),
('1.0e-40', -.00000000000000000000000000000000000000010000000000000000))
SELECT '1-'||x, bc_result, ln(1.0-x::numeric), ln(1.0-x::numeric)-bc_result AS diff FROM t
================================


-- input very close to but larger than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..40}
-- do
--   l=$(bc -ql <<< "scale=500 
================================
 l(1+10^-$p)" | head -n 1)
--   echo "('1.0e-$p', $l),"
-- done

WITH t(x, bc_result) AS (VALUES
('1.0e-1', .09531017980432486),
('1.0e-2', .009950330853168083),
('1.0e-3', .0009995003330835332),
('1.0e-4', .00009999500033330834),
('1.0e-5', .000009999950000333331),
('1.0e-6', .0000009999995000003333),
('1.0e-7', .00000009999999500000033),
('1.0e-8', .000000009999999950000000),
('1.0e-9', .0000000009999999995000000),
('1.0e-10', .00000000009999999999500000),
('1.0e-11', .000000000009999999999950000),
('1.0e-12', .0000000000009999999999995000),
('1.0e-13', .00000000000009999999999999500),
('1.0e-14', .000000000000009999999999999950),
('1.0e-15', .0000000000000009999999999999995),
('1.0e-16', .00000000000000010000000000000000),
('1.0e-17', .000000000000000010000000000000000),
('1.0e-18', .0000000000000000010000000000000000),
('1.0e-19', .00000000000000000010000000000000000),
('1.0e-20', .000000000000000000010000000000000000),
('1.0e-21', .0000000000000000000010000000000000000),
('1.0e-22', .00000000000000000000010000000000000000),
('1.0e-23', .000000000000000000000010000000000000000),
('1.0e-24', .0000000000000000000000010000000000000000),
('1.0e-25', .00000000000000000000000010000000000000000),
('1.0e-26', .000000000000000000000000010000000000000000),
('1.0e-27', .0000000000000000000000000010000000000000000),
('1.0e-28', .00000000000000000000000000010000000000000000),
('1.0e-29', .000000000000000000000000000010000000000000000),
('1.0e-30', .0000000000000000000000000000010000000000000000),
('1.0e-31', .00000000000000000000000000000010000000000000000),
('1.0e-32', .000000000000000000000000000000010000000000000000),
('1.0e-33', .0000000000000000000000000000000010000000000000000),
('1.0e-34', .00000000000000000000000000000000010000000000000000),
('1.0e-35', .000000000000000000000000000000000010000000000000000),
('1.0e-36', .0000000000000000000000000000000000010000000000000000),
('1.0e-37', .00000000000000000000000000000000000010000000000000000),
('1.0e-38', .000000000000000000000000000000000000010000000000000000),
('1.0e-39', .0000000000000000000000000000000000000010000000000000000),
('1.0e-40', .00000000000000000000000000000000000000010000000000000000))
SELECT '1+'||x, bc_result, ln(1.0+x::numeric), ln(1.0+x::numeric)-bc_result AS diff FROM t
================================


-- input very large
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..40}
-- do
--   l=$(bc -ql <<< "scale=500 
================================
 l(10^$p)" | head -n 1)
--   echo "('1.0e$p', $l),"
-- done

WITH t(x, bc_result) AS (VALUES
('1.0e1', 2.3025850929940457),
('1.0e2', 4.6051701859880914),
('1.0e3', 6.9077552789821371),
('1.0e4', 9.2103403719761827),
('1.0e5', 11.512925464970228),
('1.0e6', 13.815510557964274),
('1.0e7', 16.118095650958320),
('1.0e8', 18.420680743952365),
('1.0e9', 20.723265836946411),
('1.0e10', 23.025850929940457),
('1.0e11', 25.328436022934503),
('1.0e12', 27.631021115928548),
('1.0e13', 29.933606208922594),
('1.0e14', 32.236191301916640),
('1.0e15', 34.538776394910685),
('1.0e16', 36.841361487904731),
('1.0e17', 39.143946580898777),
('1.0e18', 41.446531673892822),
('1.0e19', 43.749116766886868),
('1.0e20', 46.051701859880914),
('1.0e21', 48.354286952874959),
('1.0e22', 50.656872045869005),
('1.0e23', 52.959457138863051),
('1.0e24', 55.262042231857096),
('1.0e25', 57.564627324851142),
('1.0e26', 59.867212417845188),
('1.0e27', 62.169797510839233),
('1.0e28', 64.472382603833279),
('1.0e29', 66.774967696827325),
('1.0e30', 69.077552789821371),
('1.0e31', 71.380137882815416),
('1.0e32', 73.682722975809462),
('1.0e33', 75.985308068803508),
('1.0e34', 78.287893161797553),
('1.0e35', 80.590478254791599),
('1.0e36', 82.893063347785645),
('1.0e37', 85.195648440779690),
('1.0e38', 87.498233533773736),
('1.0e39', 89.800818626767782),
('1.0e40', 92.103403719761827))
SELECT x, bc_result, ln(x::numeric), ln(x::numeric)-bc_result AS diff FROM t
================================


-- input huge
--
-- bc(1) results computed with a scale of 1000 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..10}
-- do
--   l=$(bc -ql <<< "scale=1000 
================================
 l(10^${p}00)" | head -n 1)
--  echo "('1.0e${p}00', $l),"
-- done

WITH t(x, bc_result) AS (VALUES
('1.0e100', 230.25850929940457),
('1.0e200', 460.51701859880914),
('1.0e300', 690.77552789821371),
('1.0e400', 921.03403719761827),
('1.0e500', 1151.2925464970228),
('1.0e600', 1381.5510557964274),
('1.0e700', 1611.8095650958320),
('1.0e800', 1842.0680743952365),
('1.0e900', 2072.3265836946411),
('1.0e1000', 2302.5850929940457))
SELECT x, bc_result, ln(x::numeric), ln(x::numeric)-bc_result AS diff FROM t
================================


-- input very small, non-exact results
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..50..7}
-- do
--   for d in {9..1..3}
--   do
--     l=$(bc -ql <<< "scale=500 
================================
 l($d*10^-$p) / l(10)" | head -n 1)
--     echo "('${d}.0e-$p', $l),"
--   done
-- done

WITH t(x, bc_result) AS (VALUES
('9.0e-1', -.04575749056067513),
('6.0e-1', -.2218487496163564),
('3.0e-1', -.5228787452803376),
('9.0e-8', -7.045757490560675),
('6.0e-8', -7.221848749616356),
('3.0e-8', -7.522878745280338),
('9.0e-15', -14.0457574905606751),
('6.0e-15', -14.2218487496163564),
('3.0e-15', -14.5228787452803376),
('9.0e-22', -21.04575749056067512540994),
('6.0e-22', -21.22184874961635636749123),
('3.0e-22', -21.52287874528033756270497),
('9.0e-29', -28.045757490560675125409944193490),
('6.0e-29', -28.221848749616356367491233202020),
('3.0e-29', -28.522878745280337562704972096745),
('9.0e-36', -35.0457574905606751254099441934897693816),
('6.0e-36', -35.2218487496163563674912332020203916640),
('3.0e-36', -35.5228787452803375627049720967448846908),
('9.0e-43', -42.04575749056067512540994419348976938159974227),
('6.0e-43', -42.22184874961635636749123320202039166403168125),
('3.0e-43', -42.52287874528033756270497209674488469079987114),
('9.0e-50', -49.045757490560675125409944193489769381599742271618608),
('6.0e-50', -49.221848749616356367491233202020391664031681254347196),
('3.0e-50', -49.522878745280337562704972096744884690799871135809304))
SELECT x, bc_result, log(x::numeric), log(x::numeric)-bc_result AS diff FROM t
================================


-- input very close to but smaller than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..40..7}
-- do
--   for d in {9..1..3}
--   do
--     l=$(bc -ql <<< "scale=500 
================================
 l(1-$d*10^-$p) / l(10)" | head -n 1)
--     echo "('${d}.0e-$p', $l),"
--   done
-- done

WITH t(x, bc_result) AS (VALUES
('9.0e-1', -1.0000000000000000),
('6.0e-1', -.3979400086720376),
('3.0e-1', -.1549019599857432),
('9.0e-8', -.000000039086505130185422),
('6.0e-8', -.000000026057669695925208),
('3.0e-8', -.000000013028834652530076),
('9.0e-15', -.0000000000000039086503371292840),
('6.0e-15', -.0000000000000026057668914195188),
('3.0e-15', -.0000000000000013028834457097574),
('9.0e-22', -.00000000000000000000039086503371292664),
('6.0e-22', -.00000000000000000000026057668914195110),
('3.0e-22', -.00000000000000000000013028834457097555),
('9.0e-29', -.000000000000000000000000000039086503371292664),
('6.0e-29', -.000000000000000000000000000026057668914195110),
('3.0e-29', -.000000000000000000000000000013028834457097555),
('9.0e-36', -.0000000000000000000000000000000000039086503371292664),
('6.0e-36', -.0000000000000000000000000000000000026057668914195110),
('3.0e-36', -.0000000000000000000000000000000000013028834457097555))
SELECT '1-'||x, bc_result, log(1.0-x::numeric), log(1.0-x::numeric)-bc_result AS diff FROM t
================================


-- input very close to but larger than 1
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {1..40..7}
-- do
--   for d in {9..1..3}
--   do
--     l=$(bc -ql <<< "scale=500 
================================
 l(1+$d*10^-$p) / l(10)" | head -n 1)
--     echo "('${d}.0e-$p', $l),"
--   done
-- done

WITH t(x, bc_result) AS (VALUES
('9.0e-1', .2787536009528290),
('6.0e-1', .2041199826559248),
('3.0e-1', .1139433523068368),
('9.0e-8', .000000039086501612400118),
('6.0e-8', .000000026057668132465074),
('3.0e-8', .000000013028834261665042),
('9.0e-15', .0000000000000039086503371292489),
('6.0e-15', .0000000000000026057668914195031),
('3.0e-15', .0000000000000013028834457097535),
('9.0e-22', .00000000000000000000039086503371292664),
('6.0e-22', .00000000000000000000026057668914195110),
('3.0e-22', .00000000000000000000013028834457097555),
('9.0e-29', .000000000000000000000000000039086503371292664),
('6.0e-29', .000000000000000000000000000026057668914195110),
('3.0e-29', .000000000000000000000000000013028834457097555),
('9.0e-36', .0000000000000000000000000000000000039086503371292664),
('6.0e-36', .0000000000000000000000000000000000026057668914195110),
('3.0e-36', .0000000000000000000000000000000000013028834457097555))
SELECT '1+'||x, bc_result, log(1.0+x::numeric), log(1.0+x::numeric)-bc_result AS diff FROM t
================================


-- input very large, non-exact results
--
-- bc(1) results computed with a scale of 500 and truncated using the script
-- below, and then rounded by hand to match the precision of LN():
--
-- for p in {10..50..7}
--   do
--   for d in {2..9..3}
--   do
--     l=$(bc -ql <<< "scale=500 
================================
 l($d*10^$p) / l(10)" | head -n 1)
--     echo "('${d}.0e$p', $l),"
--   done
-- done

WITH t(x, bc_result) AS (VALUES
('2.0e10', 10.301029995663981),
('5.0e10', 10.698970004336019),
('8.0e10', 10.903089986991944),
('2.0e17', 17.301029995663981),
('5.0e17', 17.698970004336019),
('8.0e17', 17.903089986991944),
('2.0e24', 24.301029995663981),
('5.0e24', 24.698970004336019),
('8.0e24', 24.903089986991944),
('2.0e31', 31.301029995663981),
('5.0e31', 31.698970004336019),
('8.0e31', 31.903089986991944),
('2.0e38', 38.301029995663981),
('5.0e38', 38.698970004336019),
('8.0e38', 38.903089986991944),
('2.0e45', 45.30102999566398),
('5.0e45', 45.69897000433602),
('8.0e45', 45.90308998699194))
SELECT x, bc_result, log(x::numeric), log(x::numeric)-bc_result AS diff FROM t
================================


================================


-- Test comments
COMMENT ON TRIGGER check_fkeys2_pkey_bad ON fkeys2 IS 'wrong'
================================

COMMENT ON TRIGGER check_fkeys2_pkey_exist ON fkeys2 IS 'right'
================================

COMMENT ON TRIGGER check_fkeys2_pkey_exist ON fkeys2 IS NULL
================================
 return new
================================


drop trigger trigger_alpha on trigtest
================================

5	10
20	20
30	10
50	35
80	15
\.

CREATE FUNCTION trigger_func() RETURNS trigger LANGUAGE plpgsql AS '
BEGIN
	RAISE NOTICE ''trigger_func(%) called: action = %, when = %, level = %'', TG_ARGV[0], TG_OP, TG_WHEN, TG_LEVEL
================================

	RETURN NULL
================================
'
================================


INSERT INTO main_table DEFAULT VALUES
================================

30	40
50	60
\.

SELECT * FROM main_table ORDER BY a, b
================================

CREATE TRIGGER modified_any BEFORE UPDATE OF a ON main_table
FOR EACH ROW WHEN (OLD.* IS DISTINCT FROM NEW.*) EXECUTE PROCEDURE trigger_func('modified_any')
================================

123	999
456	999
\.
DELETE FROM main_table WHERE a IN (123, 456)
================================


-- Test RENAME TRIGGER
ALTER TRIGGER modified_a ON main_table RENAME TO modified_modified_a
================================


DROP TRIGGER modified_modified_a ON main_table
================================

DROP TRIGGER modified_any ON main_table
================================

DROP TRIGGER insert_a ON main_table
================================

DROP TRIGGER delete_a ON main_table
================================

DROP TRIGGER insert_when ON main_table
================================

DROP TRIGGER delete_when ON main_table
================================


-- Test column-level triggers
DROP TRIGGER after_upd_row_trig ON main_table
================================

  RETURN NEW
================================

$$ LANGUAGE plpgsql
================================

CREATE TRIGGER error_ins_a BEFORE INSERT OF a ON main_table
FOR EACH ROW EXECUTE PROCEDURE trigger_func('error_ins_a')
================================

CREATE TRIGGER error_stmt_when BEFORE UPDATE OF a ON main_table
FOR EACH STATEMENT WHEN (OLD.* IS DISTINCT FROM NEW.*)
EXECUTE PROCEDURE trigger_func('error_stmt_when')
================================

DROP TRIGGER after_upd_a_b_row_trig ON main_table
================================

DROP TRIGGER after_upd_b_row_trig ON main_table
================================

DROP TRIGGER after_upd_b_stmt_trig ON main_table
================================

	return new
================================
$$ language plpgsql
================================


insert into trigtest default values
================================

insert into trigtest default values
================================

insert into trigtest default values
================================

insert into trigtest default values
================================

insert into trigtest default values
================================

insert into trigtest default values
================================

-- ensure we still insert, even when all triggers are disabled
insert into trigtest default values
================================

	relid text
================================


	-- plpgsql can't discover its trigger data in a hash like perl and python
	-- can, or by a sort of reflection like tcl can,
	-- so we have to hard code the names.
	raise NOTICE 'TG_NAME: %', TG_name
================================

	raise NOTICE 'TG_WHEN: %', TG_when
================================

	raise NOTICE 'TG_LEVEL: %', TG_level
================================

	raise NOTICE 'TG_OP: %', TG_op
================================

	raise NOTICE 'TG_RELID::regclass: %', relid
================================

	raise NOTICE 'TG_RELNAME: %', TG_relname
================================

	raise NOTICE 'TG_TABLE_NAME: %', TG_table_name
================================

	raise NOTICE 'TG_TABLE_SCHEMA: %', TG_table_schema
================================

	raise NOTICE 'TG_NARGS: %', TG_nargs
================================


	argstr := '['
================================

	for i in 0 .. TG_nargs - 1 loop
		if i > 0 then
			argstr := argstr || ', '
================================

		argstr := argstr || TG_argv[i]
================================

	argstr := argstr || ']'
================================

	raise NOTICE 'TG_ARGV: %', argstr
================================


	if TG_OP != 'INSERT' then
		raise NOTICE 'OLD: %', OLD
================================


	if TG_OP != 'DELETE' then
		raise NOTICE 'NEW: %', NEW
================================


	if TG_OP = 'DELETE' then
		return OLD
================================

	else
		return NEW
================================

$$
================================


DROP TRIGGER show_trigger_data_trig on trigger_test
================================

	else
		raise notice 'row % changed', new.f1
================================

	return new
================================

end$$
================================

	else
		raise notice 'row % not changed', new.f1
================================

	return new
================================

end$$
================================

	return new
================================

$$
================================


INSERT INTO serializable_update_tab SELECT a, repeat('xyzxz', 100), 'new'
	FROM generate_series(1, 50) a
================================


\set QUIET false

UPDATE min_updates_test SET f1 = f1
================================


\set QUIET true

SELECT * FROM min_updates_test
================================

        argstr := argstr || TG_argv[i]
================================


    raise notice '% % % % (%)', TG_TABLE_NAME, TG_WHEN, TG_OP, TG_LEVEL, argstr
================================


    if TG_LEVEL = 'ROW' then
        if TG_OP = 'INSERT' then
            raise NOTICE 'NEW: %', NEW
================================

            RETURN NEW
================================


        if TG_OP = 'UPDATE' then
            raise NOTICE 'OLD: %, NEW: %', OLD, NEW
================================

            if NOT FOUND then RETURN NULL
================================

            RETURN NEW
================================


        if TG_OP = 'DELETE' then
            raise NOTICE 'OLD: %', OLD
================================

            if NOT FOUND then RETURN NULL
================================

            RETURN OLD
================================


    RETURN NULL
================================

$$
================================


\set QUIET false

-- Insert into view using trigger
INSERT INTO main_view VALUES (20, 30)
================================


-- Remove table trigger to allow updates
DROP TRIGGER before_upd_a_row_trig ON main_table
================================


\set QUIET true

-- Describe view should list triggers
\d main_view

-- Test dropping view triggers
DROP TRIGGER instead_of_insert_trig ON main_view
================================

DROP TRIGGER instead_of_delete_trig ON main_view
================================

\d+ main_view
DROP VIEW main_view
================================

        if NOT FOUND then
            raise exception 'No such country: "%"', NEW.country_name
================================

    else
        NEW.continent := NULL
================================


    if NEW.city_id IS NOT NULL then
        INSERT INTO city_table
            VALUES(NEW.city_id, NEW.city_name, NEW.population, ctry_id)
================================

    else
        INSERT INTO city_table(city_name, population, country_id)
            VALUES(NEW.city_name, NEW.population, ctry_id)
            RETURNING city_id INTO NEW.city_id
================================


    RETURN NEW
================================

$$
================================

    if NOT FOUND then RETURN NULL
================================

    RETURN OLD
================================

$$
================================

        if NOT FOUND then
            raise exception 'No such country: "%"', NEW.country_name
================================

    else
        UPDATE city_table SET city_name = NEW.city_name,
                              population = NEW.population
            WHERE city_id = OLD.city_id
================================

        NEW.continent := OLD.continent
================================


    if NOT FOUND then RETURN NULL
================================

    RETURN NEW
================================

$$
================================


\set QUIET false

-- INSERT .. RETURNING
INSERT INTO city_view(city_name) VALUES('Tokyo') RETURNING *
================================


\set QUIET true

-- read-only view with WHERE clause
CREATE VIEW european_city_view AS
    SELECT * FROM city_view WHERE continent = 'Europe'
================================


CREATE FUNCTION no_op_trig_fn() RETURNS trigger LANGUAGE plpgsql
AS 'begin RETURN NULL
================================


\set QUIET false

INSERT INTO european_city_view VALUES (0, 'x', 10000, 'y', 'z')
================================


\set QUIET true

-- rules bypassing no-op triggers
CREATE RULE european_city_insert_rule AS ON INSERT TO european_city_view
DO INSTEAD INSERT INTO city_view
VALUES (NEW.city_id, NEW.city_name, NEW.population, NEW.country_name, NEW.continent)
RETURNING *
================================


\set QUIET false

-- INSERT not limited by view's WHERE clause, but UPDATE AND DELETE are
INSERT INTO european_city_view(city_name, country_name)
    VALUES ('Cambridge', 'USA') RETURNING *
================================


\set QUIET true

SELECT * FROM city_view
================================

  raise notice '%: depth = %', tg_name, pg_trigger_depth()
================================

  return new
================================

$$
================================

  exception
    when sqlstate 'U9999' then
      raise notice 'SQLSTATE = U9999: depth = %', pg_trigger_depth()
================================

  raise notice '%: depth = %', tg_name, pg_trigger_depth()
================================

  if new.id = 1 then
    execute 'insert into depth_c values (' || new.id::text || ')'
================================

  return new
================================

$$
================================

  if new.id = 1 then
    raise exception sqlstate 'U9999'
================================

  raise notice '%: depth = %', tg_name, pg_trigger_depth()
================================

  return new
================================

$$
================================

  return new
================================

$$
================================

  return old
================================

$$
================================

  return new
================================

$$
================================

  return old
================================

$$
================================

  if found then
    delete from parent where aid = old.aid
================================

    return null
================================

  return old
================================

$$
================================

  return new
================================

$$
================================

  return old
================================

$$
================================

	return null
================================

$$ language plpgsql
================================

  return new
================================

end$$ language plpgsql
================================

  return new
================================

end$$ language plpgsql
================================

    raise warning 'before update (new): %', new.*::text
================================

  elsif (TG_OP = 'INSERT') then
    raise warning 'before insert (new): %', new.*::text
================================

    if new.key % 2 = 0 then
      new.key := new.key + 1
================================

      new.color := new.color || ' trig modified'
================================

      raise warning 'before insert (new, modified): %', new.*::text
================================

  return new
================================

$$
================================

    raise warning 'after update (new): %', new.*::text
================================

  elsif (TG_OP = 'INSERT') then
    raise warning 'after insert (new): %', new.*::text
================================

  return null
================================

$$
================================
 $$ language plpgsql
================================
 $$
================================

drop trigger trg1 on trigpart1
================================
	-- fail
drop trigger trg1 on trigpart2
================================
	-- fail
drop trigger trg1 on trigpart3
================================

drop trigger trg1 on trigpart
================================

\d trigpart3
alter table trigpart detach partition trigpart3
================================

drop trigger trg1 on trigpart3
================================

drop trigger trg1 on trigpart41
================================

\d trigpart3
alter table trigpart attach partition trigpart3 FOR VALUES FROM (2000) to (3000)
================================

    if TG_LEVEL = 'ROW' then
       return NEW
================================

    return null
================================

  $$ language plpgsql
================================


with ins (a) as (
  insert into parted2_stmt_trig values (1), (2) returning a
) insert into parted_stmt_trig select a from ins returning tableoid::regclass, a
================================

1
2
\.

-- insert via copy on the first partition
copy parted_stmt_trig1(a) from stdin
================================

1
\.

-- Disabling a trigger in the parent table should disable children triggers too
alter table parted_stmt_trig disable trigger trig_ins_after_parent
================================

    arg2 integer = TG_ARGV[1]
================================

    return null
================================

  $$ language plpgsql
================================
 return true
================================
 $$
================================

    if TG_LEVEL = 'ROW' then
       return NEW
================================

    return null
================================

  $$ language plpgsql
================================

  return new
================================

$$
================================

  return new
================================

$$
================================

  return new
================================

$$
================================


-- update itself moves tuple to new partition
================================
 trigger still works
truncate table parted
================================

  return new
================================

$$
================================

  return new
================================

$$
================================

create constraint trigger parted_trig after insert on parted_constr_ancestor
  deferrable
  for each row execute procedure trigger_notice_ab()
================================

create constraint trigger parted_trig_two after insert on parted_constr
  deferrable initially deferred
  for each row when (bark(new.b) AND new.a % 2 = 1)
  execute procedure trigger_notice_ab()
================================


-- The immediate constraint is fired immediately
================================
 the WHEN clause of the
-- deferred constraint is also called immediately.  The deferred constraint
-- is fired at commit time.
begin
================================

create constraint trigger parted_trigger after update on parted_trigger
  from parted_referenced
  for each row execute procedure trigger_notice_ab()
================================

create constraint trigger parted_trigger after update on unparted_trigger
  from parted_referenced
  for each row execute procedure trigger_notice_ab()
================================

    return null
================================

$$
================================

    return null
================================

$$
================================

    return null
================================

$$
================================

AAA	42
BBB	42
CCC	42
\.

-- DML affecting parent sees tuples collected from children even if
-- there is no transition table trigger on the children
drop trigger child1_insert_trig on child1
================================

drop trigger child1_update_trig on child1
================================

drop trigger child1_delete_trig on child1
================================

drop trigger child2_insert_trig on child2
================================

drop trigger child2_update_trig on child2
================================

drop trigger child2_delete_trig on child2
================================

drop trigger child3_insert_trig on child3
================================

drop trigger child3_update_trig on child3
================================

drop trigger child3_delete_trig on child3
================================

AAA	42
BBB	42
CCC	42
\.

-- insert into parent with a before trigger on a child tuple before
-- insertion, and we capture the newly modified row in parent format
create or replace function intercept_insert() returns trigger language plpgsql as
$$
  begin
    new.b = new.b + 1000
================================

    return new
================================

$$
================================

AAA	42
BBB	42
CCC	234
\.

drop table child1, child2, child3, parent
================================


-- drop the trigger, and now we're allowed to attach it again
drop trigger child_row_trig on child
================================


--
-- Verify behavior of statement triggers on (non-partition)
-- inheritance hierarchy with transition tables
================================
 similar to the
-- partition case, except there is no rerouting on insertion and child
-- tables can have extra columns
--

-- set up inheritance hierarchy with different TupleDescriptors
create table parent (a text, b int)
================================

AAA	42
BBB	42
CCC	42
\.

-- same behavior for copy if there is an index (interesting because rows are
-- captured by a different code path in copyfrom.c if there are indexes)
create index on parent(b)
================================

DDD	42
\.

-- DML affecting parent sees tuples collected from children even if
-- there is no transition table trigger on the children
drop trigger child1_insert_trig on child1
================================

drop trigger child1_update_trig on child1
================================

drop trigger child1_delete_trig on child1
================================

drop trigger child2_insert_trig on child2
================================

drop trigger child2_update_trig on child2
================================

drop trigger child2_delete_trig on child2
================================

drop trigger child3_insert_trig on child3
================================

drop trigger child3_update_trig on child3
================================

drop trigger child3_delete_trig on child3
================================


-- drop the trigger, and now we're allowed to make it inherit again
drop trigger child_row_trig on child
================================


-- without AR trigger, cascaded deletes all end up in one transition table
drop trigger self_ref_r_trig on self_ref
================================

  return null
================================
 $$ language plpgsql
================================

  return null
================================
 $$ language plpgsql
================================


create or replace trigger my_trig
  before insert on my_table
  for each row execute procedure funcB()
================================


-- test that trigger can be replaced by another one
-- at the same level of partition table
create or replace trigger my_trig
  after insert on parted_trig
  for each row execute procedure funcA()
================================

create or replace trigger my_trig
  after insert on parted_trig
  for each row execute procedure funcB()
================================


-- test that child trigger cannot be replaced directly
create or replace trigger my_trig
  after insert on parted_trig
  for each row execute procedure funcA()
================================

create or replace trigger my_trig
  after insert on parted_trig_1
  for each row execute procedure funcB()
================================

drop trigger my_trig on parted_trig
================================

create or replace trigger my_trig
  after insert on parted_trig
  for each row execute procedure funcB()
================================
 $$
================================

return null
================================
 $$
================================

return null
================================
 $$
================================

return null
================================
 $$
================================
 $$
language plpgsql
================================


alter trigger a on grandparent rename to b
================================

alter trigger a on only grandparent rename to b
================================
	-- ONLY not supported
alter trigger b on middle rename to c
================================

alter trigger b on grandparent rename to c
================================

alter trigger p on grandparent rename to q
================================

alter trigger parenttrig on parent rename to anothertrig
================================

\d+ child

drop table parent, child
================================


================================


INSERT INTO TEMP_FLOAT (f1)
  SELECT float8(f1) FROM INT4_TBL
================================


INSERT INTO TEMP_FLOAT (f1)
  SELECT float8(f1) FROM INT2_TBL
================================


INSERT INTO TEMP_INT4 (f1)
  SELECT int4(f1) FROM FLOAT8_TBL
  WHERE (f1 > -2147483647) AND (f1 < 2147483647)
================================


INSERT INTO TEMP_INT4 (f1)
  SELECT int4(f1) FROM INT2_TBL
================================


INSERT INTO TEMP_INT2 (f1)
  SELECT int2(f1) FROM FLOAT8_TBL
  WHERE (f1 >= -32767) AND (f1 <= 32767)
================================


INSERT INTO TEMP_INT2 (f1)
  SELECT int2(f1) FROM INT4_TBL
  WHERE (f1 >= -32767) AND (f1 <= 32767)
================================


INSERT INTO TEMP_GROUP
  SELECT 1, (- i.f1), (- f.f1)
  FROM INT4_TBL i, FLOAT8_TBL f
================================


INSERT INTO TEMP_GROUP
  SELECT 2, i.f1, f.f1
  FROM INT4_TBL i, FLOAT8_TBL f
================================


================================


--
-- hash index
-- grep '^90[^0-9]' hashovfl.data
--
-- SELECT count(*) AS i988 FROM hash_ovfl_heap
--    WHERE x = 90
================================


--
-- hash index
-- grep '^1000[^0-9]' hashovfl.data
--
-- SELECT count(*) AS i0 FROM hash_ovfl_heap
--    WHERE x = 1000
================================


--
-- this is the row we just replaced
================================
 index scan should return zero rows
--
SELECT h.seqno AS emptyset
   FROM hash_name_heap h
   WHERE h.random = '76652222'::name
================================


-- UPDATE hash_ovfl_heap
--    SET x = 1000
--   WHERE x = 90
================================


-- this vacuums the index as well
-- VACUUM hash_ovfl_heap
================================


-- SELECT count(*) AS i0 FROM hash_ovfl_heap
--   WHERE x = 90
================================


-- SELECT count(*) AS i988 FROM hash_ovfl_heap
--  WHERE x = 1000
================================

INSERT INTO hash_split_heap SELECT 1 FROM generate_series(1, 500) a
================================

INSERT INTO hash_split_heap SELECT 1 FROM generate_series(1, 5000) a
================================


DECLARE c CURSOR FOR SELECT * from hash_split_heap WHERE keycol = 1
================================

MOVE FORWARD ALL FROM c
================================

MOVE BACKWARD 10000 FROM c
================================

MOVE BACKWARD ALL FROM c
================================

CLOSE c
================================

INSERT INTO hash_split_heap SELECT a/2 FROM generate_series(1, 25000) a
================================

REINDEX INDEX hash_split_index
================================


================================



-- ************************************************************
-- *
-- * Trigger procedures and functions for the patchfield
-- * test of PL/pgSQL
-- *
-- ************************************************************


-- ************************************************************
-- * AFTER UPDATE on Room
-- *	- If room no changes let wall slots follow
-- ************************************************************
create function tg_room_au() returns trigger as '
begin
    if new.roomno != old.roomno then
        update WSlot set roomno = new.roomno where roomno = old.roomno
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * AFTER DELETE on Room
-- *	- delete wall slots in this room
-- ************************************************************
create function tg_room_ad() returns trigger as '
begin
    delete from WSlot where roomno = old.roomno
================================

    return old
================================

' language plpgsql
================================

    return new
================================

$$ language plpgsql
================================



-- ************************************************************
-- * AFTER UPDATE on PField
-- *	- Let PSlots of this field follow
-- ************************************************************
create function tg_pfield_au() returns trigger as '
begin
    if new.name != old.name then
        update PSlot set pfname = new.name where pfname = old.name
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * AFTER DELETE on PField
-- *	- Remove all slots of this patchfield
-- ************************************************************
create function tg_pfield_ad() returns trigger as '
begin
    delete from PSlot where pfname = old.name
================================

    return old
================================

' language plpgsql
================================

    ps          alias for new
================================

    if not found then
        raise exception $$Patchfield "%" does not exist$$, ps.pfname
================================

    return ps
================================

$proc$ language plpgsql
================================



-- ************************************************************
-- * AFTER UPDATE on System
-- *	- If system name changes let interfaces follow
-- ************************************************************
create function tg_system_au() returns trigger as '
begin
    if new.name != old.name then
        update IFace set sysname = new.name where sysname = old.name
================================

    return new
================================

' language plpgsql
================================

    sysrec	record
================================

    if not found then
        raise exception $q$system "%" does not exist$q$, new.sysname
================================

    sname := 'IF.' || new.sysname
================================

    sname := sname || '.'
================================

    sname := sname || new.ifname
================================

    if length(sname) > 20 then
        raise exception 'IFace slotname "%" too long (20 char max)', sname
================================

    new.slotname := sname
================================

    return new
================================

$$ language plpgsql
================================



-- ************************************************************
-- * AFTER INSERT or UPDATE or DELETE on Hub
-- *	- insert/delete/rename slots as required
-- ************************************************************
create function tg_hub_a() returns trigger as '
declare
    hname	text
================================

    dummy	integer
================================

	return new
================================

    if tg_op = ''UPDATE'' then
	if new.name != old.name then
	    update HSlot set hubname = new.name where hubname = old.name
================================

	dummy := tg_hub_adjustslots(new.name, old.nslots, new.nslots)
================================

	return new
================================

    if tg_op = ''DELETE'' then
	dummy := tg_hub_adjustslots(old.name, old.nslots, 0)
================================

	return old
================================

' language plpgsql
================================



-- ************************************************************
-- * Support function to add/remove slots of Hub
-- ************************************************************
create function tg_hub_adjustslots(hname bpchar,
                                   oldnslots integer,
                                   newnslots integer)
returns integer as '
begin
    if newnslots = oldnslots then
        return 0
================================

    if newnslots < oldnslots then
        delete from HSlot where hubname = hname and slotno > newnslots
================================

	return 0
================================

    for i in oldnslots + 1 .. newnslots loop
        insert into HSlot (slotname, hubname, slotno, slotlink)
		values (''HS.dummy'', hname, i, '''')
================================

    return 0
================================


-- Test comments
COMMENT ON FUNCTION tg_hub_adjustslots_wrong(bpchar, integer, integer) IS 'function with args'
================================

COMMENT ON FUNCTION tg_hub_adjustslots(bpchar, integer, integer) IS 'function with args'
================================

COMMENT ON FUNCTION tg_hub_adjustslots(bpchar, integer, integer) IS NULL
================================


-- ************************************************************
-- * BEFORE INSERT or UPDATE on HSlot
-- *	- prevent from manual manipulation
-- *	- set the slotname to HS.hubname.slotno
-- ************************************************************
create function tg_hslot_biu() returns trigger as '
declare
    sname	text
================================

    xname	HSlot.slotname%TYPE
================================

    hubrec	record
================================

    if not found then
        raise exception ''no manual manipulation of HSlot''
================================

    if new.slotno < 1 or new.slotno > hubrec.nslots then
        raise exception ''no manual manipulation of HSlot''
================================

    if tg_op = ''UPDATE'' and new.hubname != old.hubname then
	if count(*) > 0 from Hub where name = old.hubname then
	    raise exception ''no manual manipulation of HSlot''
================================

    sname := ''HS.'' || trim(new.hubname)
================================

    sname := sname || ''.''
================================

    sname := sname || new.slotno::text
================================

    if length(sname) > 20 then
        raise exception ''HSlot slotname "%" too long (20 char max)'', sname
================================

    new.slotname := sname
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE DELETE on HSlot
-- *	- prevent from manual manipulation
-- ************************************************************
create function tg_hslot_bd() returns trigger as '
declare
    hubrec	record
================================

    if not found then
        return old
================================

    if old.slotno > hubrec.nslots then
        return old
================================

    raise exception ''no manual manipulation of HSlot''
================================

' language plpgsql
================================

    return new
================================

' language plpgsql
================================

    return new
================================

' language plpgsql
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE UPDATE on PSlot
-- *	- do delete/insert instead of update if name changes
-- ************************************************************
create function tg_pslot_bu() returns trigger as '
begin
    if new.slotname != old.slotname then
        delete from PSlot where slotname = old.slotname
================================

        return null
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE UPDATE on WSlot
-- *	- do delete/insert instead of update if name changes
-- ************************************************************
create function tg_wslot_bu() returns trigger as '
begin
    if new.slotname != old.slotname then
        delete from WSlot where slotname = old.slotname
================================

        return null
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE UPDATE on PLine
-- *	- do delete/insert instead of update if name changes
-- ************************************************************
create function tg_pline_bu() returns trigger as '
begin
    if new.slotname != old.slotname then
        delete from PLine where slotname = old.slotname
================================

        return null
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE UPDATE on IFace
-- *	- do delete/insert instead of update if name changes
-- ************************************************************
create function tg_iface_bu() returns trigger as '
begin
    if new.slotname != old.slotname then
        delete from IFace where slotname = old.slotname
================================

        return null
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE UPDATE on HSlot
-- *	- do delete/insert instead of update if name changes
-- ************************************************************
create function tg_hslot_bu() returns trigger as '
begin
    if new.slotname != old.slotname or new.hubname != old.hubname then
        delete from HSlot where slotname = old.slotname
================================

        return null
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * BEFORE UPDATE on PHone
-- *	- do delete/insert instead of update if name changes
-- ************************************************************
create function tg_phone_bu() returns trigger as '
begin
    if new.slotname != old.slotname then
        delete from PHone where slotname = old.slotname
================================

        return null
================================

    return new
================================

' language plpgsql
================================



-- ************************************************************
-- * AFTER INSERT or UPDATE or DELETE on slot with backlink
-- *	- Ensure that the opponent correctly points back to us
-- ************************************************************
create function tg_backlink_a() returns trigger as '
declare
    dummy	integer
================================

	return new
================================

    if tg_op = ''UPDATE'' then
        if new.backlink != old.backlink then
	    if old.backlink != '''' then
	        dummy := tg_backlink_unset(old.backlink, old.slotname)
================================

	    if new.backlink != '''' then
	        dummy := tg_backlink_set(new.backlink, new.slotname)
================================

	else
	    if new.slotname != old.slotname and new.backlink != '''' then
	        dummy := tg_slotlink_set(new.backlink, new.slotname)
================================

	return new
================================

    if tg_op = ''DELETE'' then
        if old.backlink != '''' then
	    dummy := tg_backlink_unset(old.backlink, old.slotname)
================================

	return old
================================

' language plpgsql
================================



-- ************************************************************
-- * Support function to set the opponents backlink field
-- * if it does not already point to the requested slot
-- ************************************************************
create function tg_backlink_set(myname bpchar, blname bpchar)
returns integer as '
declare
    mytype	char(2)
================================

    link	char(4)
================================

    rec		record
================================

    link := mytype || substr(blname, 1, 2)
================================

    if link = ''PLPL'' then
        raise exception
		''backlink between two phone lines does not make sense''
================================

    if link in (''PLWS'', ''WSPL'') then
        raise exception
		''direct link of phone line to wall slot not permitted''
================================

    if mytype = ''PS'' then
        select into rec * from PSlot where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.backlink != blname then
	    update PSlot set backlink = blname where slotname = myname
================================

	return 0
================================

    if mytype = ''WS'' then
        select into rec * from WSlot where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.backlink != blname then
	    update WSlot set backlink = blname where slotname = myname
================================

	return 0
================================

    if mytype = ''PL'' then
        select into rec * from PLine where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.backlink != blname then
	    update PLine set backlink = blname where slotname = myname
================================

	return 0
================================

    raise exception ''illegal backlink beginning with %'', mytype
================================

' language plpgsql
================================



-- ************************************************************
-- * Support function to clear out the backlink field if
-- * it still points to specific slot
-- ************************************************************
create function tg_backlink_unset(bpchar, bpchar)
returns integer as '
declare
    myname	alias for $1
================================

    blname	alias for $2
================================

    mytype	char(2)
================================

    rec		record
================================

    if mytype = ''PS'' then
        select into rec * from PSlot where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.backlink = blname then
	    update PSlot set backlink = '''' where slotname = myname
================================

	return 0
================================

    if mytype = ''WS'' then
        select into rec * from WSlot where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.backlink = blname then
	    update WSlot set backlink = '''' where slotname = myname
================================

	return 0
================================

    if mytype = ''PL'' then
        select into rec * from PLine where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.backlink = blname then
	    update PLine set backlink = '''' where slotname = myname
================================

	return 0
================================



-- ************************************************************
-- * AFTER INSERT or UPDATE or DELETE on slot with slotlink
-- *	- Ensure that the opponent correctly points back to us
-- ************************************************************
create function tg_slotlink_a() returns trigger as '
declare
    dummy	integer
================================

	return new
================================

    if tg_op = ''UPDATE'' then
        if new.slotlink != old.slotlink then
	    if old.slotlink != '''' then
	        dummy := tg_slotlink_unset(old.slotlink, old.slotname)
================================

	    if new.slotlink != '''' then
	        dummy := tg_slotlink_set(new.slotlink, new.slotname)
================================

	else
	    if new.slotname != old.slotname and new.slotlink != '''' then
	        dummy := tg_slotlink_set(new.slotlink, new.slotname)
================================

	return new
================================

    if tg_op = ''DELETE'' then
        if old.slotlink != '''' then
	    dummy := tg_slotlink_unset(old.slotlink, old.slotname)
================================

	return old
================================

' language plpgsql
================================



-- ************************************************************
-- * Support function to set the opponents slotlink field
-- * if it does not already point to the requested slot
-- ************************************************************
create function tg_slotlink_set(bpchar, bpchar)
returns integer as '
declare
    myname	alias for $1
================================

    blname	alias for $2
================================

    mytype	char(2)
================================

    link	char(4)
================================

    rec		record
================================

    link := mytype || substr(blname, 1, 2)
================================

    if link = ''PHPH'' then
        raise exception
		''slotlink between two phones does not make sense''
================================

    if link in (''PHHS'', ''HSPH'') then
        raise exception
		''link of phone to hub does not make sense''
================================

    if link in (''PHIF'', ''IFPH'') then
        raise exception
		''link of phone to hub does not make sense''
================================

    if link in (''PSWS'', ''WSPS'') then
        raise exception
		''slotlink from patchslot to wallslot not permitted''
================================

    if mytype = ''PS'' then
        select into rec * from PSlot where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.slotlink != blname then
	    update PSlot set slotlink = blname where slotname = myname
================================

	return 0
================================

    if mytype = ''WS'' then
        select into rec * from WSlot where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.slotlink != blname then
	    update WSlot set slotlink = blname where slotname = myname
================================

	return 0
================================

    if mytype = ''IF'' then
        select into rec * from IFace where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.slotlink != blname then
	    update IFace set slotlink = blname where slotname = myname
================================

	return 0
================================

    if mytype = ''HS'' then
        select into rec * from HSlot where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.slotlink != blname then
	    update HSlot set slotlink = blname where slotname = myname
================================

	return 0
================================

    if mytype = ''PH'' then
        select into rec * from PHone where slotname = myname
================================

	if not found then
	    raise exception ''% does not exist'', myname
================================

	if rec.slotlink != blname then
	    update PHone set slotlink = blname where slotname = myname
================================

	return 0
================================

    raise exception ''illegal slotlink beginning with %'', mytype
================================

' language plpgsql
================================



-- ************************************************************
-- * Support function to clear out the slotlink field if
-- * it still points to specific slot
-- ************************************************************
create function tg_slotlink_unset(bpchar, bpchar)
returns integer as '
declare
    myname	alias for $1
================================

    blname	alias for $2
================================

    mytype	char(2)
================================

    rec		record
================================

    if mytype = ''PS'' then
        select into rec * from PSlot where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.slotlink = blname then
	    update PSlot set slotlink = '''' where slotname = myname
================================

	return 0
================================

    if mytype = ''WS'' then
        select into rec * from WSlot where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.slotlink = blname then
	    update WSlot set slotlink = '''' where slotname = myname
================================

	return 0
================================

    if mytype = ''IF'' then
        select into rec * from IFace where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.slotlink = blname then
	    update IFace set slotlink = '''' where slotname = myname
================================

	return 0
================================

    if mytype = ''HS'' then
        select into rec * from HSlot where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.slotlink = blname then
	    update HSlot set slotlink = '''' where slotname = myname
================================

	return 0
================================

    if mytype = ''PH'' then
        select into rec * from PHone where slotname = myname
================================

	if not found then
	    return 0
================================

	if rec.slotlink = blname then
	    update PHone set slotlink = '''' where slotname = myname
================================

	return 0
================================

' language plpgsql
================================



-- ************************************************************
-- * Describe the backside of a patchfield slot
-- ************************************************************
create function pslot_backlink_view(bpchar)
returns text as '
<<outer>>
declare
    rec		record
================================

    bltype	char(2)
================================

    retval	text
================================

    if not found then
        return ''''
================================

    if rec.backlink = '''' then
        return ''-''
================================

    bltype := substr(rec.backlink, 1, 2)
================================

    if bltype = ''PL'' then
        declare
	    rec		record
================================

	    retval := ''Phone line '' || trim(rec.phonenumber)
================================

	    if rec.comment != '''' then
	        retval := retval || '' (''
================================

		retval := retval || rec.comment
================================

		retval := retval || '')''
================================

	    return retval
================================

    if bltype = ''WS'' then
        select into rec * from WSlot where slotname = rec.backlink
================================

	retval := trim(rec.slotname) || '' in room ''
================================

	retval := retval || trim(rec.roomno)
================================

	retval := retval || '' -> ''
================================

	return retval || wslot_slotlink_view(rec.slotname)
================================

    return rec.backlink
================================

' language plpgsql
================================



-- ************************************************************
-- * Describe the front of a patchfield slot
-- ************************************************************
create function pslot_slotlink_view(bpchar)
returns text as '
declare
    psrec	record
================================

    sltype	char(2)
================================

    retval	text
================================

    if not found then
        return ''''
================================

    if psrec.slotlink = '''' then
        return ''-''
================================

    sltype := substr(psrec.slotlink, 1, 2)
================================

    if sltype = ''PS'' then
	retval := trim(psrec.slotlink) || '' -> ''
================================

	return retval || pslot_backlink_view(psrec.slotlink)
================================

    if sltype = ''HS'' then
        retval := comment from Hub H, HSlot HS
			where HS.slotname = psrec.slotlink
			  and H.name = HS.hubname
================================

        retval := retval || '' slot ''
================================

	retval := retval || slotno::text from HSlot
			where slotname = psrec.slotlink
================================

	return retval
================================

    return psrec.slotlink
================================

' language plpgsql
================================



-- ************************************************************
-- * Describe the front of a wall connector slot
-- ************************************************************
create function wslot_slotlink_view(bpchar)
returns text as '
declare
    rec		record
================================

    sltype	char(2)
================================

    retval	text
================================

    if not found then
        return ''''
================================

    if rec.slotlink = '''' then
        return ''-''
================================

    sltype := substr(rec.slotlink, 1, 2)
================================

    if sltype = ''PH'' then
        select into rec * from PHone where slotname = rec.slotlink
================================

	retval := ''Phone '' || trim(rec.slotname)
================================

	if rec.comment != '''' then
	    retval := retval || '' (''
================================

	    retval := retval || rec.comment
================================

	    retval := retval || '')''
================================

	return retval
================================

    if sltype = ''IF'' then
	declare
	    syrow	System%RowType
================================

	    ifrow	IFace%ROWTYPE
================================

	    retval := syrow.name || '' IF ''
================================

	    retval := retval || ifrow.ifname
================================

	    if syrow.comment != '''' then
	        retval := retval || '' (''
================================

		retval := retval || syrow.comment
================================

		retval := retval || '')''
================================

	    return retval
================================

    return rec.slotlink
================================

' language plpgsql
================================



--
-- The following tests are unrelated to the scenario outlined above
================================

-- they merely exercise specific parts of PL/pgSQL
--

--
-- Test recursion, per bug report 7-Sep-01
--
CREATE FUNCTION recursion_test(int,int) RETURNS text AS '
DECLARE rslt text
================================

    ELSE
        rslt = CAST($1 AS TEXT) || '','' || recursion_test($1 - 1, $2)
================================

    RETURN rslt
================================
' LANGUAGE plpgsql
================================


create function test_found()
  returns boolean as '
  declare
  begin
  insert into found_test_tbl values (1)
================================

  if FOUND then
     insert into found_test_tbl values (2)
================================

  if FOUND then
    insert into found_test_tbl values (3)
================================
 -- matches no rows
  if not FOUND then
    insert into found_test_tbl values (4)
================================


  for i in 1 .. 10 loop
    -- no need to do anything
  end loop
================================

  if FOUND then
    insert into found_test_tbl values (5)
================================


  -- never executes the loop
  for i in 2 .. 1 loop
    -- no need to do anything
  end loop
================================

  if not FOUND then
    insert into found_test_tbl values (6)
================================

  return true
================================
' language plpgsql
================================


--
-- Test set-returning functions for PL/pgSQL
--

create function test_table_func_rec() returns setof found_test_tbl as '
DECLARE
	rec RECORD
================================

	RETURN
================================
' language plpgsql
================================


create function test_table_func_row() returns setof found_test_tbl as '
DECLARE
	row found_test_tbl%ROWTYPE
================================

	RETURN
================================
' language plpgsql
================================


create function test_ret_set_scalar(int,int) returns setof int as '
DECLARE
	i int
================================

	RETURN
================================
' language plpgsql
================================


create function test_ret_set_rec_dyn(int) returns setof record as '
DECLARE
	retval RECORD
================================

		RETURN NEXT retval
================================

		RETURN NEXT retval
================================

	ELSE
		SELECT INTO retval 50, 5::numeric, ''xxx''::text
================================

		RETURN NEXT retval
================================

		RETURN NEXT retval
================================

	RETURN
================================
' language plpgsql
================================


create function test_ret_rec_dyn(int) returns record as '
DECLARE
	retval RECORD
================================

		RETURN retval
================================

	ELSE
		SELECT INTO retval 50, 5::numeric, ''xxx''::text
================================

		RETURN retval
================================
' language plpgsql
================================

end$$ language plpgsql
================================


select f1(42) as int, f1(4.5) as num
================================

select f1(point(3,4))
================================

end$$ language plpgsql
================================


select f1(42) as int, f1(4.5) as num
================================

end$$ language plpgsql
================================


select f1(array[2,4]) as int, f1(array[4.5, 7.7]) as num
================================

end$$ language plpgsql
================================


select f1(array[2,4]) as int, f1(array[4.5, 7.7]) as num
================================

end$$ language plpgsql
================================

end$$ language plpgsql
================================


select f1(int4range(42, 49)) as int, f1(float8range(4.5, 7.8)) as num
================================

end$$ language plpgsql
================================


select f1(2, 4) as int, f1(2, 4.5) as num
================================

end$$ language plpgsql
================================


select f1(int4range(42, 49), 11, 2::smallint) as int, f1(float8range(4.5, 7.8), 7.8, 11::real) as num
================================


select f1(int4range(42, 49), 11, 4.5) as fail
================================

end$$ language plpgsql
================================

end$$ language plpgsql
================================


select f1(int4range(42, 49), array[11]) as int, f1(float8range(4.5, 7.8), array[7]) as num
================================

  y := array[c, d]
================================

end$$ language plpgsql
================================

select x, pg_typeof(x), y, pg_typeof(y)
  from f1(11, array[1, 2], point(1,2), point(3,4))
================================

select x, pg_typeof(x), y, pg_typeof(y)
  from f1(11, '{1,2}', point(1,2), '(3,4)')
================================


--
-- Test handling of OUT parameters, including polymorphic cases.
-- Note that RETURN is optional with OUT params
================================
 we try both ways.
--

-- wrong way to do it:
create function f1(in i int, out j int) returns int as $$
begin
  return i+1
================================

end$$ language plpgsql
================================

  return
================================

end$$ language plpgsql
================================

end$$ language plpgsql
================================

  return next
================================

  j := i+2
================================

  return next
================================

  return
================================

end$$ language plpgsql
================================

  j := j+1
================================

  k := 'foo'
================================

end$$ language plpgsql
================================

  k := 'foo'
================================

  return next
================================

  j := j+1
================================

  k := 'foot'
================================

  return next
================================

end$$ language plpgsql
================================

  k := array[j,j]
================================

  return
================================

end$$ language plpgsql
================================

  k := array[lower(i),upper(i)]
================================

  return
================================

end$$ language plpgsql
================================


select * from duplic(int4range(42,49))
================================


create function perform_simple_func(int) returns boolean as '
BEGIN
	IF $1 < 20 THEN
		INSERT INTO perform_test VALUES ($1, $1 + 10)
================================

		RETURN TRUE
================================

	ELSE
		RETURN FALSE
================================
' language plpgsql
================================


create function perform_test_func() returns void as '
BEGIN
	IF FOUND then
		INSERT INTO perform_test VALUES (100, 100)
================================


	PERFORM perform_simple_func(5)
================================


	IF FOUND then
		INSERT INTO perform_test VALUES (100, 100)
================================


	PERFORM perform_simple_func(50)
================================


	IF FOUND then
		INSERT INTO perform_test VALUES (100, 100)
================================


	RETURN
================================
' language plpgsql
================================

  if found then return x
================================

  return 0
================================

end$$ language plpgsql stable
================================

  IF  my_id_user > 0 THEN
    RETURN -1
================================

  my_id_user = sp_id_user( a_login )
================================

  IF  my_id_user = 0 THEN
    RETURN -2
================================

  RETURN my_id_user
================================

end$$ language plpgsql
================================

5	10
50	100
500	1000
\.

create function return_unnamed_refcursor() returns refcursor as $$
declare
    rc refcursor
================================

    return rc
================================

    x record
================================

    fetch next from rc into x
================================

    return x.a
================================

    return rc
================================

    return $1
================================

fetch next in test1
================================

fetch all from test2
================================


-- should fail
fetch next from test1
================================

    nonsense record
================================

    fetch c1 into nonsense
================================

    close c1
================================

    if found then
        return true
================================

    else
        return false
================================

    nonsense record
================================

    fetch c1 into nonsense
================================

    close c1
================================

    if found then
        return true
================================

    else
        return false
================================

    nonsense record
================================

    fetch c1 into nonsense
================================

    close c1
================================

    if found then
        return true
================================

    else
        return false
================================

  n int4
================================

  fetch c1 into n
================================

  return n
================================

  p2 int4 := 1006
================================

  n int4
================================

  fetch c1 into n
================================

  return n
================================

    return $1
================================

$$ language plpgsql
================================

    return $1
================================

$$ language plpgsql
================================

    return $1
================================

$$ language plpgsql
================================

   EXCEPTION
       WHEN syntax_error THEN
           BEGIN
               raise notice 'exception % thrown in inner block, reraising', sqlerrm
================================

               RAISE
================================

           EXCEPTION
               WHEN OTHERS THEN
                   raise notice 'RIGHT - exception % caught in inner block', sqlerrm
================================

EXCEPTION
   WHEN OTHERS THEN
       raise notice 'WRONG - exception % caught in outer block', sqlerrm
================================

$$ LANGUAGE plpgsql
================================

    Johnny Yuma
================================

    a := 10
================================

    return a
================================

end$$ language plpgsql
================================

    return 5
================================
$$ language plpgsql
================================
$$ language plpgsql
================================
$$ language plpgsql
================================
$$ language plpgsql
================================
$$ language plpgsql
================================

create type eitype as (i integer, y integer)
================================

    _rt eifoo%rowtype
================================

    _v eitype
================================

    i int
================================

    j int
================================

    k int
================================

    execute 'select (row).* from (select row(10,1)::eifoo) s' into _r
================================

    raise notice '% %', _r.i, _r.y
================================

    execute 'select * from '||$1||' limit 1' into _rt
================================

    raise notice '% %', _rt.i, _rt.y
================================

    execute 'select *, 20 from '||$1||' limit 1' into i, j, k
================================

    raise notice '% % %', i, j, k
================================

    execute 'select 1,2' into _v
================================

    return _v
================================
 $$ language plpgsql
================================

drop type eitype cascade
================================
 $$ language plpgsql
================================
 $$ language plpgsql
================================

    exception when others then
	    raise notice 'caught exception % %', sqlstate, sqlerrm
================================

	        perform 10/0
================================

        exception
            when substring_error then
                -- this exception handler shouldn't be invoked
                raise notice 'unexpected exception: % %', sqlstate, sqlerrm
================================

	        when division_by_zero then
	            raise notice 'caught exception % %', sqlstate, sqlerrm
================================

	    raise notice '% %', sqlstate, sqlerrm
================================
 $$ language plpgsql
================================

	exception when others then return sqlerrm
================================
 $$ language plpgsql
================================

    c varchar = 'xyz'
================================

    i integer
================================

    raise notice '%
================================
 %
================================
 %
================================
 %
================================
 %
================================
 %', a, a[i], c, (select c || 'abc'), row(10,'aaa',NULL,30), NULL
================================
$$ language plpgsql
================================

  y int
================================

  return x = y
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

p1 int := 2
================================

p3 text := 'foo'
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

p1 int := 2
================================

p3 text := $a$'Valame Dios!' dijo Sancho
================================
 'no le dije yo a vuestra merced que mirase bien lo que hacia?'$a$
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

p1 int := 2
================================

p3 text := 'foo'
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

p1 int := 2
================================

p3 text := 'foo'
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

p1 int := 2
================================

p3 text := 'foo'
================================

  raise notice 'x.f1 = %, x.f2 = %', x.f1, x.f2
================================

end$$ language plpgsql
================================

out1 int
================================

out1 int
================================

end$$ language plpgsql
================================

end$$ language plpgsql
================================

c1 cursor (f1 int) for select 1
================================


do $$
declare x int
================================

$$
================================


do $$
declare x int
================================

$$
================================


do $$
declare
  x int
================================

  y int
================================


do $$
declare
  x int
================================

  y int
================================


do $$
declare
  x int
================================

  y int
================================
 -- should be ok
  raise notice 'ok'
================================

$$
================================


do $$
declare
  t test_01
================================
  -- should be ok
  raise notice 'ok'
================================
 -- should fail
================================

$$
================================


do $$
declare
  t test_01
================================
 -- should fail
================================

$$
================================

  x integer
================================

  fetch last from c into x
================================

  while found loop
    return next x
================================

    fetch prior from c into x
================================

  close c
================================

$$ language plpgsql
================================

  x integer
================================

  fetch last from c into x
================================

  while found loop
    return next x
================================

    fetch prior from c into x
================================

  close c
================================

$$ language plpgsql
================================

  x integer
================================

  fetch last from c into x
================================

  while found loop
    return next x
================================

    fetch prior from c into x
================================

  close c
================================

$$ language plpgsql
================================

  x integer
================================

  fetch last from c into x
================================

  while found loop
    return next x
================================

    fetch relative -2 from c into x
================================

  close c
================================

$$ language plpgsql
================================

  x integer
================================

  fetch last from c into x
================================

  while found loop
    return next x
================================

    move backward 2 from c
================================

    fetch relative -1 from c into x
================================

  close c
================================

$$ language plpgsql
================================

  x integer
================================

  loop
      move relative 2 in c
================================

      if not found then
          exit
================================

      fetch next from c into x
================================

      if found then
          return next x
================================

  close c
================================

$$ language plpgsql
================================

  x integer
================================

  move forward all in c
================================

  fetch backward from c into x
================================

  if found then
    return next x
================================

  close c
================================

$$ language plpgsql
================================

    raise notice 'pl_qual_names.param1 = %', pl_qual_names.param1
================================

    raise notice 'outerblock.param1 = %', outerblock.param1
================================

    raise notice 'innerblock.param1 = %', innerblock.param1
================================

$$ language plpgsql
================================

    $2 := -2
================================

    return next
================================

    return query select x + 1, x * 10 from generate_series(0, 10) s (x)
================================

    return next
================================

$$ language plpgsql
================================


create type record_type as (x text, y int, z boolean)
================================

$$ language plpgsql
================================

  execute 'select $2 + $2*3 + length($1)' into i using $2,$1
================================

  return i
================================

  i int
================================

  loop
    fetch c into i
================================

    exit when not found
================================

    raise notice '%', i
================================

  close c
================================

  return
================================

$$ language plpgsql
================================

  c2 cursor
       for select * from generate_series(41,43) i
================================

  -- again, to test if cursor was closed properly
  for r in c(9,10) loop
    raise notice '% from %', r.i, c
================================

  -- and test a parameterless cursor
  for r in c2 loop
    raise notice '% from %', r.i, c2
================================

  -- and try it with a hand-assigned name
  raise notice 'after loop, c2 = %', c2
================================

  c2 := 'special_name'
================================

  for r in c2 loop
    raise notice '% from %', r.i, c2
================================

  raise notice 'after loop, c2 = %', c2
================================

  -- and try it with a generated name
  -- (which we can't show in the output because it's variable)
  c2 := null
================================

  for r in c2 loop
    raise notice '%', r.i
================================

  raise notice 'after loop, c2 = %', c2
================================

  return
================================

$$ language plpgsql
================================


-- try updating the cursor's current row

create temp table forc_test as
  select n as i, n as j from generate_series(1,10) n
================================

    update forc_test set i = i * 100, j = r.j * 2 where current of c
================================

$$ language plpgsql
================================

  r record
================================

  loop
    fetch c into r
================================

    exit when not found
================================

    raise notice '%, %', r.i, r.j
================================

    update forc_test set i = i * 100, j = r.j * 2 where current of c
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

  return query execute 'select * from (values($1),($2)) f' using 40,50
================================

$$ language plpgsql
================================

  return query execute 'select * from tabwithcols'
================================

$$ language plpgsql
================================


--
-- Tests for composite-type results
--

create type compostype as (x int, y varchar)
================================

  return v
================================

$$ language plpgsql
================================

  return v
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

  return v
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

  return next null::compostype
================================

  return next (2, 'goodbye')::compostype
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

  return v
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

drop type compostype
================================

  raise '% % %', 1, 2, 3
     using errcode = 'division_by_zero', detail = 'some detail info'
================================

$$ language plpgsql
================================


-- Since we can't actually see the thrown SQLSTATE in default psql output,
-- test it like this
================================
 this also tests re-RAISE

create or replace function raise_test() returns void as $$
begin
  raise 'check me'
     using errcode = 'division_by_zero', detail = 'some detail info'
================================

  exception
    when others then
      raise notice 'SQLSTATE: % SQLERRM: %', sqlstate, sqlerrm
================================

      raise
================================

$$ language plpgsql
================================

  exception
    when others then
      raise notice 'SQLSTATE: % SQLERRM: %', sqlstate, sqlerrm
================================

      raise
================================

$$ language plpgsql
================================

  exception
    when sqlstate '1234F' then
      raise notice 'SQLSTATE: % SQLERRM: %', sqlstate, sqlerrm
================================

      raise
================================

$$ language plpgsql
================================

  exception
    when others then
      raise notice 'SQLSTATE: % SQLERRM: %', sqlstate, sqlerrm
================================

      raise
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

        _message text
================================

        _context text
================================

exception when others then
  get stacked diagnostics
        _sqlstate = returned_sqlstate,
        _message = message_text,
        _context = pg_exception_context
================================

  raise notice 'sqlstate: %, message: %, context: [%]',
    _sqlstate, _message, replace(_context, E'\n', ' <- ')
================================

$$ language plpgsql
================================

        _hint text
================================

        _message text
================================

exception when others then
  get stacked diagnostics
        _message = message_text,
        _detail = pg_exception_detail,
        _hint = pg_exception_hint
================================

  raise notice 'message: %, detail: %, hint: %', _message, _detail, _hint
================================

$$ language plpgsql
================================

        _hint text
================================

        _message text
================================

  raise notice 'message: %, detail: %, hint: %', _message, _detail, _hint
================================

$$ language plpgsql
================================

exception
  when sqlstate '22012' then
    raise notice using message = sqlstate
================================

    raise sqlstate '22012' using message = 'substitute message'
================================

$$ language plpgsql
================================

        _constraint_name text
================================

        _datatype_name text
================================

        _table_name text
================================

        _schema_name text
================================

exception when others then
  get stacked diagnostics
        _column_name = column_name,
        _constraint_name = constraint_name,
        _datatype_name = pg_datatype_name,
        _table_name = table_name,
        _schema_name = schema_name
================================

  raise notice 'column %, constraint %, type %, table %, schema %',
    _column_name, _constraint_name, _datatype_name, _table_name, _schema_name
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

  return aux
================================

$$ language plpgsql immutable strict
================================

  return $1
================================

$$ language plpgsql immutable strict
================================

$$ language plpgsql immutable strict
================================
 b := a1 + 1
================================

  return next
================================

  a := a1 * 10
================================
 b := a1 * 10 + 1
================================

  return next
================================

$$ language plpgsql immutable strict
================================

  get diagnostics rc = row_count
================================

  raise notice '% %', found, rc
================================

  return query select * from (values(10),(20)) f(a) where false
================================

  get diagnostics rc = row_count
================================

  raise notice '% %', found, rc
================================

  return query execute 'values(10),(20)'
================================

  get diagnostics rc = row_count
================================

  raise notice '% %', found, rc
================================

  return query execute 'select * from (values(10),(20)) f(a) where false'
================================

  get diagnostics rc = row_count
================================

  raise notice '% %', found, rc
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

  EXCEPTION
    WHEN others THEN RETURN 0
================================

  RETURN 1
================================

$$ LANGUAGE plpgsql
================================

  error_code := 1
================================

  new_id := 1
================================

  RETURN
================================

$$ LANGUAGE plpgsql
================================

  lr text
================================

  i integer
================================

  lr := 'fool'
================================

  i := 1
================================

  -- use sub-SELECTs to make expressions non-simple
  arr[(SELECT i)][(SELECT i+1)] := (SELECT lr)
================================

  RETURN arr
================================

$$ LANGUAGE plpgsql
================================
  -- should throw error
  exception
    WHEN OTHERS THEN
      i := (SELECT 1::integer)
================================

  return i
================================

$$ LANGUAGE plpgsql
================================

  else
    return $1
================================

$$ language plpgsql
================================
 $$ language sql
================================

end$$
================================


create cast (integer as date) with function sql_to_date(integer) as assignment
================================

end$$ language plpgsql
================================

do $$ declare x text[]
================================

do $$ declare x text[]
================================

  return 'foo\\bar\041baz'
================================

  return E'foo\\bar\041baz'
================================

  return 'foo\\bar\041baz\'
================================

  return E'foo\\bar\041baz'
================================


-- Test anonymous code blocks.

DO $$
DECLARE r record
================================

END$$
================================


-- these are to check syntax error reporting
DO LANGUAGE plpgsql $$begin return 1
================================
 end$$
================================


DO $$
DECLARE r record
================================

END$$
================================


-- Check handling of errors thrown from/into anonymous code blocks.
do $outer$
begin
  for i in 1..10 loop
   begin
    execute $ex$
      do $$
      declare x int = 0
================================

      $$
================================

    $ex$
================================

  exception when division_by_zero then
    raise notice 'caught division by zero'
================================

$outer$
================================


-- Check variable scoping -- a var is not available in its own or prior
-- default expressions, but it is available in later ones.

do $$
declare x int := x + 1
================================

$$
================================


do $$
declare y int := x + 1
================================
  -- error
        x int := 42
================================

$$
================================


do $$
declare x int := 42
================================

        y int := x + 1
================================

$$
================================


do $$
declare x int := 42
================================

          x int := x + 2
================================

          z int := x * 10
================================

$$
================================

  q1 bigint := 42
================================

$$ language plpgsql
================================

  q1 bigint := 42
================================

$$ language plpgsql
================================

  q1 bigint := 42
================================

$$ language plpgsql
================================

  return forward
================================

  return return
================================

  comment on function unreserved_test() is 'this is a test'
================================

  return comment
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

$$ language plpgsql
================================


create type xy_tuple AS (x int, y int)
================================

$$ language plpgsql
================================
 y int
================================

$$ language plpgsql
================================

$$ language plpgsql
================================

drop type xy_tuple
================================

  r.ar[2] := 'replace'
================================

  return r.ar
================================

end$$
================================

  res[2] := x3
================================

  return res
================================

end$$
================================
 return r
================================

$$ stable
================================

$$ stable
================================


do $$
declare a int[] := array[1,2]
================================

  raise notice 'a = %', a
================================

end$$
================================

  raise notice '***%***', _context
================================

  -- lets do it again, just for fun..
  get diagnostics _context = pg_context
================================

  raise notice '***%***', _context
================================

  raise notice 'lets make sure we didnt break anything'
================================

  return 2 * $1
================================

$$ language plpgsql
================================

  myresult := inner_func($1)
================================

  raise notice 'inner_func() done'
================================

  return myresult
================================

$$ language plpgsql
================================

  myresult := outer_func($1)
================================

  raise notice 'outer_func() done'
================================

  return myresult
================================

$$ language plpgsql
================================

  sx int := 5
================================

  exception
    when division_by_zero then
      get diagnostics _context = pg_context
================================

      raise notice '***%***', _context
================================


  -- lets do it again, just for fun..
  get diagnostics _context = pg_context
================================

  raise notice '***%***', _context
================================

  raise notice 'lets make sure we didnt break anything'
================================

  return 2 * $1
================================

$$ language plpgsql
================================

  myresult := inner_func($1)
================================

  raise notice 'inner_func() done'
================================

  return myresult
================================

$$ language plpgsql
================================

  myresult := outer_func($1)
================================

  raise notice 'outer_func() done'
================================

  return myresult
================================

$$ language plpgsql
================================


--
-- Test ASSERT
--

do $$
begin
  assert 1=1
================================

$$
================================


do $$
begin
  assert 1=0
================================

$$
================================


do $$
begin
  assert NULL
================================

$$
================================

do $$
begin
  assert 1=0
================================

$$
================================


-- test custom message
do $$
declare var text := 'some value'
================================

$$
================================


-- ensure assertions are not trapped by 'others'
do $$
begin
  assert 1=0, 'unhandled assertion'
================================

exception when others then
  null
================================

$$
================================


do $$
declare v_test plpgsql_domain
================================

$$
================================


do $$
declare v_test plpgsql_domain := 1
================================

$$
================================


do $$
declare v_test plpgsql_arr_domain
================================

  v_test := v_test || 2
================================

$$
================================


do $$
declare v_test plpgsql_arr_domain := array[1]
================================

$$
================================

  l text
================================

  FOR l IN EXECUTE
           $q$
             EXPLAIN (TIMING off, COSTS off, VERBOSE on)
             SELECT * FROM newtable
           $q$ LOOP
    t = t || l || E'\n'
================================


  RAISE INFO '%', t
================================

  RETURN new
================================

$$
================================

  l text
================================

  FOR l IN EXECUTE
           $q$
             EXPLAIN (TIMING off, COSTS off, VERBOSE on)
             SELECT * FROM oldtable ot FULL JOIN newtable nt USING (id)
           $q$ LOOP
    t = t || l || E'\n'
================================


  RAISE INFO '%', t
================================

  RETURN new
================================

$$
================================

    IF FOUND THEN
      RAISE EXCEPTION 'RI error'
================================

    RETURN NULL
================================

$$
================================

    IF FOUND THEN
      RAISE EXCEPTION 'RI error'
================================

    RETURN NULL
================================

$$
================================

    IF FOUND THEN
      RAISE EXCEPTION 'RI error'
================================

    RETURN NULL
================================

$$
================================


-- create initial test data
INSERT INTO transition_table_level1 (level1_no)
  SELECT generate_series(1,200)
================================


INSERT INTO transition_table_level2 (level2_no, parent_no)
  SELECT level2_no, level2_no / 50 + 1 AS parent_no
    FROM generate_series(1,9999) level2_no
================================


INSERT INTO transition_table_status (level, node_no, status)
  SELECT 1, level1_no, 0 FROM transition_table_level1
================================


INSERT INTO transition_table_status (level, node_no, status)
  SELECT 2, level2_no, 0 FROM transition_table_level2
================================


INSERT INTO transition_table_level1(level1_no)
  SELECT generate_series(201,1000)
================================

    RETURN NULL
================================

$$
================================


DROP TRIGGER transition_table_level2_bad_usage_trigger
  ON transition_table_level2
================================

  RAISE NOTICE 'one = %', (SELECT 1 FROM alter_table_under_transition_tables LIMIT 1)
================================

  RETURN NULL
================================

$$
================================

    RAISE NOTICE 'count union = %',
      (SELECT COUNT(*)
       FROM (SELECT * FROM new_test UNION ALL SELECT * FROM new_test) ss)
================================

    RETURN NULL
================================

END$$
================================

    result partitioned_table%ROWTYPE
================================

    RETURN result
================================
 $$ LANGUAGE plpgsql
================================

    a_val partitioned_table.a%TYPE
================================

        RETURN NEXT a_val
================================

    RETURN
================================
 $$ LANGUAGE plpgsql
================================

  RETURN
================================
 $$ LANGUAGE plpgsql
================================


================================


select '"\b\f\r\n\t\v\"\''\\"'::jsonpath
================================


================================


--
-- only the 'e' array is 0-based, the others are 1-based.
--

INSERT INTO arrtest (a[1:5], b[1:1][1:2][1:2], c, d, f, g)
   VALUES ('{1,2,3,4,5}', '{{{0,0},{1,2}}}', '{}', '{}', '{}', '{}')
================================


UPDATE arrtest SET e[0] = '1.1'
================================


UPDATE arrtest SET e[1] = '2.2'
================================


INSERT INTO arrtest (a, b[1:2][1:2], c, d, e, f, g)
   VALUES ('{11,12,23}', '{{3,4},{4,5}}', '{"foobar"}',
           '{{"elt1", "elt2"}}', '{"3.4", "6.7"}',
           '{"abc","abcde"}', '{"abc","abcde"}')
================================


INSERT INTO arrtest (a, b[1:2], c, d[1:2])
   VALUES ('{}', '{3,4}', '{foo,bar}', '{bar,foo}')
================================


INSERT INTO arrtest (b[2]) VALUES(now())
================================
  -- error, type mismatch

INSERT INTO arrtest (b[1:2]) VALUES(now())
================================


SELECT arrtest.a[1],
          arrtest.b[1][1][1],
          arrtest.c[1],
          arrtest.d[1][1],
          arrtest.e[0]
   FROM arrtest
================================


SELECT a[1], b[1][1][1], c[1], d[1][1], e[0]
   FROM arrtest
================================


SELECT a[1:3],
          b[1:1][1:2][1:2],
          c[1:2],
          d[1:1][1:2]
   FROM arrtest
================================


-- returns nothing
SELECT *
   FROM arrtest
   WHERE a[1] < 5 and
         c = '{"foobar"}'::_name
================================


UPDATE arrtest
  SET a[1:2] = '{16,25}'
  WHERE NOT a = '{}'::_int2
================================


UPDATE arrtest
  SET b[1:1][1:1][1:2] = '{113, 117}',
      b[1:1][1:2][2:2] = '{142, 147}'
  WHERE array_dims(b) = '[1:1][1:2][1:2]'
================================


UPDATE arrtest
  SET c[2:2] = '{"new_word"}'
  WHERE array_dims(c) is not null
================================


SELECT a[1:3],
          b[1:1][1:2][1:2],
          c[1:2],
          d[1:1][2:2]
   FROM arrtest
================================


SELECT b[1:1][2][2],
       d[1:1][2]
   FROM arrtest
================================

UPDATE arrtest SET a[4] = NULL WHERE a[2] IS NULL
================================

SELECT a FROM arrtest WHERE a[2] IS NULL
================================

DELETE FROM arrtest WHERE a[2] IS NULL AND b IS NULL
================================

-- NULL index in assignment is an error
UPDATE arrtest
  SET c[NULL] = '{"can''t assign"}'
  WHERE array_dims(c) is not null
================================

UPDATE arrtest
  SET c[NULL:1] = '{"can''t assign"}'
  WHERE array_dims(c) is not null
================================

UPDATE arrtest
  SET c[1:NULL] = '{"can''t assign"}'
  WHERE array_dims(c) is not null
================================

SELECT a[:3], b[:2][:2] FROM arrtest_s
================================

SELECT a[2:], b[2:][2:] FROM arrtest_s
================================

SELECT a[:], b[:] FROM arrtest_s
================================


-- updates
UPDATE arrtest_s SET a[:3] = '{11, 12, 13}', b[:2][:2] = '{{11,12}, {14,15}}'
  WHERE array_lower(a,1) = 1
================================

UPDATE arrtest_s SET a[3:] = '{23, 24, 25}', b[2:][2:] = '{{25,26}, {28,29}}'
================================

UPDATE arrtest_s SET a[:] = '{11, 12, 13, 14, 15}'
================================

UPDATE arrtest_s SET a[:] = '{23, 24, 25}'
================================

UPDATE arrtest_s SET a[:] = '{11, 12, 13, 14, 15}'
================================
  -- fail, no good with null

-- check with fixed-length-array type, such as point
SELECT f1[0:1] FROM POINT_TBL
================================

SELECT f1[0:] FROM POINT_TBL
================================

SELECT f1[:1] FROM POINT_TBL
================================

SELECT f1[:] FROM POINT_TBL
================================


-- subscript assignments to fixed-width result in NULL if previous value is NULL
UPDATE point_tbl SET f1[0] = 10 WHERE f1 IS NULL RETURNING *
================================

INSERT INTO point_tbl(f1[0]) VALUES(0) RETURNING *
================================

-- NULL assignments get ignored
UPDATE point_tbl SET f1[0] = NULL WHERE f1::text = '(10,10)'::point::text RETURNING *
================================

-- but non-NULL subscript assignments work
UPDATE point_tbl SET f1[0] = -10, f1[1] = -10 WHERE f1::text = '(10,10)'::point::text RETURNING *
================================

-- but not to expand the range
UPDATE point_tbl SET f1[3] = 10 WHERE f1::text = '(-10,-10)'::point::text RETURNING *
================================

update arrtest1 set i[2] = 22, t[2] = 'twenty-two'
================================

update arrtest1 set i[5] = 5, t[5] = 'five'
================================

update arrtest1 set i[8] = 8, t[8] = 'eight'
================================

update arrtest1 set i[0] = 0, t[0] = 'zero'
================================

update arrtest1 set i[-3] = -3, t[-3] = 'minus-three'
================================

update arrtest1 set i[0:2] = array[10,11,12], t[0:2] = array['ten','eleven','twelve']
================================

update arrtest1 set i[8:10] = array[18,null,20], t[8:10] = array['p18',null,'p20']
================================

update arrtest1 set i[11:12] = array[null,22], t[11:12] = array[null,'p22']
================================

update arrtest1 set i[15:16] = array[null,26], t[15:16] = array[null,'p26']
================================

update arrtest1 set i[-5:-3] = array[-15,-14,-13], t[-5:-3] = array['m15','m14','m13']
================================

update arrtest1 set i[-7:-6] = array[-17,null], t[-7:-6] = array['m17',null]
================================

update arrtest1 set i[-12:-10] = array[-22,null,-20], t[-12:-10] = array['m22',null,'m20']
================================

update arrtest1 set i[0:5] = array[0,1,2,null,4,5], t[0:5] = array['z','p1','p2',null,'p4','p5']
================================


-- expressions
SELECT t.f[1][3][1] AS "131", t.f[2][2][1] AS "221" FROM (
  SELECT ARRAY[[[111,112],[121,122],[131,132]],[[211,212],[221,122],[231,232]]] AS f
) AS t
================================


DO $$
DECLARE
  o int
================================

  a int[] := ARRAY[1,2,3,2,3,1,2]
================================

  WHILE o IS NOT NULL
  LOOP
    RAISE NOTICE '%', o
================================

    o := array_position(a, 2, o + 1)
================================

insert into arr_pk_tbl values (1, '{3,4,5}') on conflict (pk)
  do update set f1[1] = excluded.f1[1], f1[3] = excluded.f1[3]
  returning pk, f1
================================

insert into arr_pk_tbl(pk, f1[1:2]) values (1, '{6,7,8}') on conflict (pk)
  do update set f1[1] = excluded.f1[1],
    f1[2] = excluded.f1[2],
    f1[3] = excluded.f1[3]
  returning pk, f1
================================
 -- f
select 'foo' ilike any (array['%A', '%O'])
================================
 -- t
select 'foo' ilike all (array['F%', '%O'])
================================


-- A few simple tests for arrays of composite types

create type comptype as (f1 int, f2 text)
================================


-- check that implicitly named array type _comptype isn't a problem
create type _comptype as enum('fooey')
================================


drop type _comptype
================================

drop type comptype
================================

$$ language sql immutable
================================

$$ language sql immutable
================================

update t1 set f1[5].q2 = 43
================================

insert into src
  select string_agg(random()::text,'') from generate_series(1,10000)
================================

create type textandtext as (c1 text, c2 text)
================================

insert into dest select array[row(f1,f1)::textandtext] from src
================================

select length(md5((f1[1]).c2)) from dest
================================

select length(md5((f1[1]).c2)) from dest
================================

select length(md5((f1[1]).c2)) from dest
================================

drop type textandtext
================================
 -- fail

================================
--
-- Enum tests
--

CREATE TYPE rainbow AS ENUM ('red', 'orange', 'yellow', 'green', 'blue', 'purple')
================================


--
-- adding new values
--

CREATE TYPE planets AS ENUM ( 'venus', 'earth', 'mars' )
================================


ALTER TYPE planets ADD VALUE 'uranus'
================================


ALTER TYPE planets ADD VALUE 'mercury' BEFORE 'venus'
================================

ALTER TYPE planets ADD VALUE 'saturn' BEFORE 'uranus'
================================

ALTER TYPE planets ADD VALUE 'jupiter' AFTER 'mars'
================================

ALTER TYPE planets ADD VALUE 'neptune' AFTER 'uranus'
================================


-- errors for adding labels
ALTER TYPE planets ADD VALUE
  'plutoplutoplutoplutoplutoplutoplutoplutoplutoplutoplutoplutoplutopluto'
================================


ALTER TYPE planets ADD VALUE 'pluto' AFTER 'zeus'
================================


-- if not exists tests

--  existing value gives error
ALTER TYPE planets ADD VALUE 'mercury'
================================


-- unless IF NOT EXISTS is specified
ALTER TYPE planets ADD VALUE IF NOT EXISTS 'mercury'
================================


ALTER TYPE planets ADD VALUE IF NOT EXISTS 'pluto'
================================


--
-- Test inserting so many values that we have to renumber
--

create type insenum as enum ('L1', 'L2')
================================


alter type insenum add value 'i1' before 'L2'
================================

alter type insenum add value 'i2' before 'L2'
================================

alter type insenum add value 'i3' before 'L2'
================================

alter type insenum add value 'i4' before 'L2'
================================

alter type insenum add value 'i5' before 'L2'
================================

alter type insenum add value 'i6' before 'L2'
================================

alter type insenum add value 'i7' before 'L2'
================================

alter type insenum add value 'i8' before 'L2'
================================

alter type insenum add value 'i9' before 'L2'
================================

alter type insenum add value 'i10' before 'L2'
================================

alter type insenum add value 'i11' before 'L2'
================================

alter type insenum add value 'i12' before 'L2'
================================

alter type insenum add value 'i13' before 'L2'
================================

alter type insenum add value 'i14' before 'L2'
================================

alter type insenum add value 'i15' before 'L2'
================================

alter type insenum add value 'i16' before 'L2'
================================

alter type insenum add value 'i17' before 'L2'
================================

alter type insenum add value 'i18' before 'L2'
================================

alter type insenum add value 'i19' before 'L2'
================================

alter type insenum add value 'i20' before 'L2'
================================

alter type insenum add value 'i21' before 'L2'
================================

alter type insenum add value 'i22' before 'L2'
================================

alter type insenum add value 'i23' before 'L2'
================================

alter type insenum add value 'i24' before 'L2'
================================

alter type insenum add value 'i25' before 'L2'
================================

alter type insenum add value 'i26' before 'L2'
================================

alter type insenum add value 'i27' before 'L2'
================================

alter type insenum add value 'i28' before 'L2'
================================

alter type insenum add value 'i29' before 'L2'
================================

alter type insenum add value 'i30' before 'L2'
================================


-- The exact values of enumsortorder will now depend on the local properties
-- of float4, but in any reasonable implementation we should get at least
-- 20 splits before having to renumber
================================
 so only hide values > 20.

SELECT enumlabel,
       case when enumsortorder > 20 then null else enumsortorder end as so
FROM pg_enum
WHERE enumtypid = 'insenum'::regtype
ORDER BY enumsortorder
================================

blue
purple
\.
SELECT * FROM enumtest
================================
  -- fail
--
-- cross-type RI should fail
--
CREATE TYPE bogus AS ENUM('good', 'bad', 'ugly')
================================

DROP TYPE bogus
================================


-- check renaming a value
ALTER TYPE rainbow RENAME VALUE 'red' TO 'crimson'
================================

-- check that renaming a non-existent value fails
ALTER TYPE rainbow RENAME VALUE 'red' TO 'crimson'
================================

-- check that renaming to an existent value fails
ALTER TYPE rainbow RENAME VALUE 'blue' TO 'green'
================================


--
-- check transactional behaviour of ALTER TYPE ... ADD VALUE
--
CREATE TYPE bogus AS ENUM('good')
================================

ALTER TYPE bogus ADD VALUE 'new'
================================


-- check that we recognize the case where the enum already existed but was
-- modified in the current txn
================================
 this should not be considered safe
BEGIN
================================

ALTER TYPE bogus RENAME TO bogon
================================

ALTER TYPE bogon ADD VALUE 'bad'
================================

ALTER TYPE bogus RENAME VALUE 'good' to 'bad'
================================


DROP TYPE bogus
================================

CREATE TYPE bogus AS ENUM('good','bad','ugly')
================================

ALTER TYPE bogus RENAME TO bogon
================================


-- ideally, we'd allow this usage
================================
 but it requires keeping track of whether
-- the enum type was created in the current transaction, which is expensive
BEGIN
================================

CREATE TYPE bogus AS ENUM('good')
================================

ALTER TYPE bogus RENAME TO bogon
================================

ALTER TYPE bogon ADD VALUE 'bad'
================================

ALTER TYPE bogon ADD VALUE 'ugly'
================================

DROP TYPE rainbow
================================


================================

CREATE GROUP regress_dep_group
================================

DROP GROUP regress_dep_group
================================

DROP GROUP regress_dep_group
================================


-- can't drop the owner of an object
-- the error message detail here would include a pg_toast_nnn name that
-- is not constant, so suppress it
\set VERBOSITY terse
ALTER TABLE deptest OWNER TO regress_dep_user3
================================

\set VERBOSITY default

-- if we drop the object, we can drop the user too
DROP TABLE deptest
================================

-- permission denied
DROP OWNED BY regress_dep_user1
================================

DROP OWNED BY regress_dep_user0, regress_dep_user2
================================

REASSIGN OWNED BY regress_dep_user0 TO regress_dep_user1
================================

REASSIGN OWNED BY regress_dep_user1 TO regress_dep_user0
================================

-- this one is allowed
DROP OWNED BY regress_dep_user0
================================

\z deptest1

DROP OWNED BY regress_dep_user1
================================

-- all grants revoked
\z deptest1
-- table was dropped
\d deptest

-- Test REASSIGN OWNED
GRANT ALL ON deptest1 TO regress_dep_user1
================================

CREATE SCHEMA deptest
================================

ALTER DEFAULT PRIVILEGES FOR ROLE regress_dep_user1 IN SCHEMA deptest
  GRANT ALL ON TABLES TO regress_dep_user2
================================
 $$
================================

CREATE TYPE deptest_enum AS ENUM ('red')
================================

CREATE TYPE deptest_range AS RANGE (SUBTYPE = int4)
================================


-- When reassigning ownership of a composite type, its pg_class entry
-- should match
CREATE TYPE deptest_t AS (a int)
================================

REASSIGN OWNED BY regress_dep_user1 TO regress_dep_user2
================================

\dt deptest

SELECT typowner = relowner
FROM pg_type JOIN pg_class c ON typrelid = c.oid WHERE typname = 'deptest_t'
================================

DROP OWNED BY regress_dep_user1
================================

DROP OWNED BY regress_dep_user2, regress_dep_user0
================================


================================
--
-- CREATE_TYPE
--

--
-- Note: widget_in/out were created in create_function_1, without any
-- prior shell-type creation.  These commands therefore complete a test
-- of the "old style" approach of making the functions first.
--
CREATE TYPE widget (
   internallength = 24,
   input = widget_in,
   output = widget_out,
   typmod_in = numerictypmodin,
   typmod_out = numerictypmodout,
   alignment = double
)
================================


CREATE TYPE city_budget (
   internallength = 16,
   input = int44in,
   output = int44out,
   element = int4,
   category = 'x',   -- just to verify the system will take it
   preferred = true  -- ditto
)
================================


-- Test creation and destruction of shell types
CREATE TYPE shell
================================

CREATE TYPE shell
================================
   -- fail, type already present
DROP TYPE shell
================================

DROP TYPE shell
================================
     -- fail, type not exist

-- also, let's leave one around for purposes of pg_dump testing
CREATE TYPE myshell
================================


--
-- Test type-related default values (broken in releases before PG 7.2)
--
-- This part of the test also exercises the "new style" approach of making
-- a shell type and then filling it in.
--
CREATE TYPE int42
================================

CREATE TYPE text_w_default
================================


CREATE TYPE int42 (
   internallength = 4,
   input = int42_in,
   output = int42_out,
   alignment = int4,
   default = 42,
   passedbyvalue
)
================================


CREATE TYPE text_w_default (
   internallength = variable,
   input = text_w_default_in,
   output = text_w_default_out,
   alignment = int4,
   default = 'zippo'
)
================================


INSERT INTO default_test DEFAULT VALUES
================================


-- We need a shell type to test some CREATE TYPE failure cases with
CREATE TYPE bogus_type
================================


-- invalid: non-lowercase quoted identifiers
CREATE TYPE bogus_type (
	"Internallength" = 4,
	"Input" = int42_in,
	"Output" = int42_out,
	"Alignment" = int4,
	"Default" = 42,
	"Passedbyvalue"
)
================================


-- invalid: input/output function incompatibility
CREATE TYPE bogus_type (INPUT = array_in,
    OUTPUT = array_out,
    ELEMENT = int,
    INTERNALLENGTH = 32)
================================


DROP TYPE bogus_type
================================


-- It no longer is possible to issue CREATE TYPE without making a shell first
CREATE TYPE bogus_type (INPUT = array_in,
    OUTPUT = array_out,
    ELEMENT = int,
    INTERNALLENGTH = 32)
================================


-- Test stand-alone composite type

CREATE TYPE default_test_row AS (f1 text_w_default, f2 int42)
================================


CREATE FUNCTION get_default_test() RETURNS SETOF default_test_row AS '
  SELECT * FROM default_test
================================

' LANGUAGE SQL
================================


-- Test comments
COMMENT ON TYPE bad IS 'bad comment'
================================

COMMENT ON TYPE default_test_row IS 'good comment'
================================

COMMENT ON TYPE default_test_row IS NULL
================================

COMMENT ON COLUMN default_test_row.nope IS 'bad comment'
================================

COMMENT ON COLUMN default_test_row.f1 IS 'good comment'
================================

COMMENT ON COLUMN default_test_row.f1 IS NULL
================================


-- Check shell type create for existing types
CREATE TYPE text_w_default
================================
		-- should fail

DROP TYPE default_test_row CASCADE
================================


-- Check dependencies are established when creating a new type
CREATE TYPE base_type
================================

CREATE TYPE base_type(INPUT = base_fn_in, OUTPUT = base_fn_out)
================================
 -- error
DROP TYPE base_type
================================
 -- error
DROP TYPE base_type CASCADE
================================


--
-- Test CREATE/ALTER TYPE using a type that's compatible with varchar,
-- so we can re-use those support functions
--
CREATE TYPE myvarchar
================================


CREATE FUNCTION myvarcharsend(myvarchar) RETURNS bytea
LANGUAGE internal STABLE PARALLEL SAFE STRICT AS 'varcharsend'
================================


-- fail, it's still a shell:
ALTER TYPE myvarchar SET (storage = extended)
================================


CREATE TYPE myvarchar (
    input = myvarcharin,
    output = myvarcharout,
    alignment = integer,
    storage = main
)
================================


ALTER TYPE myvarchar SET (storage = plain)
================================
  -- not allowed

ALTER TYPE myvarchar SET (storage = extended)
================================


ALTER TYPE myvarchar SET (
    send = myvarcharsend,
    receive = myvarcharrecv,
    typmod_in = varchartypmodin,
    typmod_out = varchartypmodout,
    -- these are bogus, but it's safe as long as we don't use the type:
    analyze = ts_typanalyze,
    subscript = raw_array_subscript_handler
)
================================
  -- fail
DROP TYPE myvarchar
================================
  -- fail

DROP TYPE myvarchar CASCADE
================================


================================


INSERT INTO BIT_TABLE VALUES (B'10')
================================
 -- too short
INSERT INTO BIT_TABLE VALUES (B'00000000000')
================================

INSERT INTO BIT_TABLE VALUES (B'11011000000')
================================

INSERT INTO BIT_TABLE VALUES (B'01010101010')
================================

INSERT INTO BIT_TABLE VALUES (B'101011111010')
================================
 -- too long
--INSERT INTO BIT_TABLE VALUES ('X554')
================================

--INSERT INTO BIT_TABLE VALUES ('X555')
================================

INSERT INTO VARBIT_TABLE VALUES (B'0')
================================

INSERT INTO VARBIT_TABLE VALUES (B'010101')
================================

INSERT INTO VARBIT_TABLE VALUES (B'01010101010')
================================

INSERT INTO VARBIT_TABLE VALUES (B'101011111010')
================================
 -- too long
--INSERT INTO VARBIT_TABLE VALUES ('X554')
================================

--INSERT INTO VARBIT_TABLE VALUES ('X555')
================================

SELECT SUBSTRING('01010101'::varbit FROM 2 FOR 2147483646) AS "1010101"
================================

SELECT SUBSTRING('01010101'::varbit FROM -10 FOR 2147483646) AS "01010101"
================================

SELECT SUBSTRING('01010101'::varbit FROM -10 FOR -2147483646) AS "error"
================================

X0F	X10
X1F	X11
X2F	X12
X3F	X13
X8F	X04
X000F	X0010
X0123	XFFFF
X2468	X2468
XFA50	X05AF
X1234	XFFF5
\.

SELECT a, b, ~a AS "~ a", a & b AS "a & b",
       a | b AS "a | b", a # b AS "a # b" FROM varbit_table
================================

SELECT a,a<<4 AS "a<<4",b,b>>2 AS "b>>2" FROM varbit_table
================================

X0F00	X1000
X1F00	X1100
X2F00	X1200
X3F00	X1300
X8F00	X0400
X000F	X0010
X0123	XFFFF
X2468	X2468
XFA50	X05AF
X1234	XFFF5
\.

SELECT a,b,~a AS "~ a",a & b AS "a & b",
	a|b AS "a | b", a # b AS "a # b" FROM bit_table
================================

SELECT a,a<<4 AS "a<<4",b,b>>2 AS "b>>2" FROM bit_table
================================


-- More position tests, checking all the boundary cases
SELECT POSITION(B'1010' IN B'0000101')
================================
   -- 0
SELECT POSITION(B'1010' IN B'00001010')
================================
  -- 5
SELECT POSITION(B'1010' IN B'00000101')
================================
  -- 0
SELECT POSITION(B'1010' IN B'000001010')
================================
  -- 6

SELECT POSITION(B'' IN B'00001010')
================================
  -- 1
SELECT POSITION(B'0' IN B'')
================================
  -- 0
SELECT POSITION(B'101101' IN B'001011011011011000')
================================
  -- 3
SELECT POSITION(B'10110110' IN B'001011011011010')
================================
  -- 3
SELECT POSITION(B'1011011011011' IN B'001011011011011')
================================
  -- 3
SELECT POSITION(B'1011011011011' IN B'00001011011011011')
================================
  -- 5

SELECT POSITION(B'11101011' IN B'11101011')
================================
 -- 1
SELECT POSITION(B'11101011' IN B'011101011')
================================
 -- 2
SELECT POSITION(B'11101011' IN B'00011101011')
================================
 -- 4
SELECT POSITION(B'11101011' IN B'0000011101011')
================================
 -- 6

SELECT POSITION(B'111010110' IN B'111010110')
================================
 -- 1
SELECT POSITION(B'111010110' IN B'0111010110')
================================
 -- 2
SELECT POSITION(B'111010110' IN B'000111010110')
================================
 -- 4
SELECT POSITION(B'111010110' IN B'00000111010110')
================================
 -- 6

SELECT POSITION(B'111010110' IN B'11101011')
================================
 -- 0
SELECT POSITION(B'111010110' IN B'011101011')
================================
 -- 0
SELECT POSITION(B'111010110' IN B'00011101011')
================================
 -- 0
SELECT POSITION(B'111010110' IN B'0000011101011')
================================
 -- 0

SELECT POSITION(B'111010110' IN B'111010110')
================================
 -- 1
SELECT POSITION(B'111010110' IN B'0111010110')
================================
 -- 2
SELECT POSITION(B'111010110' IN B'000111010110')
================================
 -- 4
SELECT POSITION(B'111010110' IN B'00000111010110')
================================
 -- 6

SELECT POSITION(B'111010110' IN B'000001110101111101011')
================================
 -- 0
SELECT POSITION(B'111010110' IN B'0000001110101111101011')
================================
 -- 0
SELECT POSITION(B'111010110' IN B'000000001110101111101011')
================================
 -- 0
SELECT POSITION(B'111010110' IN B'00000000001110101111101011')
================================
 -- 0

SELECT POSITION(B'111010110' IN B'0000011101011111010110')
================================
 -- 14
SELECT POSITION(B'111010110' IN B'00000011101011111010110')
================================
 -- 15
SELECT POSITION(B'111010110' IN B'0000000011101011111010110')
================================
 -- 17
SELECT POSITION(B'111010110' IN B'000000000011101011111010110')
================================
 -- 19

SELECT POSITION(B'000000000011101011111010110' IN B'000000000011101011111010110')
================================
 -- 1
SELECT POSITION(B'00000000011101011111010110' IN B'000000000011101011111010110')
================================
 -- 2
SELECT POSITION(B'0000000000011101011111010110' IN B'000000000011101011111010110')
================================

INSERT INTO BIT_SHIFT_TABLE VALUES (B'1101100000000000')
================================

INSERT INTO BIT_SHIFT_TABLE SELECT b>>1 FROM BIT_SHIFT_TABLE
================================

INSERT INTO BIT_SHIFT_TABLE SELECT b>>2 FROM BIT_SHIFT_TABLE
================================

INSERT INTO BIT_SHIFT_TABLE SELECT b>>4 FROM BIT_SHIFT_TABLE
================================

INSERT INTO BIT_SHIFT_TABLE SELECT b>>8 FROM BIT_SHIFT_TABLE
================================

SELECT POSITION(B'1101' IN b),
       POSITION(B'11011' IN b),
       b
       FROM BIT_SHIFT_TABLE 
================================

SELECT b, b >> 1 AS bsr, b << 1 AS bsl
       FROM BIT_SHIFT_TABLE 
================================

SELECT b, b >> 8 AS bsr8, b << 8 AS bsl8
       FROM BIT_SHIFT_TABLE 
================================

SELECT b::bit(15), b::bit(15) >> 1 AS bsr, b::bit(15) << 1 AS bsl
       FROM BIT_SHIFT_TABLE 
================================

SELECT b::bit(15), b::bit(15) >> 8 AS bsr8, b::bit(15) << 8 AS bsl8
       FROM BIT_SHIFT_TABLE 
================================

INSERT INTO VARBIT_SHIFT_TABLE VALUES (B'11011')
================================

INSERT INTO VARBIT_SHIFT_TABLE SELECT CAST(v || B'0' AS BIT VARYING(6)) >>1 FROM VARBIT_SHIFT_TABLE
================================

INSERT INTO VARBIT_SHIFT_TABLE SELECT CAST(v || B'00' AS BIT VARYING(8)) >>2 FROM VARBIT_SHIFT_TABLE
================================

INSERT INTO VARBIT_SHIFT_TABLE SELECT CAST(v || B'0000' AS BIT VARYING(12)) >>4 FROM VARBIT_SHIFT_TABLE
================================

INSERT INTO VARBIT_SHIFT_TABLE SELECT CAST(v || B'00000000' AS BIT VARYING(20)) >>8 FROM VARBIT_SHIFT_TABLE
================================

SELECT POSITION(B'1101' IN v),
       POSITION(B'11011' IN v),
       v
       FROM VARBIT_SHIFT_TABLE 
================================

SELECT v, v >> 1 AS vsr, v << 1 AS vsl
       FROM VARBIT_SHIFT_TABLE 
================================

SELECT v, v >> 8 AS vsr8, v << 8 AS vsl8
       FROM VARBIT_SHIFT_TABLE 
================================


-- Get/Set bit
SELECT get_bit(B'0101011000100', 10)
================================

SELECT set_bit(B'0101011000100100', 15, 1)
================================

SELECT set_bit(B'0101011000100100', 16, 1)
================================
	-- fail

-- Overlay
SELECT overlay(B'0101011100' placing '001' from 2 for 3)
================================

SELECT overlay(B'0101011100' placing '101' from 6)
================================

SELECT overlay(B'0101011100' placing '001' from 11)
================================

SELECT overlay(B'0101011100' placing '001' from 20)
================================


-- bit_count
SELECT bit_count(B'0101011100'::bit(10))
================================

SELECT bit_count(B'1111111111'::bit(10))
================================


-- This table is intentionally left around to exercise pg_dump/pg_upgrade
CREATE TABLE bit_defaults(
  b1 bit(4) DEFAULT '1001',
  b2 bit(4) DEFAULT B'0101',
  b3 bit varying(5) DEFAULT '1001',
  b4 bit varying(5) DEFAULT B'0101'
)
================================

\d bit_defaults
INSERT INTO bit_defaults DEFAULT VALUES
================================


================================


CREATE SCHEMA temp_func_test
================================

CREATE FUNCTION functest_S_2(a text[]) RETURNS int
    RETURN a[1]::int
================================

CREATE FUNCTION functest_S_3() RETURNS boolean
    RETURN false
================================

CREATE FUNCTION functest_S_3a() RETURNS boolean
    BEGIN ATOMIC
        
================================

================================
RETURN false
================================

================================


CREATE FUNCTION functest_S_13() RETURNS boolean
    BEGIN ATOMIC
        SELECT 1
================================


CREATE FUNCTION functest_S_14() RETURNS bigint
    RETURN (SELECT count(*) FROM functestv3)
================================


CREATE FUNCTION functest_sri1() RETURNS SETOF int
LANGUAGE SQL
STABLE
AS '
    SELECT * FROM functest3
================================

'
================================


-- Cleanup
DROP SCHEMA temp_func_test CASCADE
================================


================================

/* This is an example of SQL which should not execute:
 * select 'multi-line'
================================

 */
SELECT 'after multi-line' AS fifth
================================


--
-- Nested comments
--

/*
SELECT 'trailing' as x1
================================
 -- inside block comment
*/

/* This block comment surrounds a query which itself has a block comment...
SELECT /* embedded single line */ 'embedded' AS x2
================================

*/

SELECT -- continued after the following block comments...
/* Deeply nested comment.
   This includes a single apostrophe to make sure we aren't decoding this part as a string.
SELECT 'deep nest' AS n1
================================

/* Second level of nesting...
SELECT 'deeper nest' as n2
================================

/* Third level of nesting...
SELECT 'deepest nest' as n3
================================

*/
Hoo boy. Still two deep...
*/
Now just one deep...
*/
'deeply nested example' AS sixth
================================


/* and this is the end of the file */

================================


-- absolute value
SELECT f.f1, @f.f1 AS abs_f1
   FROM FLOAT8_TBL f
================================


-- square root
SELECT sqrt(float8 '64') AS eight
================================


SELECT f.f1, |/f.f1 AS sqrt_f1
   FROM FLOAT8_TBL f
   WHERE f.f1 > '0.0'
================================


-- power
SELECT power(float8 '144', float8 '0.5')
================================

SELECT power(float8 'NaN', float8 '0.5')
================================

SELECT power(float8 '144', float8 'NaN')
================================

SELECT power(float8 'NaN', float8 'NaN')
================================

SELECT power(float8 '-1', float8 'NaN')
================================

SELECT power(float8 '1', float8 'NaN')
================================

SELECT power(float8 'NaN', float8 '0')
================================

SELECT power(float8 'inf', float8 '0')
================================

SELECT power(float8 '-inf', float8 '0')
================================

SELECT power(float8 '0', float8 'inf')
================================

SELECT power(float8 '0', float8 '-inf')
================================

SELECT power(float8 '1', float8 'inf')
================================

SELECT power(float8 '1', float8 '-inf')
================================

SELECT power(float8 '-1', float8 'inf')
================================

SELECT power(float8 '-1', float8 '-inf')
================================

SELECT power(float8 '0.1', float8 'inf')
================================

SELECT power(float8 '-0.1', float8 'inf')
================================

SELECT power(float8 '1.1', float8 'inf')
================================

SELECT power(float8 '-1.1', float8 'inf')
================================

SELECT power(float8 '0.1', float8 '-inf')
================================

SELECT power(float8 '-0.1', float8 '-inf')
================================

SELECT power(float8 '1.1', float8 '-inf')
================================

SELECT power(float8 '-1.1', float8 '-inf')
================================

SELECT power(float8 'inf', float8 '-2')
================================

SELECT power(float8 'inf', float8 '2')
================================

SELECT power(float8 'inf', float8 'inf')
================================

SELECT power(float8 'inf', float8 '-inf')
================================

-- Intel's icc misoptimizes the code that controls the sign of this result,
-- even with -mp1.  Pending a fix for that, only test for "is it zero".
SELECT power(float8 '-inf', float8 '-2') = '0'
================================

SELECT power(float8 '-inf', float8 '-3')
================================

SELECT power(float8 '-inf', float8 '2')
================================

SELECT power(float8 '-inf', float8 '3')
================================

SELECT power(float8 '-inf', float8 '3.5')
================================

SELECT power(float8 '-inf', float8 'inf')
================================

SELECT power(float8 '-inf', float8 '-inf')
================================


-- cube root
SELECT ||/ float8 '27' AS three
================================


SELECT f.f1, ||/f.f1 AS cbrt_f1 FROM FLOAT8_TBL f
================================


-- hyperbolic functions
-- we run these with extra_float_digits = 0 too, since different platforms
-- tend to produce results that vary in the last place.
SELECT sinh(float8 '1')
================================

SELECT cosh(float8 '1')
================================

SELECT tanh(float8 '1')
================================

SELECT asinh(float8 '1')
================================

SELECT acosh(float8 '2')
================================

SELECT atanh(float8 '0.5')
================================

-- test Inf/NaN cases for hyperbolic functions
SELECT sinh(float8 'infinity')
================================

SELECT sinh(float8 '-infinity')
================================

SELECT sinh(float8 'nan')
================================

SELECT cosh(float8 'infinity')
================================

SELECT cosh(float8 '-infinity')
================================

SELECT cosh(float8 'nan')
================================

SELECT tanh(float8 'infinity')
================================

SELECT tanh(float8 '-infinity')
================================

SELECT tanh(float8 'nan')
================================

SELECT asinh(float8 'infinity')
================================

SELECT asinh(float8 '-infinity')
================================

SELECT asinh(float8 'nan')
================================

-- acosh(Inf) should be Inf, but some mingw versions produce NaN, so skip test
-- SELECT acosh(float8 'infinity')
================================

SELECT acosh(float8 '-infinity')
================================

SELECT acosh(float8 'nan')
================================

SELECT atanh(float8 'infinity')
================================

SELECT atanh(float8 '-infinity')
================================

SELECT atanh(float8 'nan')
================================


--
-- test output (and round-trip safety) of various values.
-- To ensure we're testing what we think we're testing, start with
-- float values specified by bit patterns (as a useful side effect,
-- this means we'll fail on non-IEEE platforms).

create type xfloat8
================================

create type xfloat8 (input = xfloat8in, output = xfloat8out, like = float8)
================================

create cast (xfloat8 as float8) without function
================================

create cast (float8 as xfloat8) without function
================================

create cast (xfloat8 as bigint) without function
================================

create cast (bigint as xfloat8) without function
================================


-- float8: seeeeeee eeeeeeee eeeeeeee mmmmmmmm mmmmmmmm(x4)

-- we don't care to assume the platform's strtod() handles subnormals
-- correctly
================================
 those are "use at your own risk". However we do test
-- subnormal outputs, since those are under our control.

with testdata(bits) as (values
  -- small subnormals
  (x'0000000000000001'),
  (x'0000000000000002'), (x'0000000000000003'),
  (x'0000000000001000'), (x'0000000100000000'),
  (x'0000010000000000'), (x'0000010100000000'),
  (x'0000400000000000'), (x'0000400100000000'),
  (x'0000800000000000'), (x'0000800000000001'),
  -- these values taken from upstream testsuite
  (x'00000000000f4240'),
  (x'00000000016e3600'),
  (x'0000008cdcdea440'),
  -- borderline between subnormal and normal
  (x'000ffffffffffff0'), (x'000ffffffffffff1'),
  (x'000ffffffffffffe'), (x'000fffffffffffff'))
select float8send(flt) as ibits,
       flt
  from (select bits::bigint::xfloat8::float8 as flt
          from testdata
	offset 0) s
================================


-- clean up, lest opr_sanity complain
drop type xfloat8 cascade
================================


================================


-- Test vacuum-root operation. It gets invoked when the root is also a leaf,
-- i.e. the index is very small.
insert into spgist_point_tbl (id, p)
select g, point(g*10, g*10) from generate_series(1, 10) g
================================


-- Insert more data, to make the index a few levels deep.
insert into spgist_point_tbl (id, p)
select g,      point(g*10, g*10) from generate_series(1, 10000) g
================================

insert into spgist_point_tbl (id, p)
select g+100000, point(g*10+1, g*10+1) from generate_series(1, 10000) g
================================

insert into spgist_box_tbl(b)
select box(point(i,j),point(i+s,j+s))
  from generate_series(1,100,5) i,
       generate_series(1,100,5) j,
       generate_series(1,10) s
================================


select count(*)
  from (values (point(5,5)),(point(8,8)),(point(12,12))) v(p)
 where exists(select * from spgist_box_tbl b where b.b && box(v.p,v.p))
================================


insert into spgist_text_tbl (id, t)
select g, 'f' || repeat('o', 100) || g from generate_series(1, 10000) g
union all
select g, 'baaaaaaaaaaaaaar' || g from generate_series(1, 1000) g
================================


-- Do a lot of insertions that have to split an existing node. Hopefully
-- one of these will cause the page to run out of space, causing the inner
-- tuple to be moved to another page.
insert into spgist_text_tbl (id, t)
select -g, 'f' || repeat('o', 100-g) || 'surprise' from generate_series(1, 100) g
================================

reindex index spgist_point_idx
================================


================================

PREPARE TRANSACTION 'foobar'
================================

PREPARE TRANSACTION 'foobar'
================================

LOCK hs1
================================

LOCK hs1 IN SHARE UPDATE EXCLUSIVE MODE
================================

LOCK hs1 IN SHARE MODE
================================

LOCK hs1 IN SHARE ROW EXCLUSIVE MODE
================================

LOCK hs1 IN EXCLUSIVE MODE
================================

LOCK hs1 IN ACCESS EXCLUSIVE MODE
================================


-- Listen
listen a
================================

notify a
================================


CLUSTER hs2 using hs1_pkey
================================


REINDEX TABLE hs2
================================


================================


INSERT INTO brintest SELECT
	repeat(stringu1, 8)::bytea,
	substr(stringu1, 1, 1)::"char",
	stringu1::name, 142857 * tenthous,
	thousand,
	twothousand,
	repeat(stringu1, 8),
	unique1::oid,
	format('(%s,%s)', tenthous, twenty)::tid,
	(four + 1.0)/(hundred+1),
	odd::float8 / (tenthous + 1),
	format('%s:00:%s:00:%s:00', to_hex(odd), to_hex(even), to_hex(hundred))::macaddr,
	inet '10.2.3.4/24' + tenthous,
	cidr '10.2.3/24' + tenthous,
	substr(stringu1, 1, 1)::bpchar,
	date '1995-08-15' + tenthous,
	time '01:20:30' + thousand * interval '18.5 second',
	timestamp '1942-07-23 03:05:09' + tenthous * interval '36.38 hours',
	timestamptz '1972-10-10 03:00' + thousand * interval '1 hour',
	justify_days(justify_hours(tenthous * interval '12 minutes')),
	timetz '01:30:20+02' + hundred * interval '15 seconds',
	thousand::bit(10),
	tenthous::bit(16)::varbit,
	tenthous::numeric(36,30) * fivethous * even / (hundred + 1),
	format('%s%s-%s-%s-%s-%s%s%s', to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'))::uuid,
	int4range(thousand, twothousand),
	format('%s/%s%s', odd, even, tenthous)::pg_lsn,
	box(point(odd, even), point(thousand, twothousand))
FROM tenk1 ORDER BY unique2 LIMIT 100
================================


-- throw in some NULL's and different values
INSERT INTO brintest (inetcol, cidrcol, int4rangecol) SELECT
	inet 'fe80::6e40:8ff:fea9:8c46' + tenthous,
	cidr 'fe80::6e40:8ff:fea9:8c46' + tenthous,
	'empty'::int4range
FROM tenk1 ORDER BY thousand, tenthous LIMIT 25
================================


DO $x$
DECLARE
	r record
================================

	r2 record
================================

	cond text
================================

	idx_ctids tid[]
================================

	ss_ctids tid[]
================================

	count int
================================

	plan_ok bool
================================

	plan_line text
================================

		ELSE
			cond := format('%I %s %L::%s', r.colname, r.oper, r.value, r.typ)
================================


		plan_ok := false
================================

		FOR plan_line IN EXECUTE format($y$EXPLAIN SELECT array_agg(ctid) FROM brintest WHERE %s $y$, cond) LOOP
			IF plan_line LIKE '%Bitmap Heap Scan on brintest%' THEN
				plan_ok := true
================================

		IF NOT plan_ok THEN
			RAISE WARNING 'did not get bitmap indexscan plan for %', r
================================


		EXECUTE format($y$SELECT array_agg(ctid) FROM brintest WHERE %s $y$, cond)
			INTO idx_ctids
================================


		plan_ok := false
================================

		FOR plan_line IN EXECUTE format($y$EXPLAIN SELECT array_agg(ctid) FROM brintest WHERE %s $y$, cond) LOOP
			IF plan_line LIKE '%Seq Scan on brintest%' THEN
				plan_ok := true
================================

		IF NOT plan_ok THEN
			RAISE WARNING 'did not get seqscan plan for %', r
================================


		EXECUTE format($y$SELECT array_agg(ctid) FROM brintest WHERE %s $y$, cond)
			INTO ss_ctids
================================


		-- make sure both return the same results
		count := array_length(idx_ctids, 1)
================================


		IF NOT (count = array_length(ss_ctids, 1) AND
				idx_ctids @> ss_ctids AND
				idx_ctids <@ ss_ctids) THEN
			-- report the results of each scan to make the differences obvious
			RAISE WARNING 'something not right in %: count %', r, count
================================

			FOR r2 IN EXECUTE 'SELECT ' || r.colname || ' FROM brintest WHERE ' || cond LOOP
				RAISE NOTICE 'seqscan: %', r2
================================

			FOR r2 IN EXECUTE 'SELECT ' || r.colname || ' FROM brintest WHERE ' || cond LOOP
				RAISE NOTICE 'bitmapscan: %', r2
================================


		-- make sure we found expected number of matches
		IF count != r.matches THEN RAISE WARNING 'unexpected number of results % for %', count, r
================================

$x$
================================


INSERT INTO brintest SELECT
	repeat(stringu1, 42)::bytea,
	substr(stringu1, 1, 1)::"char",
	stringu1::name, 142857 * tenthous,
	thousand,
	twothousand,
	repeat(stringu1, 42),
	unique1::oid,
	format('(%s,%s)', tenthous, twenty)::tid,
	(four + 1.0)/(hundred+1),
	odd::float8 / (tenthous + 1),
	format('%s:00:%s:00:%s:00', to_hex(odd), to_hex(even), to_hex(hundred))::macaddr,
	inet '10.2.3.4' + tenthous,
	cidr '10.2.3/24' + tenthous,
	substr(stringu1, 1, 1)::bpchar,
	date '1995-08-15' + tenthous,
	time '01:20:30' + thousand * interval '18.5 second',
	timestamp '1942-07-23 03:05:09' + tenthous * interval '36.38 hours',
	timestamptz '1972-10-10 03:00' + thousand * interval '1 hour',
	justify_days(justify_hours(tenthous * interval '12 minutes')),
	timetz '01:30:20' + hundred * interval '15 seconds',
	thousand::bit(10),
	tenthous::bit(16)::varbit,
	tenthous::numeric(36,30) * fivethous * even / (hundred + 1),
	format('%s%s-%s-%s-%s-%s%s%s', to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'), to_char(tenthous, 'FM0000'))::uuid,
	int4range(thousand, twothousand),
	format('%s/%s%s', odd, even, tenthous)::pg_lsn,
	box(point(odd, even), point(thousand, twothousand))
FROM tenk1 ORDER BY unique2 LIMIT 5 OFFSET 5
================================

-- Fill a few pages
DO $$
DECLARE curtid tid
================================

    EXIT WHEN curtid > tid '(2, 0)'
================================

$$
================================

INSERT INTO brintest_2 VALUES (numrange(0, 2^1000::numeric))
================================

INSERT INTO brin_test SELECT x/100,x%100 FROM generate_series(1,10000) x(x)
================================


-- long random strings (~2000 chars each, so ~6kB for min/max on two
-- columns) to trigger toasting
WITH rand_value AS (SELECT string_agg(md5(i::text),'') AS val FROM generate_series(1,60) s(i))
INSERT INTO brintest_3
SELECT val, val, val, val FROM rand_value
================================


-- retry insert with a different random-looking (but deterministic) value
-- the value is different, and so should replace either min or max in the
-- brin summary
WITH rand_value AS (SELECT string_agg(md5((-i)::text),'') AS val FROM generate_series(1,60) s(i))
INSERT INTO brintest_3
SELECT val, val, val, val FROM rand_value
================================


================================

DISCARD SEQUENCES
================================



\d sequence_test4
\d serialtest2_f2_seq


-- Test comments
COMMENT ON SEQUENCE asdf IS 'won''t work'
================================

COMMENT ON SEQUENCE sequence_test2 IS 'will work'
================================

COMMENT ON SEQUENCE sequence_test2 IS NULL
================================

DISCARD SEQUENCES
================================


================================


-- ensure we detect contradictions in clauses
================================
 a can't be NULL and NOT NULL.
explain (costs off) select * from lp where a <> 'a' and a is null
================================


insert into list_part select generate_series(1,4)
================================


-- Don't select an actual value out of the table as the order of the Append's
-- subnodes may not be stable.
declare cur SCROLL CURSOR for select 1 from list_part where a > (select 1) and a < (select 4)
================================


-- move beyond the final row
move 3 from cur
================================


-- Ensure we get two rows.
fetch backward all from cur
================================
$$ language plpgsql stable
================================

        ln := regexp_replace(ln, 'actual rows=\d+ loops=\d+', 'actual rows=N loops=N')
================================

        ln := regexp_replace(ln, 'Rows Removed by Filter: \d+', 'Rows Removed by Filter: N')
================================

        return next ln
================================

$$
================================

-- Insert some values we won't find in ab
insert into lprt_a select 0 from generate_series(1,100)
================================

insert into ma_test select x,x from generate_series(0,29) t(x)
================================


-- enum type list partition key
create type pp_colors as enum ('green', 'blue', 'black')
================================

drop type pp_colors
================================


-- record type as partition key
create type pp_rectype as (a int, b int)
================================

drop type pp_rectype
================================

explain (costs off) select * from pp_intrangepart where a = '[1,2]'::int4range
================================

explain (costs off) select * from pp_intrangepart where a = '(1,2)'::int4range
================================


-- Ensure that listp_12_2 is not scanned.  (The nested Append is not seen in
-- the plan as it's pulled in setref.c due to having just a single subnode).
select explain_parallel_append('select * from listp where a = (select 1)
================================
')
================================


-- Like the above but throw some more complexity at the planner by adding
-- a UNION ALL.  We expect both sides of the union not to scan the
-- non-required partitions.
select explain_parallel_append(
'select * from listp where a = (select 1)
  union all
select * from listp where a = (select 2)
================================
')
================================


--
-- Check that gen_partprune_steps() detects self-contradiction from clauses
-- regardless of the order of the clauses (Here we use a custom operator to
-- prevent the equivclass.c machinery from reordering the clauses)
--

create operator === (
   leftarg = int4,
   rightarg = int4,
   procedure = int4eq,
   commutator = ===,
   hashes
)
================================

create operator class part_test_int4_ops2
for type int4
using hash as
operator 1 ===,
function 2 part_hashint4_noop(int4, int8)
================================


explain (costs off) select * from hp_contradict_test where a is null and a === 1 and b === 1
================================

explain (costs off) select * from hp_contradict_test where a === 1 and b === 1 and a is null
================================

drop operator class part_test_int4_ops2 using hash
================================

drop operator ===(int4, int4)
================================


================================
--
-- \crosstabview
--

CREATE TABLE ctv_data (v, h, c, i, d) AS
VALUES
   ('v1','h2','foo', 3, '2015-04-01'::date),
   ('v2','h1','bar', 3, '2015-01-02'),
   ('v1','h0','baz', NULL, '2015-07-12'),
   ('v0','h4','qux', 4, '2015-07-15'),
   ('v0','h4','dbl', -3, '2014-12-15'),
   ('v0',NULL,'qux', 5, '2014-07-15'),
   ('v1','h2','quux',7, '2015-04-04')
================================

-- basic usage with 3 columns
 \crosstabview

-- ordered months in horizontal header, quoted column name
SELECT v, to_char(d, 'Mon') AS "month name", EXTRACT(month FROM d) AS num,
 count(*) FROM ctv_data  GROUP BY 1,2,3 ORDER BY 1
 \crosstabview v "month name" 4 num

-- ordered months in vertical header, ordered years in horizontal header
SELECT EXTRACT(year FROM d) AS year, to_char(d,'Mon') AS """month"" name",
  EXTRACT(month FROM d) AS month,
  format('sum=%s avg=%s', sum(i), avg(i)::numeric(2,1))
  FROM ctv_data
  GROUP BY EXTRACT(year FROM d), to_char(d,'Mon'), EXTRACT(month FROM d)
ORDER BY month
\crosstabview """month"" name" year format year

-- combine contents vertically into the same cell (V/H duplicates)
SELECT v, h, string_agg(c, E'\n') FROM ctv_data GROUP BY v, h ORDER BY 1,2,3
 \crosstabview 1 2 3

-- horizontal ASC order from window function
SELECT v,h, string_agg(c, E'\n') AS c, row_number() OVER(ORDER BY h) AS r
FROM ctv_data GROUP BY v, h ORDER BY 1,3,2
 \crosstabview v h c r

-- horizontal DESC order from window function
SELECT v, h, string_agg(c, E'\n') AS c, row_number() OVER(ORDER BY h DESC) AS r
FROM ctv_data GROUP BY v, h ORDER BY 1,3,2
 \crosstabview v h c r

-- horizontal ASC order from window function, NULLs pushed rightmost
SELECT v,h, string_agg(c, E'\n') AS c, row_number() OVER(ORDER BY h NULLS LAST) AS r
FROM ctv_data GROUP BY v, h ORDER BY 1,3,2
 \crosstabview v h c r

-- only null, no column name, 2 columns: error
SELECT null,null \crosstabview

-- only null, no column name, 3 columns: works
SELECT null,null,null \crosstabview

-- null display
\pset null '#null#'
SELECT v,h, string_agg(i::text, E'\n') AS i FROM ctv_data
GROUP BY v, h ORDER BY h,v
 \crosstabview v h i
\pset null ''

-- refer to columns by position
SELECT v,h,string_agg(i::text, E'\n'), string_agg(c, E'\n')
FROM ctv_data GROUP BY v, h ORDER BY h,v
 \crosstabview 2 1 4

-- refer to columns by positions and names mixed
SELECT v,h, string_agg(i::text, E'\n') AS i, string_agg(c, E'\n') AS c
FROM ctv_data GROUP BY v, h ORDER BY h,v
 \crosstabview 1 "h" 4

-- refer to columns by quoted names, check downcasing of unquoted name
SELECT 1 as "22", 2 as b, 3 as "Foo"
 \crosstabview "22" B "Foo"

-- error: bad column name
SELECT v,h,c,i FROM ctv_data
 \crosstabview v h j

-- error: need to quote name
SELECT 1 as "22", 2 as b, 3 as "Foo"
 \crosstabview 1 2 Foo

-- error: need to not quote name
SELECT 1 as "22", 2 as b, 3 as "Foo"
 \crosstabview 1 "B" "Foo"

-- error: bad column number
SELECT v,h,i,c FROM ctv_data
 \crosstabview 2 1 5

-- error: same H and V columns
SELECT v,h,i,c FROM ctv_data
 \crosstabview 2 h 4

-- error: too many columns
SELECT a,a,1 FROM generate_series(1,3000) AS a
 \crosstabview

-- error: only one column
SELECT 1 \crosstabview

DROP TABLE ctv_data
================================


INSERT INTO ctv_data SELECT 1, x, '*' || x FROM generate_series(1,10) x
================================


================================


--Should work. Valid NOTIFY/LISTEN/UNLISTEN commands
NOTIFY notify_async2
================================

LISTEN notify_async2
================================

UNLISTEN notify_async2
================================

UNLISTEN *
================================


================================


================================
--
-- CREATE_AGGREGATE
--

-- all functions CREATEd
CREATE AGGREGATE newavg (
   sfunc = int4_avg_accum, basetype = int4, stype = _int8,
   finalfunc = int8_avg,
   initcond1 = '{0,0}'
)
================================


-- test comments
COMMENT ON AGGREGATE newavg_wrong (int4) IS 'an agg comment'
================================

COMMENT ON AGGREGATE newavg (int4) IS 'an agg comment'
================================

COMMENT ON AGGREGATE newavg (int4) IS NULL
================================


-- without finalfunc
================================
 test obsolete spellings 'sfunc1' etc
CREATE AGGREGATE newsum (
   sfunc1 = int4pl, basetype = int4, stype1 = int4,
   initcond1 = '0'
)
================================


-- zero-argument aggregate
CREATE AGGREGATE newcnt (*) (
   sfunc = int8inc, stype = int8,
   initcond = '0', parallel = safe
)
================================


-- old-style spelling of same (except without parallel-safe
================================
 that's too new)
CREATE AGGREGATE oldcnt (
   sfunc = int8inc, basetype = 'ANY', stype = int8,
   initcond = '0'
)
================================


-- aggregate that only cares about null/nonnull input
CREATE AGGREGATE newcnt ("any") (
   sfunc = int8inc_any, stype = int8,
   initcond = '0'
)
================================


COMMENT ON AGGREGATE nosuchagg (*) IS 'should fail'
================================

COMMENT ON AGGREGATE newcnt (*) IS 'an agg(*) comment'
================================

COMMENT ON AGGREGATE newcnt ("any") IS 'an agg(any) comment'
================================


create aggregate sum2(int8,int8) (
   sfunc = sum3, stype = int8,
   initcond = '0'
)
================================


-- multi-argument aggregates sensitive to distinct/order, strict/nonstrict
create type aggtype as (a integer, b integer, c text)
================================


create aggregate aggfstr(integer,integer,text) (
   sfunc = aggf_trans, stype = aggtype[],
   initcond = '{}'
)
================================


create aggregate aggfns(integer,integer,text) (
   sfunc = aggfns_trans, stype = aggtype[], sspace = 10000,
   initcond = '{}'
)
================================


create aggregate least_agg(int4) (
  stype = int8, sfunc = least_accum
)
================================


create aggregate least_agg(int4) (
  stype = int8, sfunc = least_accum
)
================================
  -- fails

create aggregate least_agg(int8) (
  stype = int8, sfunc = least_accum
)
================================


create aggregate least_agg(variadic items anyarray) (
  stype = anyelement, sfunc = least_accum
)
================================


create aggregate cleast_agg(variadic items anycompatiblearray) (
  stype = anycompatible, sfunc = cleast_accum
)
================================


-- test ordered-set aggs using built-in support functions
create aggregate my_percentile_disc(float8 ORDER BY anyelement) (
  stype = internal,
  sfunc = ordered_set_transition,
  finalfunc = percentile_disc_final,
  finalfunc_extra = true,
  finalfunc_modify = read_write
)
================================


create aggregate my_rank(VARIADIC "any" ORDER BY VARIADIC "any") (
  stype = internal,
  sfunc = ordered_set_transition_multi,
  finalfunc = rank_final,
  finalfunc_extra = true,
  hypothetical
)
================================


alter aggregate my_percentile_disc(float8 ORDER BY anyelement)
  rename to test_percentile_disc
================================

alter aggregate my_rank(VARIADIC "any" ORDER BY VARIADIC "any")
  rename to test_rank
================================


\da test_*

-- moving-aggregate options

CREATE AGGREGATE sumdouble (float8)
(
    stype = float8,
    sfunc = float8pl,
    mstype = float8,
    msfunc = float8pl,
    minvfunc = float8mi
)
================================


-- aggregate combine and serialization functions

-- can't specify just one of serialfunc and deserialfunc
CREATE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	serialfunc = numeric_avg_serialize
)
================================


-- serialfunc must have correct parameters
CREATE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	serialfunc = numeric_avg_deserialize,
	deserialfunc = numeric_avg_deserialize
)
================================


-- deserialfunc must have correct parameters
CREATE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	serialfunc = numeric_avg_serialize,
	deserialfunc = numeric_avg_serialize
)
================================


-- ensure combine function parameters are checked
CREATE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	serialfunc = numeric_avg_serialize,
	deserialfunc = numeric_avg_deserialize,
	combinefunc = int4larger
)
================================


-- ensure create aggregate works.
CREATE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	finalfunc = numeric_avg,
	serialfunc = numeric_avg_serialize,
	deserialfunc = numeric_avg_deserialize,
	combinefunc = numeric_avg_combine,
	finalfunc_modify = shareable  -- just to test a non-default setting
)
================================


DROP AGGREGATE myavg (numeric)
================================


-- create or replace aggregate
CREATE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	finalfunc = numeric_avg
)
================================


CREATE OR REPLACE AGGREGATE myavg (numeric)
(
	stype = internal,
	sfunc = numeric_avg_accum,
	finalfunc = numeric_avg,
	serialfunc = numeric_avg_serialize,
	deserialfunc = numeric_avg_deserialize,
	combinefunc = numeric_avg_combine,
	finalfunc_modify = shareable  -- just to test a non-default setting
)
================================


-- can change stype:
CREATE OR REPLACE AGGREGATE myavg (numeric)
(
	stype = numeric,
	sfunc = numeric_add
)
================================


-- can't change return type:
CREATE OR REPLACE AGGREGATE myavg (numeric)
(
	stype = numeric,
	sfunc = numeric_add,
	finalfunc = numeric_out
)
================================


-- can't change to a different kind:
CREATE OR REPLACE AGGREGATE myavg (order by numeric)
(
	stype = numeric,
	sfunc = numeric_add
)
================================


CREATE OR REPLACE AGGREGATE sum3 (int8,int8,int8)
(
	stype = int8,
	sfunc = sum4
)
================================


DROP AGGREGATE myavg (numeric)
================================


-- invalid: bad parallel-safety marking
CREATE AGGREGATE mysum (int)
(
	stype = int,
	sfunc = int4pl,
	parallel = pear
)
================================
 $$
LANGUAGE SQL
================================


CREATE AGGREGATE invalidsumdouble (float8)
(
    stype = float8,
    sfunc = float8pl,
    mstype = float8,
    msfunc = float8pl,
    minvfunc = float8mi_n
)
================================
 $$
LANGUAGE SQL
================================


CREATE AGGREGATE wrongreturntype (float8)
(
    stype = float8,
    sfunc = float8pl,
    mstype = float8,
    msfunc = float8pl,
    minvfunc = float8mi_int
)
================================


-- invalid: non-lowercase quoted identifiers

CREATE AGGREGATE case_agg ( -- old syntax
	"Sfunc1" = int4pl,
	"Basetype" = int4,
	"Stype1" = int4,
	"Initcond1" = '0',
	"Parallel" = safe
)
================================


CREATE AGGREGATE case_agg(float8)
(
	"Stype" = internal,
	"Sfunc" = ordered_set_transition,
	"Finalfunc" = percentile_disc_final,
	"Finalfunc_extra" = true,
	"Finalfunc_modify" = read_write,
	"Parallel" = safe
)
================================


================================


================================

SELECT xmlelement(name foo, bytea 'bar')
================================

SELECT xmlelement(name foo, bytea 'bar')
================================

SELECT xmlparse(content '<undefinedentity>&idontexist
================================
</undefinedentity>')
================================

SELECT xmlparse(content '<invalidns xmlns=''&lt
================================
''/>')
================================

SELECT xmlparse(content '<twoerrors>&idontexist
================================
</unbalanced>')
================================

SELECT xmlparse(document '<undefinedentity>&idontexist
================================
</abc>')
================================

SELECT xmlparse(document '<invalidns xmlns=''&lt
================================
''/>')
================================

SELECT xmlparse(document '<twoerrors>&idontexist
================================
</unbalanced>')
================================

SELECT xpath('//text()', '<root>&lt
================================
</root>')
================================

SELECT xpath('//@value', '<root value="&lt
================================
"/>')
================================


-- Round-trip non-ASCII data through xpath().
DO $$
DECLARE
  xml_declaration text := '<?xml version="1.0" encoding="ISO-8859-1"?>'
================================

  degree_symbol text
================================

  res xml[]
================================

    RETURN
================================


  degree_symbol := convert_from('\xc2b0', 'UTF8')
================================

  res := xpath('text()', (xml_declaration ||
    '<x>' || degree_symbol || '</x>')::xml)
================================

  IF degree_symbol <> res[1]::text THEN
    RAISE 'expected % (%), got % (%)',
      degree_symbol, convert_to(degree_symbol, 'UTF8'),
      res[1], convert_to(res[1]::text, 'UTF8')
================================

EXCEPTION
  -- character with byte sequence 0xc2 0xb0 in encoding "UTF8" has no equivalent in encoding "LATIN8"
  WHEN untranslatable_character
  -- default conversion function for encoding "UTF8" to "MULE_INTERNAL" does not exist
  OR undefined_function
  -- unsupported XML feature
  OR feature_not_supported THEN
    RAISE LOG 'skip: %', SQLERRM
================================

SELECT xml_is_well_formed('<undefinedentity>&idontexist
================================
</abc>')
================================

SELECT xml_is_well_formed('<invalidns xmlns=''&lt
================================
''/>')
================================

SELECT xml_is_well_formed('<twoerrors>&idontexist
================================
</unbalanced>')
================================


-- Since xpath() deals with namespaces, it's a bit stricter about
-- what's well-formed and what's not. If we don't obey these rules
-- (i.e. ignore namespace-related errors from libxml), xpath()
-- fails in subtle ways. The following would for example produce
-- the xml value
--   <invalidns xmlns='<'/>
-- which is invalid because '<' may not appear un-escaped in
-- attribute values.
-- Since different libxml versions emit slightly different
-- error messages, we suppress the DETAIL in this test.
\set VERBOSITY terse
SELECT xpath('/*', '<invalidns xmlns=''&lt
================================
''/>')
================================

\set VERBOSITY default

-- Again, the XML isn't well-formed for namespace purposes
SELECT xpath('/*', '<nosuchprefix:tag/>')
================================


-- External entity references should not leak filesystem information.
SELECT XMLPARSE(DOCUMENT '<!DOCTYPE foo [<!ENTITY c SYSTEM "/etc/passwd">]><foo>&c
================================
</foo>')
================================

SELECT XMLPARSE(DOCUMENT '<!DOCTYPE foo [<!ENTITY c SYSTEM "/etc/no.such.file">]><foo>&c
================================
</foo>')
================================

-- This might or might not load the requested DTD, but it mustn't throw error.
SELECT XMLPARSE(DOCUMENT '<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN" "http://www.oasis-open.org/docbook/xml/4.1.2/docbookx.dtd"><chapter>&nbsp
================================
</chapter>')
================================


\sv xmltableview1

EXPLAIN (COSTS OFF) SELECT * FROM xmltableview1
================================


-- XMLNAMESPACES tests
SELECT * FROM XMLTABLE(XMLNAMESPACES('http://x.y' AS zz),
                      '/zz:rows/zz:row'
                      PASSING '<rows xmlns="http://x.y"><row><a>10</a></row></rows>'
                      COLUMNS a int PATH 'zz:a')
================================


CREATE VIEW xmltableview2 AS SELECT * FROM XMLTABLE(XMLNAMESPACES('http://x.y' AS zz),
                      '/zz:rows/zz:row'
                      PASSING '<rows xmlns="http://x.y"><row><a>10</a></row></rows>'
                      COLUMNS a int PATH 'zz:a')
================================


SELECT * FROM XMLTABLE(XMLNAMESPACES(DEFAULT 'http://x.y'),
                      '/rows/row'
                      PASSING '<rows xmlns="http://x.y"><row><a>10</a></row></rows>'
                      COLUMNS a int PATH 'a')
================================


SELECT * FROM XMLTABLE('.'
                       PASSING '<foo/>'
                       COLUMNS a text PATH 'foo/namespace::node()')
================================


EXECUTE pp
================================

SELECT * FROM xmltable('/root' passing '<root><element>a1a<!-- aaaa -->a2a<?aaaaa?> <!--z-->  bbbb<x>xxx</x>cccc</element></root>' COLUMNS element text PATH 'element/text()')
================================


-- XML builtin entities
SELECT * FROM xmltable('/x/a' PASSING '<x><a><ent>&apos
================================
</ent></a><a><ent>&quot
================================
</ent></a><a><ent>&amp
================================
</ent></a><a><ent>&lt
================================
</ent></a><a><ent>&gt
================================
</ent></a></x>' COLUMNS ent text)
================================

SELECT * FROM xmltable('/x/a' PASSING '<x><a><ent>&apos
================================
</ent></a><a><ent>&quot
================================
</ent></a><a><ent>&amp
================================
</ent></a><a><ent>&lt
================================
</ent></a><a><ent>&gt
================================
</ent></a></x>' COLUMNS ent xml)
================================


-- if all is ok, then result is empty
-- one line xml test
WITH
   x AS (SELECT proname, proowner, procost::numeric, pronargs,
                array_to_string(proargnames,',') as proargnames,
                case when proargtypes <> '' then array_to_string(proargtypes::oid[],',') end as proargtypes
           FROM pg_proc WHERE proname = 'f_leak'),
   y AS (SELECT xmlelement(name proc,
                           xmlforest(proname, proowner,
                                     procost, pronargs,
                                     proargnames, proargtypes)) as proc
           FROM x),
   z AS (SELECT xmltable.*
           FROM y,
                LATERAL xmltable('/proc' PASSING proc
                                 COLUMNS proname name,
                                         proowner oid,
                                         procost float,
                                         pronargs int,
                                         proargnames text,
                                         proargtypes text))
   SELECT * FROM z
   EXCEPT SELECT * FROM x
================================


-- multi line xml test, result should be empty too
WITH
   x AS (SELECT proname, proowner, procost::numeric, pronargs,
                array_to_string(proargnames,',') as proargnames,
                case when proargtypes <> '' then array_to_string(proargtypes::oid[],',') end as proargtypes
           FROM pg_proc),
   y AS (SELECT xmlelement(name data,
                           xmlagg(xmlelement(name proc,
                                             xmlforest(proname, proowner, procost,
                                                       pronargs, proargnames, proargtypes)))) as doc
           FROM x),
   z AS (SELECT xmltable.*
           FROM y,
                LATERAL xmltable('/data/proc' PASSING doc
                                 COLUMNS proname name,
                                         proowner oid,
                                         procost float,
                                         pronargs int,
                                         proargnames text,
                                         proargtypes text))
   SELECT * FROM z
   EXCEPT SELECT * FROM x
================================


-- XPath result can be boolean or number too
SELECT * FROM XMLTABLE('*' PASSING '<a>a</a>' COLUMNS a xml PATH '.', b text PATH '.', c text PATH '"hi"', d boolean PATH '. = "a"', e integer PATH 'string-length(.)')
================================

\x
SELECT * FROM XMLTABLE('*' PASSING '<e>pre<!--c1--><?pi arg?><![CDATA[&ent1]]><n2>&amp
================================
deep</n2>post</e>' COLUMNS x xml PATH 'node()', y xml PATH '/')
================================

\x

SELECT * FROM XMLTABLE('.' PASSING XMLELEMENT(NAME a) columns a varchar(20) PATH '"<foo/>"', b xml PATH '"<foo/>"')
================================


================================


================================


-- Test null limit and offset.  The planner would discard a simple null
-- constant, so to ensure executor is exercised, do this:
select * from int8_tbl limit (case when random() < 0.5 then null::bigint end)
================================

select * from int8_tbl offset (case when random() < 0.5 then null::bigint end)
================================


declare c1 cursor for select * from int8_tbl limit 10
================================

fetch all in c1
================================

fetch 1 in c1
================================

fetch backward 1 in c1
================================

fetch backward all in c1
================================

fetch backward 1 in c1
================================

fetch all in c1
================================


declare c2 cursor for select * from int8_tbl limit 3
================================

fetch all in c2
================================

fetch 1 in c2
================================

fetch backward 1 in c2
================================

fetch backward all in c2
================================

fetch backward 1 in c2
================================

fetch all in c2
================================


declare c3 cursor for select * from int8_tbl offset 3
================================

fetch all in c3
================================

fetch 1 in c3
================================

fetch backward 1 in c3
================================

fetch backward all in c3
================================

fetch backward 1 in c3
================================

fetch all in c3
================================


declare c4 cursor for select * from int8_tbl offset 10
================================

fetch all in c4
================================

fetch 1 in c4
================================

fetch backward 1 in c4
================================

fetch backward all in c4
================================

fetch backward 1 in c4
================================

fetch all in c4
================================


declare c5 cursor for select * from int8_tbl order by q1 fetch first 2 rows with ties
================================

fetch all in c5
================================

fetch 1 in c5
================================

fetch backward 1 in c5
================================

fetch backward 1 in c5
================================

fetch all in c5
================================

fetch backward all in c5
================================

fetch all in c5
================================

fetch backward all in c5
================================


-- Stress test for variable LIMIT in conjunction with bounded-heap sorting

SELECT
  (SELECT n
     FROM (VALUES (1)) AS x,
          (SELECT n FROM generate_series(1,10) AS n
             ORDER BY n LIMIT 1 OFFSET s-1) AS y) AS z
  FROM generate_series(1,10) AS s
================================


--
-- FETCH FIRST
-- Check the WITH TIES clause
--

SELECT  thousand
		FROM onek WHERE thousand < 5
		ORDER BY thousand FETCH FIRST 2 ROW WITH TIES
================================


SELECT  thousand
		FROM onek WHERE thousand < 5
		ORDER BY thousand FETCH FIRST ROWS WITH TIES
================================


SELECT  thousand
		FROM onek WHERE thousand < 5
		ORDER BY thousand FETCH FIRST 1 ROW WITH TIES
================================


SELECT  thousand
		FROM onek WHERE thousand < 5
		ORDER BY thousand FETCH FIRST 2 ROW ONLY
================================


-- SKIP LOCKED and WITH TIES are incompatible
SELECT  thousand
		FROM onek WHERE thousand < 5
		ORDER BY thousand FETCH FIRST 1 ROW WITH TIES FOR UPDATE SKIP LOCKED
================================


-- should fail
SELECT ''::text AS two, unique1, unique2, stringu1
		FROM onek WHERE unique1 > 50
		FETCH FIRST 2 ROW WITH TIES
================================

\d+ limit_thousand_v_1
CREATE VIEW limit_thousand_v_2 AS SELECT thousand FROM onek WHERE thousand < 995
		ORDER BY thousand OFFSET 10 FETCH FIRST 5 ROWS ONLY
================================

\d+ limit_thousand_v_2
CREATE VIEW limit_thousand_v_3 AS SELECT thousand FROM onek WHERE thousand < 995
		ORDER BY thousand FETCH FIRST NULL ROWS WITH TIES
================================

\d+ limit_thousand_v_3
CREATE VIEW limit_thousand_v_4 AS SELECT thousand FROM onek WHERE thousand < 995
		ORDER BY thousand FETCH FIRST NULL ROWS ONLY
================================

\d+ limit_thousand_v_4
-- leave these views

================================
--
-- Tests for psql features that aren't closely connected to any
-- specific server features
--

-- \set

-- fail: invalid name
\set invalid/name foo
-- fail: invalid value for special variable
\set AUTOCOMMIT foo
\set FETCH_COUNT foo
-- check handling of built-in boolean variable
\echo :ON_ERROR_ROLLBACK
\set ON_ERROR_ROLLBACK
\echo :ON_ERROR_ROLLBACK
\set ON_ERROR_ROLLBACK foo
\echo :ON_ERROR_ROLLBACK
\set ON_ERROR_ROLLBACK on
\echo :ON_ERROR_ROLLBACK
\unset ON_ERROR_ROLLBACK
\echo :ON_ERROR_ROLLBACK

-- \g and \gx

SELECT 1 as one, 2 as two \g
\gx
SELECT 3 as three, 4 as four \gx
\g

-- \gx should work in FETCH_COUNT mode too
\set FETCH_COUNT 1

SELECT 1 as one, 2 as two \g
\gx
SELECT 3 as three, 4 as four \gx
\g

\unset FETCH_COUNT

-- \g/\gx with pset options

SELECT 1 as one, 2 as two \g (format=csv csv_fieldsep='\t')
\g
SELECT 1 as one, 2 as two \gx (title='foo bar')
\g

-- \gset

select 10 as test01, 20 as test02, 'Hello' as test03 \gset pref01_

\echo :pref01_test01 :pref01_test02 :pref01_test03

-- should fail: bad variable name
select 10 as "bad name"
\gset

select 97 as "EOF", 'ok' as _foo \gset IGNORE
\echo :IGNORE_foo :IGNOREEOF

-- multiple backslash commands in one line
select 1 as x, 2 as y \gset pref01_ \\ \echo :pref01_x
select 3 as x, 4 as y \gset pref01_ \echo :pref01_x \echo :pref01_y
select 5 as x, 6 as y \gset pref01_ \\ \g \echo :pref01_x :pref01_y
select 7 as x, 8 as y \g \gset pref01_ \echo :pref01_x :pref01_y

-- NULL should unset the variable
\set var2 xyz
select 1 as var1, NULL as var2, 3 as var3 \gset
\echo :var1 :var2 :var3

-- \gset requires just one tuple
select 10 as test01, 20 as test02 from generate_series(1,3) \gset
select 10 as test01, 20 as test02 from generate_series(1,0) \gset

-- \gset should work in FETCH_COUNT mode too
\set FETCH_COUNT 1

select 1 as x, 2 as y \gset pref01_ \\ \echo :pref01_x
select 3 as x, 4 as y \gset pref01_ \echo :pref01_x \echo :pref01_y
select 10 as test01, 20 as test02 from generate_series(1,3) \gset
select 10 as test01, 20 as test02 from generate_series(1,0) \gset

\unset FETCH_COUNT

-- \gdesc

SELECT
    NULL AS zero,
    1 AS one,
    2.0 AS two,
    'three' AS three,
    $1 AS four,
    sin($2) as five,
    'foo'::varchar(4) as six,
    CURRENT_DATE AS now
\gdesc

-- should work with tuple-returning utilities, such as EXECUTE
PREPARE test AS SELECT 1 AS first, 2 AS second
================================

EXECUTE test \gdesc
EXPLAIN EXECUTE test \gdesc

-- should fail cleanly - syntax error
SELECT 1 + \gdesc

-- check behavior with empty results
SELECT \gdesc
CREATE TABLE bububu(a int) \gdesc

-- subject command should not have executed
TABLE bububu
================================

select format('create index on gexec_test(%I)', attname)
from pg_attribute
where attrelid = 'gexec_test'::regclass and attnum > 0
order by attnum
\gexec

-- \gexec should work in FETCH_COUNT mode too
-- (though the fetch limit applies to the executed queries not the meta query)
\set FETCH_COUNT 1

select 'select 1 as ones', 'select x.y, x.y*2 as double from generate_series(1,4) as x(y)'
union all
select 'drop table gexec_test', NULL
union all
select 'drop table gexec_test', 'select ''2000-01-01''::date as party_over'
\gexec

\unset FETCH_COUNT

-- show all pset options
\pset

-- test multi-line headers, wrapping, and newline indicators
-- in aligned, unaligned, and wrapped formats
prepare q as select array_to_string(array_agg(repeat('x',2*n)),E'\n') as "ab

c", array_to_string(array_agg(repeat('y',20-2*n)),E'\n') as "a
bc" from generate_series(1,10) as n(n) group by n>1 order by n>1
================================


\pset linestyle ascii

\pset expanded off
\pset columns 40

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset expanded on
\pset columns 20

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset linestyle old-ascii

\pset expanded off
\pset columns 40

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset expanded on
\pset columns 20

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset linestyle ascii

\pset expanded off
\pset columns 40

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset expanded on
\pset columns 30

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset expanded on
\pset columns 20

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset linestyle old-ascii

\pset expanded off
\pset columns 40

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset expanded on

\pset border 0
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 1
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset border 2
\pset format unaligned
execute q
================================

\pset format aligned
execute q
================================

\pset format wrapped
execute q
================================


\pset linestyle ascii
\pset border 1

-- support table for output-format tests (useful to create a footer)

create table psql_serial_tab (id serial)
================================


-- test header/footer/tuples_only behavior in aligned/unaligned/wrapped cases

\pset format aligned

\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
-- empty table is a special case for this format
select 1 where false
================================


\pset format unaligned

\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

\pset format wrapped

\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

-- check conditional am display
\pset expanded off

CREATE SCHEMA tableam_display
================================

ALTER SCHEMA tableam_display OWNER TO regress_display_role
================================

CREATE ACCESS METHOD heap_psql TYPE TABLE HANDLER heap_tableam_handler
================================

CREATE MATERIALIZED VIEW mat_view_heap_psql USING heap_psql AS SELECT f1 from tbl_heap_psql
================================

\d+ tbl_heap_psql
\d+ tbl_heap
\set HIDE_TABLEAM off
\d+ tbl_heap_psql
\d+ tbl_heap
-- AM is displayed for tables, indexes and materialized views.
\d+
\dt+
\dm+
-- But not for views and sequences.
\dv+
\set HIDE_TABLEAM on
\d+
RESET ROLE
================================

DROP SCHEMA tableam_display CASCADE
================================

DROP ACCESS METHOD heap_psql
================================


-- test numericlocale (as best we can without control of psql's locale)

\pset format aligned
\pset expanded off
\pset numericlocale true

select n, -n as m, n * 111 as x, '1e90'::float8 as f
from generate_series(0,3) n
================================


\pset numericlocale false

-- test asciidoc output format

\pset format asciidoc

\pset border 1
\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

prepare q as
  select 'some|text' as "a|title", '        ' as "empty ", n as int
  from generate_series(1,2) as n
================================


\pset expanded off
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


\pset expanded on
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


-- test csv output format

\pset format csv

\pset border 1
\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

prepare q as
  select 'some"text' as "a""title", E'  <foo>\n<bar>' as "junk",
         '   ' as "empty", n as int
  from generate_series(1,2) as n
================================


\pset expanded off
execute q
================================


\pset expanded on
execute q
================================


-- special cases
\pset expanded off
select 'comma,comma' as comma, 'semi
================================
semi' as semi
================================

\pset csv_fieldsep '
================================
'
select 'comma,comma' as comma, 'semi
================================
semi' as semi
================================

\pset csv_fieldsep '.'
select '\' as d1, '' as d2
================================


-- illegal csv separators
\pset csv_fieldsep ''
\pset csv_fieldsep '\0'
\pset csv_fieldsep '\n'
\pset csv_fieldsep '\r'
\pset csv_fieldsep '"'
\pset csv_fieldsep ',,'

\pset csv_fieldsep ','

-- test html output format

\pset format html

\pset border 1
\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

prepare q as
  select 'some"text' as "a&title", E'  <foo>\n<bar>' as "junk",
         '   ' as "empty", n as int
  from generate_series(1,2) as n
================================


\pset expanded off
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset tableattr foobar
execute q
================================

\pset tableattr

\pset expanded on
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset tableattr foobar
execute q
================================

\pset tableattr

deallocate q
================================


-- test latex output format

\pset format latex

\pset border 1
\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

prepare q as
  select 'some\more_text' as "a$title", E'  #<foo>%&^~|\n{bar}' as "junk",
         '   ' as "empty", n as int
  from generate_series(1,2) as n
================================


\pset expanded off
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


\pset border 3
execute q
================================


\pset expanded on
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


\pset border 3
execute q
================================


-- test latex-longtable output format

\pset format latex-longtable

\pset border 1
\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

prepare q as
  select 'some\more_text' as "a$title", E'  #<foo>%&^~|\n{bar}' as "junk",
         '   ' as "empty", n as int
  from generate_series(1,2) as n
================================


\pset expanded off
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


\pset border 3
execute q
================================


\pset tableattr lr
execute q
================================

\pset tableattr

\pset expanded on
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


\pset border 3
execute q
================================


\pset tableattr lr
execute q
================================

\pset tableattr

deallocate q
================================


-- test troff-ms output format

\pset format troff-ms

\pset border 1
\pset expanded off
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false
\pset expanded on
\d psql_serial_tab_id_seq
\pset tuples_only true
\df exp
\pset tuples_only false

prepare q as
  select 'some\text' as "a\title", E'  <foo>\n<bar>' as "junk",
         '   ' as "empty", n as int
  from generate_series(1,2) as n
================================


\pset expanded off
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


\pset expanded on
\pset border 0
execute q
================================


\pset border 1
execute q
================================


\pset border 2
execute q
================================


-- check ambiguous format requests

\pset format a
\pset format l

-- clean up after output format tests

drop table psql_serial_tab
================================


\pset format aligned
\pset expanded off
\pset border 1

-- \echo and allied features

\echo this is a test
\echo -n without newline
\echo with -n newline
\echo '-n' with newline

\set foo bar
\echo foo = :foo

\qecho this is a test
\qecho foo = :foo

\warn this is a test
\warn foo = :foo

-- tests for \if ... \endif

\if true
  select 'okay'
================================

\else
  not okay
================================

  still not okay
\endif

-- at this point query buffer should still have last valid line
\g

-- \if should work okay on part of a query
select
  \if true
    42
  \else
    (bogus
  \endif
  forty_two
================================


-- test a large nested if using a variety of true-equivalents
\if true
	\if 1
		\if yes
			\if on
				\echo 'all true'
			\else
				\echo 'should not print #1-1'
			\endif
		\else
			\echo 'should not print #1-2'
		\endif
	\else
		\echo 'should not print #1-3'
	\endif
\else
	\echo 'should not print #1-4'
\endif

-- test a variety of false-equivalents in an if/elif/else structure
\if false
	\echo 'should not print #2-1'
\elif 0
	\echo 'should not print #2-2'
\elif no
	\echo 'should not print #2-3'
\elif off
	\echo 'should not print #2-4'
\else
	\echo 'all false'
\endif

-- test true-false elif after initial true branch
\if true
	\echo 'should print #2-5'
\elif true
	\echo 'should not print #2-6'
\elif false
	\echo 'should not print #2-7'
\else
	\echo 'should not print #2-8'
\endif

-- test simple true-then-else
\if true
	\echo 'first thing true'
\else
	\echo 'should not print #3-1'
\endif

-- test simple false-true-else
\if false
	\echo 'should not print #4-1'
\elif true
	\echo 'second thing true'
\else
	\echo 'should not print #5-1'
\endif

-- invalid boolean expressions are false
\if invalid boolean expression
	\echo 'will not print #6-1'
\else
	\echo 'will print anyway #6-2'
\endif

-- test un-matched endif
\endif

-- test un-matched else
\else

-- test un-matched elif
\elif

-- test double-else error
\if true
\else
\else
\endif

-- test elif out-of-order
\if false
\else
\elif
\endif

-- test if-endif matching in a false branch
\if false
    \if false
        \echo 'should not print #7-1'
    \else
        \echo 'should not print #7-2'
    \endif
    \echo 'should not print #7-3'
\else
    \echo 'should print #7-4'
\endif

-- show that vars and backticks are not expanded when ignoring extra args
\set foo bar
\echo :foo :'foo' :"foo"
\pset fieldsep | `nosuchcommand` :foo :'foo' :"foo"

-- show that vars and backticks are not expanded and commands are ignored
-- when in a false if-branch
\set try_to_quit '\\q'
\if false
	:try_to_quit
	\echo `nosuchcommand` :foo :'foo' :"foo"
	\pset fieldsep | `nosuchcommand` :foo :'foo' :"foo"
	\a
	\C arg1
	\c arg1 arg2 arg3 arg4
	\cd arg1
	\conninfo
	\copy arg1 arg2 arg3 arg4 arg5 arg6
	\copyright
	SELECT 1 as one, 2, 3 \crosstabview
	\dt arg1
	\e arg1 arg2
	\ef whole_line
	\ev whole_line
	\echo arg1 arg2 arg3 arg4 arg5
	\echo arg1
	\encoding arg1
	\errverbose
	\f arg1
	\g arg1
	\gx arg1
	\gexec
	SELECT 1 AS one \gset
	\h
	\?
	\html
	\i arg1
	\ir arg1
	\l arg1
	\lo arg1 arg2
	\lo_list
	\o arg1
	\p
	\password arg1
	\prompt arg1 arg2
	\pset arg1 arg2
	\q
	\reset
	\s arg1
	\set arg1 arg2 arg3 arg4 arg5 arg6 arg7
	\setenv arg1 arg2
	\sf whole_line
	\sv whole_line
	\t arg1
	\T arg1
	\timing arg1
	\unset arg1
	\w arg1
	\watch arg1
	\x arg1
	-- \else here is eaten as part of OT_FILEPIPE argument
	\w |/no/such/file \else
	-- \endif here is eaten as part of whole-line argument
	\! whole_line \endif
	\z
\else
	\echo 'should print #8-1'
\endif

-- :{?...} defined variable test
\set i 1
\if :{?i}
  \echo '#9-1 ok, variable i is defined'
\else
  \echo 'should not print #9-2'
\endif

\if :{?no_such_variable}
  \echo 'should not print #10-1'
\else
  \echo '#10-2 ok, variable no_such_variable is not defined'
\endif

SELECT :{?i} AS i_is_defined
================================


SELECT NOT :{?no_such_var} AS no_such_var_is_not_defined
================================


-- SHOW_CONTEXT

\set SHOW_CONTEXT never
do $$
begin
  raise notice 'foo'
================================

  raise exception 'bar'
================================


\set SHOW_CONTEXT errors
do $$
begin
  raise notice 'foo'
================================

  raise exception 'bar'
================================


\set SHOW_CONTEXT always
do $$
begin
  raise notice 'foo'
================================

  raise exception 'bar'
================================

\p
SELECT 2 \r
\p
SELECT 3 \p
UNION SELECT 4 \p
UNION SELECT 5
ORDER BY 1
================================

\r
\p

-- tests for special result variables

-- working query, 2 rows selected
SELECT 1 AS stuff UNION SELECT 2
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT

-- syntax error
SELECT 1 UNION
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT
\echo 'last error message:' :LAST_ERROR_MESSAGE
\echo 'last error code:' :LAST_ERROR_SQLSTATE

-- empty query

================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT
-- must have kept previous values
\echo 'last error message:' :LAST_ERROR_MESSAGE
\echo 'last error code:' :LAST_ERROR_SQLSTATE

-- other query error
DROP TABLE this_table_does_not_exist
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT
\echo 'last error message:' :LAST_ERROR_MESSAGE
\echo 'last error code:' :LAST_ERROR_SQLSTATE

-- nondefault verbosity error settings (except verbose, which is too unstable)
\set VERBOSITY terse
SELECT 1 UNION
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'last error message:' :LAST_ERROR_MESSAGE

\set VERBOSITY sqlstate
SELECT 1/0
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'last error message:' :LAST_ERROR_MESSAGE

\set VERBOSITY default

-- working \gdesc
SELECT 3 AS three, 4 AS four \gdesc
\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT

-- \gdesc with an error
SELECT 4 AS \gdesc
\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT
\echo 'last error message:' :LAST_ERROR_MESSAGE
\echo 'last error code:' :LAST_ERROR_SQLSTATE

-- check row count for a cursor-fetched query
\set FETCH_COUNT 10
select unique2 from tenk1 order by unique2 limit 19
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT

-- cursor-fetched query with an error after the first group
select 1/(15-unique2) from tenk1 order by unique2 limit 19
================================

\echo 'error:' :ERROR
\echo 'error code:' :SQLSTATE
\echo 'number of rows:' :ROW_COUNT
\echo 'last error message:' :LAST_ERROR_MESSAGE
\echo 'last error code:' :LAST_ERROR_SQLSTATE

\unset FETCH_COUNT

create schema testpart
================================


alter schema testpart owner to regress_partitioning_role
================================


-- only partition related object should be displayed
\dP test*apple*
\dPt test*apple*
\dPi test*apple*

drop table testtable_apple
================================


\dPt
\dPi

\dP testpart.*
\dP

\dPtn
\dPin
\dPn
\dPn testpart.*

drop table parent_tab cascade
================================


drop schema testpart
================================


-- \d on toast table (use pg_statistic's toast table, which has a known name)
\d pg_toast.pg_toast_2619

-- check printing info about access methods
\dA
\dA *
\dA h*
\dA foo
\dA foo bar
\dA+
\dA+ *
\dA+ h*
\dA+ foo
\dAc brin pg*.oid*
\dAf spgist
\dAf btree int4
\dAo+ btree float_ops
\dAo * pg_catalog.jsonb_path_ops
\dAp+ btree float_ops
\dAp * pg_catalog.uuid_ops

-- check \df, \do with argument specifications
\df *sqrt
\df *sqrt num*
\df int*pl
\df int*pl int4
\df int*pl * pg_catalog.int8
\df acl* aclitem[]
\df has_database_privilege oid text
\df has_database_privilege oid text -
\dfa bit* small*
\do - pg_catalog.int4
\do && anyarray *

-- AUTOCOMMIT

CREATE TABLE ac_test (a int)
================================

\set AUTOCOMMIT off

INSERT INTO ac_test VALUES (1)
================================


\set AUTOCOMMIT on
DROP TABLE ac_test
================================
  -- should be gone now

-- ON_ERROR_ROLLBACK

\set ON_ERROR_ROLLBACK on
CREATE TABLE oer_test (a int)
================================

\set ON_ERROR_ROLLBACK off

-- ECHO errors
\set ECHO errors
SELECT * FROM notexists
================================

\set ECHO none

================================


DROP SCHEMA IF EXISTS regress_rls_schema CASCADE
================================


CREATE SCHEMA regress_rls_schema
================================
 RETURN true
================================


-- user's security level must be higher than or equal to document's
CREATE POLICY p1 ON document AS PERMISSIVE
    USING (dlevel <= (SELECT seclv FROM uaccount WHERE pguser = current_user))
================================


-- try to create a policy of bogus type
CREATE POLICY p1 ON document AS UGLY
    USING (dlevel <= (SELECT seclv FROM uaccount WHERE pguser = current_user))
================================


-- but Dave isn't allowed to anything at cid 50 or above
-- this is to make sure that we sort the policies by name first
-- when applying WITH CHECK, a later INSERT by Dave should fail due
-- to p1r first
CREATE POLICY p2r ON document AS RESTRICTIVE TO regress_rls_dave
    USING (cid <> 44 AND cid < 50)
================================


-- and Dave isn't allowed to see manga documents
CREATE POLICY p1r ON document AS RESTRICTIVE TO regress_rls_dave
    USING (cid <> 44)
================================


\dp
\d document
SELECT * FROM pg_policies WHERE schemaname = 'regress_rls_schema' AND tablename = 'document' ORDER BY policyname
================================
 -- fail

-- only owner can change policies
ALTER POLICY p1 ON document USING (true)
================================
    --fail
DROP POLICY p1 ON document
================================

ALTER POLICY p1 ON document USING (dauthor = current_user)
================================

CREATE POLICY p2 ON category
    USING (CASE WHEN current_user = 'regress_rls_bob' THEN cid IN (11, 33)
           WHEN current_user = 'regress_rls_carol' THEN cid IN (22, 44)
           ELSE false END)
================================

101	1	aba
102	2	bbb
103	3	ccc
104	4	dad
\.

CREATE TABLE t2 (c float) INHERITS (t1)
================================

201	1	abc	1.1
202	2	bcd	2.2
203	3	cde	3.3
204	4	def	4.4
\.

CREATE TABLE t3 (id int not null primary key, c text, b text, a int)
================================

301	1	xxx	X
302	2	yyy	Y
303	3	zzz	Z
\.

CREATE POLICY p1 ON t1 FOR ALL TO PUBLIC USING (a % 2 = 0)
================================
 -- be even number
CREATE POLICY p2 ON t2 FOR ALL TO PUBLIC USING (a % 2 = 1)
================================


-- Create policy on parent
-- user's security level must be higher than or equal to document's
CREATE POLICY pp1 ON part_document AS PERMISSIVE
    USING (dlevel <= (SELECT seclv FROM uaccount WHERE pguser = current_user))
================================


-- Dave is only allowed to see cid < 55
CREATE POLICY pp1r ON part_document AS RESTRICTIVE TO regress_rls_dave
    USING (cid < 55)
================================


\d+ part_document
SELECT * FROM pg_policies WHERE schemaname = 'regress_rls_schema' AND tablename like '%part_document%' ORDER BY policyname
================================

CREATE POLICY pp3 ON part_document_satire AS RESTRICTIVE
    USING (cid < 55)
================================


-- only owner can change policies
ALTER POLICY pp1 ON part_document USING (true)
================================
    --fail
DROP POLICY pp1 ON part_document
================================

ALTER POLICY pp1 ON part_document USING (dauthor = current_user)
================================

CREATE POLICY pp3 ON part_document AS RESTRICTIVE
    USING ((SELECT dlevel <= seclv FROM uaccount WHERE pguser = current_user))
================================

CREATE POLICY d1 ON dependent FOR ALL
    TO PUBLIC
    USING (x = (SELECT d.x FROM dependee d WHERE d.y = y))
================================

CREATE POLICY r1 ON rec1 USING (x = (SELECT r.x FROM rec1 r WHERE y = r.y))
================================

ALTER POLICY r1 ON rec1 USING (x = (SELECT a FROM rec2 WHERE b = y))
================================

CREATE POLICY r2 ON rec2 USING (a = (SELECT x FROM rec1 WHERE y = b))
================================

ALTER POLICY r1 ON rec1 USING (x = (SELECT a FROM rec2v WHERE b = y))
================================

ALTER POLICY r2 ON rec2 USING (a = (SELECT x FROM rec1v WHERE y = b))
================================

CREATE POLICY r1 ON rec1 USING (x = (SELECT a FROM rec2v WHERE b = y))
================================

CREATE POLICY r2 ON rec2 USING (a = (SELECT x FROM rec1v WHERE y = b))
================================

INSERT INTO s1 (SELECT x, md5(x::text) FROM generate_series(-10,10) x)
================================

INSERT INTO s2 (SELECT x, md5(x::text) FROM generate_series(-6,6) x)
================================


CREATE POLICY p1 ON s1 USING (a in (select x from s2 where y like '%2f%'))
================================

CREATE POLICY p2 ON s2 USING (x in (select a from s1 where b like '%22%'))
================================

CREATE POLICY p3 ON s1 FOR INSERT WITH CHECK (a = (SELECT a FROM s1))
================================

DROP POLICY p3 on s1
================================

ALTER POLICY p2 ON s2 USING (x % 2 = 0)
================================

ALTER POLICY p1 ON s1 USING (a in (select x from v2))
================================

ALTER POLICY p2 ON s2 USING (x in (select a from s1 where b like '%d2%'))
================================

INSERT INTO b1 (SELECT x, md5(x::text) FROM generate_series(-10,10) x)
================================


CREATE POLICY p1 ON b1 USING (a % 2 = 0)
================================

DROP POLICY p1 ON document
================================

DROP POLICY p1r ON document
================================


CREATE POLICY p1 ON document FOR SELECT USING (true)
================================

CREATE POLICY p2 ON document FOR INSERT WITH CHECK (dauthor = current_user)
================================

CREATE POLICY p3 ON document FOR UPDATE
  USING (cid = (SELECT cid from category WHERE cname = 'novel'))
  WITH CHECK (dauthor = current_user)
================================

DROP POLICY p1 ON document
================================

DROP POLICY p2 ON document
================================

DROP POLICY p3 ON document
================================


CREATE POLICY p3_with_default ON document FOR UPDATE
  USING (cid = (SELECT cid from category WHERE cname = 'novel'))
================================

DROP POLICY p3_with_default ON document
================================


--
-- Test ALL policies with ON CONFLICT DO UPDATE (much the same as existing UPDATE
-- tests)
--
CREATE POLICY p3_with_all ON document FOR ALL
  USING (cid = (SELECT cid from category WHERE cname = 'novel'))
  WITH CHECK (dauthor = current_user)
================================


CREATE POLICY p1 ON z1 TO regress_rls_group1 USING (a % 2 = 0)
================================

CREATE POLICY p2 ON z1 TO regress_rls_group2 USING (a % 2 = 1)
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test2
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test3
================================


EXPLAIN (COSTS OFF) EXECUTE plancache_test
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test2
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test3
================================


EXPLAIN (COSTS OFF) EXECUTE plancache_test
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test2
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test3
================================


EXPLAIN (COSTS OFF) EXECUTE plancache_test
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test2
================================

EXPLAIN (COSTS OFF) EXECUTE plancache_test3
================================


CREATE POLICY p0 ON x1 FOR ALL USING (c = current_user)
================================

CREATE POLICY p1 ON x1 FOR SELECT USING (a % 2 = 0)
================================

CREATE POLICY p2 ON x1 FOR INSERT WITH CHECK (a % 2 = 1)
================================

CREATE POLICY p3 ON x1 FOR UPDATE USING (a % 2 = 0)
================================

CREATE POLICY p4 ON x1 FOR DELETE USING (a < 8)
================================


CREATE POLICY p1 ON y1 FOR ALL USING (a % 2 = 0)
================================

CREATE POLICY p2 ON y1 FOR SELECT USING (a > 2)
================================

CREATE POLICY p1 ON y1 FOR SELECT USING (a % 2 = 1)
================================
  --fail
CREATE POLICY p1 ON y2 FOR ALL USING (a % 2 = 0)
================================

INSERT INTO y2 (SELECT x, md5(x::text) FROM generate_series(0,20) x)
================================

CREATE POLICY p2 ON y2 USING (a % 3 = 0)
================================

CREATE POLICY p3 ON y2 USING (a % 4 = 0)
================================


CREATE POLICY p1 ON t1 TO regress_rls_bob USING ((a % 2) = 0)
================================

CREATE POLICY p2 ON t1 TO regress_rls_carol USING ((a % 4) = 0)
================================

-- Check plan
EXPLAIN (COSTS OFF) EXECUTE role_inval
================================

-- Check plan- should be different
EXPLAIN (COSTS OFF) EXECUTE role_inval
================================

-- Check plan- should be back to original
EXPLAIN (COSTS OFF) EXECUTE role_inval
================================

CREATE POLICY p1 ON t1 USING (a % 2 = 0)
================================


INSERT INTO t1 (SELECT x, md5(x::text) FROM generate_series(0,20) x)
================================

ALTER POLICY p1 ON t1 RENAME TO p1
================================


ALTER POLICY p1 ON t1 RENAME TO p2
================================

INSERT INTO t2 (SELECT * FROM t1)
================================

EXPLAIN (COSTS OFF) INSERT INTO t2 (SELECT * FROM t1)
================================

CREATE TABLE t3 AS SELECT * FROM t1
================================


CREATE POLICY blog_1 ON blog USING (id % 2 = 0)
================================

CREATE POLICY comment_1 ON comment USING (blog_id < 4)
================================

DROP POLICY p2 ON t1
================================

CREATE POLICY p1 ON copy_t USING (a % 2 = 0)
================================


INSERT INTO copy_t (SELECT x, md5(x::text) FROM generate_series(0,10) x)
================================
 --ok

-- Check COPY TO as user without permissions. SET row_security TO OFF
================================
 --fail - permission denied

-- Check COPY relation TO
================================
 keep it just one row to avoid reordering issues
RESET SESSION AUTHORIZATION
================================

CREATE POLICY p1 ON copy_rel_to USING (a % 2 = 0)
================================
 --ok

-- Check COPY TO as user without permissions. SET row_security TO OFF
================================
 --ok
1	abc
2	bcd
3	cde
4	def
\.
SET row_security TO ON
================================
 --ok
1	abc
2	bcd
3	cde
4	def
\.

-- Check COPY FROM as user with permissions.
SET SESSION AUTHORIZATION regress_rls_bob
================================
 --ok
1	abc
2	bcd
3	cde
4	def
\.

-- Check COPY FROM as user without permissions.
SET SESSION AUTHORIZATION regress_rls_carol
================================


CREATE POLICY p1 ON current_check FOR SELECT USING (currentid % 2 = 0)
================================

CREATE POLICY p2 ON current_check FOR DELETE USING (currentid = 4 AND rlsuser = current_user)
================================

CREATE POLICY p3 ON current_check FOR UPDATE USING (currentid = 4) WITH CHECK (rlsuser = current_user)
================================


DECLARE current_check_cursor SCROLL CURSOR FOR SELECT * FROM current_check
================================

-- Returns rows that can be seen according to SELECT policy, like plain SELECT
-- above (even rows)
FETCH ABSOLUTE 1 FROM current_check_cursor
================================

-- Still cannot UPDATE row 2 through cursor
UPDATE current_check SET payload = payload || '_new' WHERE CURRENT OF current_check_cursor RETURNING *
================================

-- Can update row 4 through cursor, which is the next visible row
FETCH RELATIVE 1 FROM current_check_cursor
================================

UPDATE current_check SET payload = payload || '_new' WHERE CURRENT OF current_check_cursor RETURNING *
================================

-- Plan should be a subquery TID scan
EXPLAIN (COSTS OFF) UPDATE current_check SET payload = payload WHERE CURRENT OF current_check_cursor
================================

-- Similarly can only delete row 4
FETCH ABSOLUTE 1 FROM current_check_cursor
================================

DELETE FROM current_check WHERE CURRENT OF current_check_cursor RETURNING *
================================

FETCH RELATIVE 1 FROM current_check_cursor
================================

DELETE FROM current_check WHERE CURRENT OF current_check_cursor RETURNING *
================================

CREATE TABLE coll_t (c) AS VALUES ('bar'::text)
================================

CREATE POLICY coll_p ON coll_t USING (c < ('foo'::text COLLATE "C"))
================================

CREATE TABLE tbl1 (c) AS VALUES ('bar'::text)
================================

CREATE POLICY P ON tbl1 TO regress_rls_eve, regress_rls_frank USING (true)
================================


ALTER POLICY p ON tbl1 TO regress_rls_frank USING (true)
================================


DROP POLICY p ON tbl1
================================

CREATE POLICY p ON t USING (c % 2 = 1)
================================


DROP POLICY p ON t
================================

CREATE TABLE t (c) AS VALUES ('bar'::text)
================================

CREATE POLICY p ON t USING (max(c))
================================


CREATE POLICY p1 ON r1 USING (true)
================================


CREATE POLICY p1 ON r2 FOR SELECT USING (true)
================================

CREATE POLICY p2 ON r2 FOR INSERT WITH CHECK (false)
================================

CREATE POLICY p3 ON r2 FOR UPDATE USING (false)
================================

CREATE POLICY p4 ON r2 FOR DELETE USING (false)
================================
 -- Deletes nothing

-- r2 can be used as a non-target relation in DML
INSERT INTO r1 SELECT a + 1 FROM r2 RETURNING *
================================


CREATE POLICY p1 ON r1 USING (false)
================================


-- Create policies on r2 which prevent the
-- owner from seeing any rows, but RI should
-- still see them.
CREATE POLICY p1 ON r2 USING (false)
================================


-- Reset r2 to no-RLS
DROP POLICY p1 ON r2
================================


-- Change r1 to not allow rows to be seen
CREATE POLICY p1 ON r1 USING (false)
================================


-- Create policies on r2 which prevent the
-- owner from seeing any rows, but RI should
-- still see them.
CREATE POLICY p1 ON r2 USING (false)
================================


-- Create policies on r2 which prevent the
-- owner from seeing any rows, but RI should
-- still see them.
CREATE POLICY p1 ON r2 USING (false)
================================


CREATE POLICY p1 ON r1 FOR SELECT USING (false)
================================

CREATE POLICY p2 ON r1 FOR INSERT WITH CHECK (true)
================================


CREATE POLICY p1 ON r1 FOR SELECT USING (a < 20)
================================

CREATE POLICY p2 ON r1 FOR UPDATE USING (a < 20) WITH CHECK (true)
================================

CREATE POLICY p3 ON r1 FOR INSERT WITH CHECK (true)
================================


CREATE POLICY dep_p1 ON dep1 TO regress_rls_bob USING (c1 > (select max(dep2.c1) from dep2))
================================

ALTER POLICY dep_p1 ON dep1 TO regress_rls_bob,regress_rls_carol
================================


ALTER POLICY dep_p1 ON dep1 USING (true)
================================


CREATE POLICY p1 ON dob_t1 TO regress_rls_dob_role1 USING (true)
================================

DROP OWNED BY regress_rls_dob_role1
================================

DROP POLICY p1 ON dob_t1
================================
 -- should fail, already gone

CREATE POLICY p1 ON dob_t1 TO regress_rls_dob_role1,regress_rls_dob_role2 USING (true)
================================

DROP OWNED BY regress_rls_dob_role1
================================

DROP POLICY p1 ON dob_t1
================================
 -- should succeed

-- same cases with duplicate polroles entries
CREATE POLICY p1 ON dob_t1 TO regress_rls_dob_role1,regress_rls_dob_role1 USING (true)
================================

DROP OWNED BY regress_rls_dob_role1
================================

DROP POLICY p1 ON dob_t1
================================
 -- should fail, already gone

CREATE POLICY p1 ON dob_t1 TO regress_rls_dob_role1,regress_rls_dob_role1,regress_rls_dob_role2 USING (true)
================================

DROP OWNED BY regress_rls_dob_role1
================================

DROP POLICY p1 ON dob_t1
================================
 -- should succeed

-- partitioned target
CREATE POLICY p1 ON dob_t2 TO regress_rls_dob_role1,regress_rls_dob_role2 USING (true)
================================

DROP OWNED BY regress_rls_dob_role1
================================

DROP POLICY p1 ON dob_t2
================================

CREATE POLICY p1 ON rls_tbl USING (EXISTS (SELECT 1 FROM ref_tbl))
================================

ALTER VIEW rls_view OWNER TO regress_rls_bob
================================

INSERT INTO rls_tbl SELECT x/10 FROM generate_series(1, 100) x
================================
 RETURN $1 < $2
================================

CREATE OPERATOR <<< (procedure = op_leak, leftarg = int, rightarg = int,
                     restrict = scalarltsel)
================================

SELECT * FROM rls_tbl WHERE a <<< 1000
================================

DROP OPERATOR <<< (int, int)
================================

CREATE POLICY p1 ON rls_tbl USING (rls_tbl >= ROW(1,1,1))
================================


INSERT INTO rls_tbl SELECT 10, 20, 30
================================

EXPLAIN (VERBOSE, COSTS OFF)
INSERT INTO rls_tbl
  SELECT * FROM (SELECT b, c FROM rls_tbl ORDER BY a) ss
================================

INSERT INTO rls_tbl
  SELECT * FROM (SELECT b, c FROM rls_tbl ORDER BY a) ss
================================


DROP SCHEMA regress_rls_schema CASCADE
================================


-- Arrange to have a few policies left over, for testing
-- pg_dump/pg_restore
CREATE SCHEMA regress_rls_schema
================================

CREATE POLICY p1 ON rls_tbl USING (c1 > 5)
================================

CREATE POLICY p2 ON rls_tbl FOR SELECT USING (c1 <= 3)
================================

CREATE POLICY p3 ON rls_tbl FOR UPDATE USING (c1 <= 3) WITH CHECK (c1 > 5)
================================

CREATE POLICY p4 ON rls_tbl FOR DELETE USING (c1 <= 3)
================================

CREATE POLICY p1 ON rls_tbl_force USING (c1 = 5) WITH CHECK (c1 < 5)
================================

CREATE POLICY p2 ON rls_tbl_force FOR SELECT USING (c1 = 8)
================================

CREATE POLICY p3 ON rls_tbl_force FOR UPDATE USING (c1 = 8) WITH CHECK (c1 >= 5)
================================

CREATE POLICY p4 ON rls_tbl_force FOR DELETE USING (c1 = 8)
================================


================================


-- Test generic object addressing/identification functions
CREATE SCHEMA addr_nsp
================================

CREATE FOREIGN DATA WRAPPER addr_fdw
================================

CREATE SERVER addr_fserv FOREIGN DATA WRAPPER addr_fdw
================================

CREATE TEXT SEARCH DICTIONARY addr_ts_dict (template=simple)
================================

CREATE TEXT SEARCH CONFIGURATION addr_ts_conf (copy=english)
================================

CREATE TEXT SEARCH TEMPLATE addr_ts_temp (lexize=dsimple_lexize)
================================

CREATE TEXT SEARCH PARSER addr_ts_prs
    (start = prsd_start, gettoken = prsd_nexttoken, end = prsd_end, lextypes = prsd_lextype)
================================

CREATE MATERIALIZED VIEW addr_nsp.genmatview AS SELECT * FROM addr_nsp.gentable
================================

CREATE TYPE addr_nsp.gencomptype AS (a int)
================================

CREATE TYPE addr_nsp.genenum AS ENUM ('one', 'two')
================================

CREATE FOREIGN TABLE addr_nsp.genftable (a int) SERVER addr_fserv
================================

CREATE AGGREGATE addr_nsp.genaggr(int4) (sfunc = int4pl, stype = int4)
================================
 $$
================================

CREATE POLICY genpol ON addr_nsp.gentable
================================

CREATE SERVER "integer" FOREIGN DATA WRAPPER addr_fdw
================================

ALTER DEFAULT PRIVILEGES FOR ROLE regress_addr_user IN SCHEMA public GRANT ALL ON TABLES TO regress_addr_user
================================

ALTER DEFAULT PRIVILEGES FOR ROLE regress_addr_user REVOKE DELETE ON TABLES FROM regress_addr_user
================================

-- this transform would be quite unsafe to leave lying around,
-- except that the SQL language pays no attention to transforms:
CREATE TRANSFORM FOR int LANGUAGE SQL (
	FROM SQL WITH FUNCTION prsd_lextype(internal),
	TO SQL WITH FUNCTION int4recv(internal))
================================

CREATE PUBLICATION addr_pub FOR TABLE addr_nsp.gentable
================================

CREATE PUBLICATION addr_pub_schema FOR ALL TABLES IN SCHEMA addr_nsp
================================

CREATE SUBSCRIPTION regress_addr_sub CONNECTION '' PUBLICATION bar WITH (connect = false, slot_name = NONE)
================================

CREATE STATISTICS addr_nsp.gentable_stat ON a, b FROM addr_nsp.gentable
================================


-- unrecognized object types
DO $$
DECLARE
	objtype text
================================

		EXCEPTION WHEN invalid_parameter_value THEN
			RAISE WARNING 'error for %: %', objtype, sqlerrm
================================

$$
================================


DO $$
DECLARE
	objtype text
================================

	names	text[]
================================

	args	text[]
================================

				EXCEPTION WHEN OTHERS THEN
						RAISE WARNING 'error for %,%,%: %', objtype, names, args, sqlerrm
================================

$$
================================


-- test successful cases
WITH objects (type, name, args) AS (VALUES
				('table', '{addr_nsp, gentable}'::text[], '{}'::text[]),
				('table', '{addr_nsp, parttable}'::text[], '{}'::text[]),
				('index', '{addr_nsp, gentable_pkey}', '{}'),
				('index', '{addr_nsp, parttable_pkey}', '{}'),
				('sequence', '{addr_nsp, gentable_a_seq}', '{}'),
				-- toast table
				('view', '{addr_nsp, genview}', '{}'),
				('materialized view', '{addr_nsp, genmatview}', '{}'),
				('foreign table', '{addr_nsp, genftable}', '{}'),
				('table column', '{addr_nsp, gentable, b}', '{}'),
				('foreign table column', '{addr_nsp, genftable, a}', '{}'),
				('aggregate', '{addr_nsp, genaggr}', '{int4}'),
				('function', '{pg_catalog, pg_identify_object}', '{pg_catalog.oid, pg_catalog.oid, int4}'),
				('procedure', '{addr_nsp, proc}', '{int4}'),
				('type', '{pg_catalog._int4}', '{}'),
				('type', '{addr_nsp.gendomain}', '{}'),
				('type', '{addr_nsp.gencomptype}', '{}'),
				('type', '{addr_nsp.genenum}', '{}'),
				('cast', '{int8}', '{int4}'),
				('collation', '{default}', '{}'),
				('table constraint', '{addr_nsp, gentable, a_chk}', '{}'),
				('domain constraint', '{addr_nsp.gendomain}', '{domconstr}'),
				('conversion', '{pg_catalog, koi8_r_to_mic}', '{}'),
				('default value', '{addr_nsp, gentable, b}', '{}'),
				('language', '{plpgsql}', '{}'),
				-- large object
				('operator', '{+}', '{int4, int4}'),
				('operator class', '{btree, int4_ops}', '{}'),
				('operator family', '{btree, integer_ops}', '{}'),
				('operator of access method', '{btree,integer_ops,1}', '{integer,integer}'),
				('function of access method', '{btree,integer_ops,2}', '{integer,integer}'),
				('rule', '{addr_nsp, genview, _RETURN}', '{}'),
				('trigger', '{addr_nsp, gentable, t}', '{}'),
				('schema', '{addr_nsp}', '{}'),
				('text search parser', '{addr_ts_prs}', '{}'),
				('text search dictionary', '{addr_ts_dict}', '{}'),
				('text search template', '{addr_ts_temp}', '{}'),
				('text search configuration', '{addr_ts_conf}', '{}'),
				('role', '{regress_addr_user}', '{}'),
				-- database
				-- tablespace
				('foreign-data wrapper', '{addr_fdw}', '{}'),
				('server', '{addr_fserv}', '{}'),
				('user mapping', '{regress_addr_user}', '{integer}'),
				('default acl', '{regress_addr_user,public}', '{r}'),
				('default acl', '{regress_addr_user}', '{r}'),
				-- extension
				-- event trigger
				('policy', '{addr_nsp, gentable, genpol}', '{}'),
				('transform', '{int}', '{sql}'),
				('access method', '{btree}', '{}'),
				('publication', '{addr_pub}', '{}'),
				('publication namespace', '{addr_nsp}', '{addr_pub_schema}'),
				('publication relation', '{addr_nsp, gentable}', '{addr_pub}'),
				('subscription', '{regress_addr_sub}', '{}'),
				('statistics object', '{addr_nsp, gentable_stat}', '{}')
        )
SELECT (pg_identify_object(addr1.classid, addr1.objid, addr1.objsubid)).*,
	-- test roundtrip through pg_identify_object_as_address
	ROW(pg_identify_object(addr1.classid, addr1.objid, addr1.objsubid)) =
	ROW(pg_identify_object(addr2.classid, addr2.objid, addr2.objsubid))
	  FROM objects, pg_get_object_address(type, name, args) addr1,
			pg_identify_object_as_address(classid, objid, objsubid) ioa(typ,nms,args),
			pg_get_object_address(typ, nms, ioa.args) as addr2
	ORDER BY addr1.classid, addr1.objid, addr1.objsubid
================================


---
--- Cleanup resources
---
DROP FOREIGN DATA WRAPPER addr_fdw CASCADE
================================

DROP PUBLICATION addr_pub
================================

DROP PUBLICATION addr_pub_schema
================================

DROP SUBSCRIPTION regress_addr_sub
================================


DROP SCHEMA addr_nsp CASCADE
================================


DROP OWNED BY regress_addr_user
================================


--
-- Checks for invalid objects
--
-- Make sure that NULL handling is correct.
\pset null 'NULL'
-- Temporarily disable fancy output, so as future additions never create
-- a large amount of diffs.
\a\t

-- Keep this list in the same order as getObjectIdentityParts()
-- in objectaddress.c.
WITH objects (classid, objid, objsubid) AS (VALUES
    ('pg_class'::regclass, 0, 0), -- no relation
    ('pg_class'::regclass, 'pg_class'::regclass, 100), -- no column for relation
    ('pg_proc'::regclass, 0, 0), -- no function
    ('pg_type'::regclass, 0, 0), -- no type
    ('pg_cast'::regclass, 0, 0), -- no cast
    ('pg_collation'::regclass, 0, 0), -- no collation
    ('pg_constraint'::regclass, 0, 0), -- no constraint
    ('pg_conversion'::regclass, 0, 0), -- no conversion
    ('pg_attrdef'::regclass, 0, 0), -- no default attribute
    ('pg_language'::regclass, 0, 0), -- no language
    ('pg_largeobject'::regclass, 0, 0), -- no large object, no error
    ('pg_operator'::regclass, 0, 0), -- no operator
    ('pg_opclass'::regclass, 0, 0), -- no opclass, no need to check for no access method
    ('pg_opfamily'::regclass, 0, 0), -- no opfamily
    ('pg_am'::regclass, 0, 0), -- no access method
    ('pg_amop'::regclass, 0, 0), -- no AM operator
    ('pg_amproc'::regclass, 0, 0), -- no AM proc
    ('pg_rewrite'::regclass, 0, 0), -- no rewrite
    ('pg_trigger'::regclass, 0, 0), -- no trigger
    ('pg_namespace'::regclass, 0, 0), -- no schema
    ('pg_statistic_ext'::regclass, 0, 0), -- no statistics
    ('pg_ts_parser'::regclass, 0, 0), -- no TS parser
    ('pg_ts_dict'::regclass, 0, 0), -- no TS dictionnary
    ('pg_ts_template'::regclass, 0, 0), -- no TS template
    ('pg_ts_config'::regclass, 0, 0), -- no TS configuration
    ('pg_authid'::regclass, 0, 0), -- no role
    ('pg_database'::regclass, 0, 0), -- no database
    ('pg_tablespace'::regclass, 0, 0), -- no tablespace
    ('pg_foreign_data_wrapper'::regclass, 0, 0), -- no FDW
    ('pg_foreign_server'::regclass, 0, 0), -- no server
    ('pg_user_mapping'::regclass, 0, 0), -- no user mapping
    ('pg_default_acl'::regclass, 0, 0), -- no default ACL
    ('pg_extension'::regclass, 0, 0), -- no extension
    ('pg_event_trigger'::regclass, 0, 0), -- no event trigger
    ('pg_policy'::regclass, 0, 0), -- no policy
    ('pg_publication'::regclass, 0, 0), -- no publication
    ('pg_publication_rel'::regclass, 0, 0), -- no publication relation
    ('pg_subscription'::regclass, 0, 0), -- no subscription
    ('pg_transform'::regclass, 0, 0) -- no transformation
  )
SELECT ROW(pg_identify_object(objects.classid, objects.objid, objects.objsubid))
         AS ident,
       ROW(pg_identify_object_as_address(objects.classid, objects.objid, objects.objsubid))
         AS addr,
       pg_describe_object(objects.classid, objects.objid, objects.objsubid)
         AS descr
FROM objects
ORDER BY objects.classid, objects.objid, objects.objsubid
================================


-- restore normal output mode
\a\t

================================


-- UNION DISTINCT requires hashable type
WITH RECURSIVE t(n) AS (
    VALUES (1::money)
UNION
    SELECT n+1::money FROM t WHERE n < 100::money
)
SELECT sum(n) FROM t
================================


\d+ sums_1_100

-- corner case in which sub-WITH gets initialized first
with recursive q as (
      select * from department
    union all
      (with x as (select * from q)
       select * from x)
    )
select * from q limit 24
================================


--
-- get all paths from "second level" nodes to leaf nodes
--
WITH RECURSIVE t(id, path) AS (
    VALUES(1,ARRAY[]::integer[])
UNION ALL
    SELECT tree.id, t.path || tree.id
    FROM tree JOIN t ON (tree.parent_id = t.id)
)
SELECT t1.*, t2.* FROM t AS t1 JOIN t AS t2 ON
	(t1.path[1] = t2.path[1] AND
	array_upper(t1.path,1) = 1 AND
	array_upper(t2.path,1) > 1)
	ORDER BY t1.id, t2.id
================================


-- just count 'em
WITH RECURSIVE t(id, path) AS (
    VALUES(1,ARRAY[]::integer[])
UNION ALL
    SELECT tree.id, t.path || tree.id
    FROM tree JOIN t ON (tree.parent_id = t.id)
)
SELECT t1.id, count(t2.*) FROM t AS t1 JOIN t AS t2 ON
	(t1.path[1] = t2.path[1] AND
	array_upper(t1.path,1) = 1 AND
	array_upper(t2.path,1) > 1)
	GROUP BY t1.id
	ORDER BY t1.id
================================


-- this variant tickled a whole-row-variable bug in 8.4devel
WITH RECURSIVE t(id, path) AS (
    VALUES(1,ARRAY[]::integer[])
UNION ALL
    SELECT tree.id, t.path || tree.id
    FROM tree JOIN t ON (tree.parent_id = t.id)
)
SELECT t1.id, t2.path, t2 FROM t AS t1 JOIN t AS t2 ON
(t1.id=t2.id)
================================


explain (verbose, costs off)
with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set seq
select * from search_graph order by seq
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set seq
select * from search_graph order by seq
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union distinct
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set seq
select * from search_graph order by seq
================================


explain (verbose, costs off)
with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search breadth first by f, t set seq
select * from search_graph order by seq
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search breadth first by f, t set seq
select * from search_graph order by seq
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union distinct
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search breadth first by f, t set seq
select * from search_graph order by seq
================================


-- various syntax errors
with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by foo, tar set seq
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set label
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t, f set seq
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set seq
select * from search_graph order by seq
================================


with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	(select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t)
) search depth first by f, t set seq
select * from search_graph order by seq
================================


-- test ruleutils and view expansion
create temp view v_search as
with recursive search_graph(f, t, label) as (
	select * from graph0 g
	union all
	select g.*
	from graph0 g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set seq
select f, t, label from search_graph
================================


with recursive search_graph(f, t, label, is_cycle, path) as (
	select *, false, array[row(g.f, g.t)] from graph g
	union all
	select g.*, row(g.f, g.t) = any(path), path || row(g.f, g.t)
	from graph g, search_graph sg
	where g.f = sg.t and not is_cycle
)
select * from search_graph
================================


-- UNION DISTINCT exercises row type hashing support
with recursive search_graph(f, t, label, is_cycle, path) as (
	select *, false, array[row(g.f, g.t)] from graph g
	union distinct
	select g.*, row(g.f, g.t) = any(path), path || row(g.f, g.t)
	from graph g, search_graph sg
	where g.f = sg.t and not is_cycle
)
select * from search_graph
================================


-- ordering by the path column has same effect as SEARCH DEPTH FIRST
with recursive search_graph(f, t, label, is_cycle, path) as (
	select *, false, array[row(g.f, g.t)] from graph g
	union all
	select g.*, row(g.f, g.t) = any(path), path || row(g.f, g.t)
	from graph g, search_graph sg
	where g.f = sg.t and not is_cycle
)
select * from search_graph order by path
================================


-- CYCLE clause

explain (verbose, costs off)
with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union distinct
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle to 'Y' default 'N' using path
select * from search_graph
================================


-- multiple CTEs
with recursive
graph(f, t, label) as (
  values (1, 2, 'arc 1 -> 2'),
         (1, 3, 'arc 1 -> 3'),
         (2, 3, 'arc 2 -> 3'),
         (1, 4, 'arc 1 -> 4'),
         (4, 5, 'arc 4 -> 5'),
         (5, 1, 'arc 5 -> 1')
),
search_graph(f, t, label) as (
        select * from graph g
        union all
        select g.*
        from graph g, search_graph sg
        where g.f = sg.t
) cycle f, t set is_cycle to true default false using path
select f, t, label from search_graph
================================


-- star expansion
with recursive a as (
	select 1 as b
	union all
	select * from a
) cycle b set c using p
select * from a
================================


-- search+cycle
with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set seq
  cycle f, t set is_cycle using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) search breadth first by f, t set seq
  cycle f, t set is_cycle using path
select * from search_graph
================================


-- various syntax errors
with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle foo, tar set is_cycle using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle to true default 55 using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle to point '(1,1)' default point '(0,0)' using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set label to true default false using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle to true default false using label
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set foo to true default false using foo
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t, f set is_cycle to true default false using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set foo
  cycle f, t set foo to true default false using path
select * from search_graph
================================


with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) search depth first by f, t set foo
  cycle f, t set is_cycle to true default false using foo
select * from search_graph
================================


-- test ruleutils and view expansion
create temp view v_cycle1 as
with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle using path
select f, t, label from search_graph
================================


create temp view v_cycle2 as
with recursive search_graph(f, t, label) as (
	select * from graph g
	union all
	select g.*
	from graph g, search_graph sg
	where g.f = sg.t
) cycle f, t set is_cycle to 'Y' default 'N' using path
select f, t, label from search_graph
================================

INSERT INTO y SELECT generate_series(1, 10)
================================


WITH t AS (
	SELECT a FROM y
)
INSERT INTO y
SELECT a+20 FROM t RETURNING *
================================

INSERT INTO y SELECT generate_series(1, 10)
================================


DROP RULE y_rule ON y
================================


-- check merging of outer CTE with CTE in a rule action
CREATE TEMP TABLE bug6051 AS
  select i from generate_series(1,3) as t(i)
================================


WITH t1 AS ( DELETE FROM bug6051 RETURNING * )
INSERT INTO bug6051 SELECT * FROM t1
================================


WITH t1 AS ( DELETE FROM bug6051 RETURNING * )
INSERT INTO bug6051 SELECT * FROM t1
================================


WITH t1 AS ( DELETE FROM bug6051 RETURNING * )
INSERT INTO bug6051 SELECT * FROM t1
================================


-- silly example to verify that hasModifyingCTE flag is propagated
CREATE TEMP TABLE bug6051_3 AS
  SELECT a FROM generate_series(11,13) AS a
================================


WITH t1 AS ( DELETE FROM bug6051_3 RETURNING * )
  INSERT INTO bug6051_3 SELECT * FROM t1
================================


-- data-modifying WITH in a modifying statement
WITH t AS (
    DELETE FROM y
    WHERE a <= 10
    RETURNING *
)
INSERT INTO y SELECT -a FROM t RETURNING *
================================


-- data-modifying WITH containing INSERT...ON CONFLICT DO UPDATE
CREATE TABLE withz AS SELECT i AS k, (i || ' v')::text v FROM generate_series(1, 16, 3) i
================================

INSERT INTO y SELECT generate_series(1, 3)
================================

INSERT INTO y SELECT generate_series(1, 10)
================================

  return new
================================

$$ LANGUAGE plpgsql
================================


DROP TRIGGER y_trig ON y
================================


DROP TRIGGER y_trig ON y
================================

  return null
================================

$$ LANGUAGE plpgsql
================================


DROP TRIGGER y_trig ON y
================================

CREATE OR REPLACE RULE y_rule AS ON INSERT TO y
  DO INSTEAD (NOTIFY foo
================================
 NOTIFY bar)
================================

DROP RULE y_rule ON y
================================


-- check that parser lookahead for WITH doesn't cause any odd behavior
create table foo (with baz)
================================
  -- fail, WITH is a reserved word
create table foo (with ordinality)
================================

with test as (select 42) insert into test select * from test
================================


================================


--
-- Test the "high colormap" logic with single characters and ranges that
-- exceed the MAX_SIMPLE_CHR cutoff, here assumed to be less than U+2000.
--

-- trivial cases:
SELECT 'a' ~ U&'a\24D0' AS t
================================

SELECT 'a' ~ U&'a\24D1' AS f
================================


================================


-- create a materialized view with no data, and confirm correct behavior
EXPLAIN (costs off)
  CREATE MATERIALIZED VIEW mvtest_tm AS SELECT type, sum(amt) AS totamt FROM mvtest_t GROUP BY type WITH NO DATA
================================

CREATE MATERIALIZED VIEW mvtest_tm AS SELECT type, sum(amt) AS totamt FROM mvtest_t GROUP BY type WITH NO DATA
================================

REFRESH MATERIALIZED VIEW mvtest_tm
================================


-- create various views
EXPLAIN (costs off)
  CREATE MATERIALIZED VIEW mvtest_tvm AS SELECT * FROM mvtest_tv ORDER BY type
================================

CREATE MATERIALIZED VIEW mvtest_tvm AS SELECT * FROM mvtest_tv ORDER BY type
================================

CREATE MATERIALIZED VIEW mvtest_tmm AS SELECT sum(totamt) AS grandtot FROM mvtest_tm
================================

CREATE MATERIALIZED VIEW mvtest_tvmm AS SELECT sum(totamt) AS grandtot FROM mvtest_tvm
================================

EXPLAIN (costs off)
  CREATE MATERIALIZED VIEW mvtest_tvvm AS SELECT * FROM mvtest_tvv
================================

CREATE MATERIALIZED VIEW mvtest_tvvm AS SELECT * FROM mvtest_tvv
================================

CREATE MATERIALIZED VIEW mvtest_bb AS SELECT * FROM mvtest_tvvmv
================================


-- check that plans seem reasonable
\d+ mvtest_tvm
\d+ mvtest_tvm
\d+ mvtest_tvvm
\d+ mvtest_bb

-- test schema behavior
CREATE SCHEMA mvtest_mvschema
================================

ALTER MATERIALIZED VIEW mvtest_tvm SET SCHEMA mvtest_mvschema
================================

\d+ mvtest_tvm
\d+ mvtest_tvmm
SET search_path = mvtest_mvschema, public
================================

\d+ mvtest_tvm

-- modify the underlying table data
INSERT INTO mvtest_t VALUES (6, 'z', 13)
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_tm
================================

REFRESH MATERIALIZED VIEW mvtest_tvm
================================

REFRESH MATERIALIZED VIEW mvtest_tmm
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_tvmm
================================

REFRESH MATERIALIZED VIEW mvtest_tvmm
================================

REFRESH MATERIALIZED VIEW mvtest_tvvm
================================


-- test diemv when the mv does not exist
DROP MATERIALIZED VIEW IF EXISTS no_such_mv
================================


-- make sure invalid combination of options is prohibited
REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_tvmm WITH NO DATA
================================

\d+ mvtest_vt2
CREATE MATERIALIZED VIEW mv_test2 AS SELECT moo, 2*moo FROM mvtest_vt2 UNION ALL SELECT moo, 3*moo FROM mvtest_vt2
================================

\d+ mv_test2
CREATE MATERIALIZED VIEW mv_test3 AS SELECT * FROM mv_test2 WHERE moo = 12345
================================


-- test that duplicate values on unique index prevent refresh
CREATE TABLE mvtest_foo(a, b) AS VALUES(1, 10)
================================

CREATE MATERIALIZED VIEW mvtest_mv AS SELECT * FROM mvtest_foo
================================

INSERT INTO mvtest_foo SELECT * FROM mvtest_foo
================================

REFRESH MATERIALIZED VIEW mvtest_mv
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_mv
================================


-- make sure that all columns covered by unique indexes works
CREATE TABLE mvtest_foo(a, b, c) AS VALUES(1, 2, 3)
================================

CREATE MATERIALIZED VIEW mvtest_mv AS SELECT * FROM mvtest_foo
================================

REFRESH MATERIALIZED VIEW mvtest_mv
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_mv
================================


-- allow subquery to reference unpopulated matview if WITH NO DATA is specified
CREATE MATERIALIZED VIEW mvtest_mv1 AS SELECT 1 AS col1 WITH NO DATA
================================

CREATE MATERIALIZED VIEW mvtest_mv2 AS SELECT * FROM mvtest_mv1
  WHERE col1 = (SELECT LEAST(col1) FROM mvtest_mv1) WITH NO DATA
================================

DROP MATERIALIZED VIEW mvtest_mv1 CASCADE
================================

CREATE MATERIALIZED VIEW mvtest_boxmv AS SELECT * FROM mvtest_boxes
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_boxmv
================================

CREATE MATERIALIZED VIEW mvtest_mv_v (ii, jj, kk) AS SELECT i, j FROM mvtest_v
================================
 -- error
CREATE MATERIALIZED VIEW mvtest_mv_v (ii, jj) AS SELECT i, j FROM mvtest_v
================================
 -- ok
CREATE MATERIALIZED VIEW mvtest_mv_v_2 (ii) AS SELECT i, j FROM mvtest_v
================================
 -- ok
CREATE MATERIALIZED VIEW mvtest_mv_v_3 (ii, jj, kk) AS SELECT i, j FROM mvtest_v WITH NO DATA
================================
 -- error
CREATE MATERIALIZED VIEW mvtest_mv_v_3 (ii, jj) AS SELECT i, j FROM mvtest_v WITH NO DATA
================================
 -- ok
CREATE MATERIALIZED VIEW mvtest_mv_v_4 (ii) AS SELECT i, j FROM mvtest_v WITH NO DATA
================================

REFRESH MATERIALIZED VIEW mvtest_mv_v
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_mv_v
================================

REFRESH MATERIALIZED VIEW mvtest_mv_v_2
================================

REFRESH MATERIALIZED VIEW mvtest_mv_v_3
================================

REFRESH MATERIALIZED VIEW mvtest_mv_v_4
================================


-- Check that unknown literals are converted to "text" in CREATE MATVIEW,
-- so that we don't end up with unknown-type columns.
CREATE MATERIALIZED VIEW mv_unspecified_types AS
  SELECT 42 as i, 42.5 as num, 'foo' as u, 'foo'::unknown as u2, null as n
================================

\d+ mv_unspecified_types
SELECT * FROM mv_unspecified_types
================================

DROP MATERIALIZED VIEW mv_unspecified_types
================================


-- make sure that create WITH NO DATA does not plan the query (bug #13907)
create materialized view mvtest_error as select 1/0 as x
================================
  -- fail
create materialized view mvtest_error as select 1/0 as x with no data
================================

refresh materialized view mvtest_error
================================
  -- fail here
drop materialized view mvtest_error
================================


-- make sure that matview rows can be referenced as source rows (bug #9398)
CREATE TABLE mvtest_v AS SELECT generate_series(1,10) AS a
================================

CREATE MATERIALIZED VIEW mvtest_mv_v AS SELECT a FROM mvtest_v WHERE a <= 5
================================

-- this test case also checks for ambiguity in the queries issued by
-- refresh_by_match_merge(), by choosing column names that intentionally
-- duplicate all the aliases used in those queries
CREATE TABLE mvtest_foo_data AS SELECT i,
  i+1 AS tid,
  md5(random()::text) AS mv,
  md5(random()::text) AS newdata,
  md5(random()::text) AS newdata2,
  md5(random()::text) AS diff
  FROM generate_series(1, 10) i
================================

CREATE MATERIALIZED VIEW mvtest_mv_foo AS SELECT * FROM mvtest_foo_data
================================

CREATE MATERIALIZED VIEW mvtest_mv_foo AS SELECT * FROM mvtest_foo_data
================================

CREATE MATERIALIZED VIEW IF NOT EXISTS mvtest_mv_foo AS SELECT * FROM mvtest_foo_data
================================

REFRESH MATERIALIZED VIEW mvtest_mv_foo
================================

REFRESH MATERIALIZED VIEW CONCURRENTLY mvtest_mv_foo
================================

DROP OWNED BY regress_user_mvtest CASCADE
================================

  CREATE MATERIALIZED VIEW mvtest2 AS SELECT 1 AS x WITH NO DATA
================================

$$ LANGUAGE plpgsql
================================


-- INSERT privileges if relation owner is not allowed to insert.
CREATE SCHEMA matview_schema
================================

ALTER DEFAULT PRIVILEGES FOR ROLE regress_matview_user
  REVOKE INSERT ON TABLES FROM regress_matview_user
================================

CREATE MATERIALIZED VIEW matview_schema.mv_withdata1 (a) AS
  SELECT generate_series(1, 10) WITH DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE MATERIALIZED VIEW matview_schema.mv_withdata2 (a) AS
  SELECT generate_series(1, 10) WITH DATA
================================

REFRESH MATERIALIZED VIEW matview_schema.mv_withdata2
================================

CREATE MATERIALIZED VIEW matview_schema.mv_nodata1 (a) AS
  SELECT generate_series(1, 10) WITH NO DATA
================================

EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE MATERIALIZED VIEW matview_schema.mv_nodata2 (a) AS
  SELECT generate_series(1, 10) WITH NO DATA
================================

REFRESH MATERIALIZED VIEW matview_schema.mv_nodata2
================================


ALTER DEFAULT PRIVILEGES FOR ROLE regress_matview_user
  GRANT INSERT ON TABLES TO regress_matview_user
================================


DROP SCHEMA matview_schema CASCADE
================================


-- CREATE MATERIALIZED VIEW ... IF NOT EXISTS
CREATE MATERIALIZED VIEW matview_ine_tab AS SELECT 1
================================

CREATE MATERIALIZED VIEW matview_ine_tab AS SELECT 1 / 0
================================
 -- error
CREATE MATERIALIZED VIEW IF NOT EXISTS matview_ine_tab AS
  SELECT 1 / 0
================================
 -- ok
CREATE MATERIALIZED VIEW matview_ine_tab AS
  SELECT 1 / 0 WITH NO DATA
================================
 -- error
CREATE MATERIALIZED VIEW IF NOT EXISTS matview_ine_tab AS
  SELECT 1 / 0 WITH NO DATA
================================
 -- ok
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE MATERIALIZED VIEW matview_ine_tab AS
    SELECT 1 / 0
================================
 -- error
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE MATERIALIZED VIEW IF NOT EXISTS matview_ine_tab AS
    SELECT 1 / 0
================================
 -- ok
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE MATERIALIZED VIEW matview_ine_tab AS
    SELECT 1 / 0 WITH NO DATA
================================
 -- error
EXPLAIN (ANALYZE, COSTS OFF, SUMMARY OFF, TIMING OFF)
  CREATE MATERIALIZED VIEW IF NOT EXISTS matview_ine_tab AS
    SELECT 1 / 0 WITH NO DATA
================================
 -- ok
DROP MATERIALIZED VIEW matview_ine_tab
================================


================================


INSERT INTO quad_poly_tbl
	SELECT (x - 1) * 100 + y, polygon(circle(point(x * 10, y * 10), 1 + (x + y) % 10))
	FROM generate_series(1, 100) x,
		 generate_series(1, 100) y
================================


INSERT INTO quad_poly_tbl
	SELECT i, polygon '((200, 300),(210, 310),(230, 290))'
	FROM generate_series(10001, 11000) AS i
================================


CREATE TEMP TABLE quad_poly_tbl_ord_seq2 AS
SELECT rank() OVER (ORDER BY p <-> point '123,456') n, p <-> point '123,456' dist, id
FROM quad_poly_tbl WHERE p <@ polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p << polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p << polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p &< polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p &< polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p && polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p && polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p &> polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p &> polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p >> polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p >> polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p <<| polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p <<| polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p &<| polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p &<| polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p |&> polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p |&> polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p |>> polygon '((300,300),(400,600),(600,500),(700,200))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p |>> polygon '((300,300),(400,600),(600,500),(700,200))'
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM quad_poly_tbl WHERE p ~= polygon '((200, 300),(210, 310),(230, 290))'
================================

SELECT count(*) FROM quad_poly_tbl WHERE p ~= polygon '((200, 300),(210, 310),(230, 290))'
================================


EXPLAIN (COSTS OFF)
SELECT rank() OVER (ORDER BY p <-> point '123,456') n, p <-> point '123,456' dist, id
FROM quad_poly_tbl WHERE p <@ polygon '((300,300),(400,600),(600,500),(700,200))'
================================


CREATE TEMP TABLE quad_poly_tbl_ord_idx2 AS
SELECT rank() OVER (ORDER BY p <-> point '123,456') n, p <-> point '123,456' dist, id
FROM quad_poly_tbl WHERE p <@ polygon '((300,300),(400,600),(600,500),(700,200))'
================================


================================


================================
--
-- PATH
--

--DROP TABLE PATH_TBL
================================


================================

SELECT q1, q2, q1 / q2 AS divide, q1 % q2 AS mod FROM INT8_TBL
================================


SELECT q1, float8(q1) FROM INT8_TBL
================================

SELECT q2, float8(q2) FROM INT8_TBL
================================



-- bit operations

SELECT q1, q2, q1 & q2 AS "and", q1 | q2 AS "or", q1 # q2 AS "xor", ~q1 AS "not" FROM INT8_TBL
================================

SELECT q1, q1 << 2 AS "shl", q1 >> 3 AS "shr" FROM INT8_TBL
================================


-- corner case
SELECT (-1::int8<<63)::text
================================

SELECT ((-1::int8<<63)+1)::text
================================
 -- overflow

================================

CREATE PUBLICATION testpub_default
================================


COMMENT ON PUBLICATION testpub_default IS 'test publication'
================================

CREATE PUBLICATION testpib_ins_trunct WITH (publish = insert)
================================


ALTER PUBLICATION testpub_default SET (publish = update)
================================


-- error cases
CREATE PUBLICATION testpub_xxx WITH (foo)
================================

CREATE PUBLICATION testpub_xxx WITH (publish = 'cluster, vacuum')
================================

CREATE PUBLICATION testpub_xxx WITH (publish_via_partition_root = 'true', publish_via_partition_root = '0')
================================


\dRp

ALTER PUBLICATION testpub_default SET (publish = 'insert, update, delete')
================================


\dRp

--- adding tables
CREATE SCHEMA pub_test
================================

CREATE PUBLICATION testpub_foralltables FOR ALL TABLES WITH (publish = 'insert')
================================

ALTER PUBLICATION testpub_foralltables SET (publish = 'insert, update')
================================

-- fail - can't add to for all tables publication
ALTER PUBLICATION testpub_foralltables ADD TABLE testpub_tbl2
================================

-- fail - can't drop from all tables publication
ALTER PUBLICATION testpub_foralltables DROP TABLE testpub_tbl2
================================

-- fail - can't add to for all tables publication
ALTER PUBLICATION testpub_foralltables SET TABLE pub_test.testpub_nopk
================================


-- fail - can't add schema to 'FOR ALL TABLES' publication
ALTER PUBLICATION testpub_foralltables ADD ALL TABLES IN SCHEMA pub_test
================================

-- fail - can't drop schema from 'FOR ALL TABLES' publication
ALTER PUBLICATION testpub_foralltables DROP ALL TABLES IN SCHEMA pub_test
================================

-- fail - can't set schema to 'FOR ALL TABLES' publication
ALTER PUBLICATION testpub_foralltables SET ALL TABLES IN SCHEMA pub_test
================================

CREATE PUBLICATION testpub_fortable FOR TABLE testpub_tbl1
================================

-- should be able to add schema to 'FOR TABLE' publication
ALTER PUBLICATION testpub_fortable ADD ALL TABLES IN SCHEMA pub_test
================================

\dRp+ testpub_fortable
-- should be able to drop schema from 'FOR TABLE' publication
ALTER PUBLICATION testpub_fortable DROP ALL TABLES IN SCHEMA pub_test
================================

\dRp+ testpub_fortable
-- should be able to set schema to 'FOR TABLE' publication
ALTER PUBLICATION testpub_fortable SET ALL TABLES IN SCHEMA pub_test
================================

\dRp+ testpub_fortable

SET client_min_messages = 'ERROR'
================================

CREATE PUBLICATION testpub_forschema FOR ALL TABLES IN SCHEMA pub_test
================================

-- fail - can't create publication with schema and table of the same schema
CREATE PUBLICATION testpub_for_tbl_schema FOR ALL TABLES IN SCHEMA pub_test, TABLE pub_test.testpub_nopk
================================

-- fail - can't add a table of the same schema to the schema publication
ALTER PUBLICATION testpub_forschema ADD TABLE pub_test.testpub_nopk
================================

-- fail - can't drop a table from the schema publication which isn't in the
-- publication
ALTER PUBLICATION testpub_forschema DROP TABLE pub_test.testpub_nopk
================================

-- should be able to set table to schema publication
ALTER PUBLICATION testpub_forschema SET TABLE pub_test.testpub_nopk
================================

\dRp+ testpub_forschema

SELECT pubname, puballtables FROM pg_publication WHERE pubname = 'testpub_foralltables'
================================

\d+ testpub_tbl2
\dRp+ testpub_foralltables

DROP TABLE testpub_tbl2
================================

DROP PUBLICATION testpub_foralltables, testpub_fortable, testpub_forschema
================================

CREATE PUBLICATION testpub3 FOR TABLE testpub_tbl3
================================

CREATE PUBLICATION testpub4 FOR TABLE ONLY testpub_tbl3
================================

\dRp+ testpub3
\dRp+ testpub4

DROP TABLE testpub_tbl3, testpub_tbl3a
================================

DROP PUBLICATION testpub3, testpub4
================================

CREATE PUBLICATION testpub_forparted
================================

CREATE PUBLICATION testpub_forparted1
================================

ALTER PUBLICATION testpub_forparted1 SET (publish='insert')
================================

-- only parent is listed as being in publication, not the partition
ALTER PUBLICATION testpub_forparted ADD TABLE testpub_parted
================================

\dRp+ testpub_forparted
-- should now fail, because parent's publication replicates updates
UPDATE testpub_parted1 SET a = 1
================================

ALTER PUBLICATION testpub_forparted SET (publish_via_partition_root = true)
================================

\dRp+ testpub_forparted
-- still fail, because parent's publication replicates updates
UPDATE testpub_parted2 SET a = 2
================================

ALTER PUBLICATION testpub_forparted DROP TABLE testpub_parted
================================

DROP PUBLICATION testpub_forparted, testpub_forparted1
================================

CREATE PUBLICATION testpub_foralltables FOR ALL TABLES
================================

DROP PUBLICATION testpub_foralltables
================================


-- fail - view
CREATE PUBLICATION testpub_fortbl FOR TABLE testpub_view
================================

-- fail - temporary table
CREATE PUBLICATION testpub_fortemptbl FOR TABLE testpub_temptbl
================================

-- fail - unlogged table
CREATE PUBLICATION testpub_forunloggedtbl FOR TABLE testpub_unloggedtbl
================================


-- fail - system table
CREATE PUBLICATION testpub_forsystemtbl FOR TABLE pg_publication
================================

CREATE PUBLICATION testpub_fortbl FOR TABLE testpub_tbl1, pub_test.testpub_nopk
================================

-- fail - already added
ALTER PUBLICATION testpub_fortbl ADD TABLE testpub_tbl1
================================

-- fail - already added
CREATE PUBLICATION testpub_fortbl FOR TABLE testpub_tbl1
================================


\dRp+ testpub_fortbl

-- fail - view
ALTER PUBLICATION testpub_default ADD TABLE testpub_view
================================


ALTER PUBLICATION testpub_default ADD TABLE testpub_tbl1
================================

ALTER PUBLICATION testpub_default SET TABLE testpub_tbl1
================================

ALTER PUBLICATION testpub_default ADD TABLE pub_test.testpub_nopk
================================


ALTER PUBLICATION testpib_ins_trunct ADD TABLE pub_test.testpub_nopk, testpub_tbl1
================================


\d+ pub_test.testpub_nopk
\d+ testpub_tbl1
\dRp+ testpub_default

ALTER PUBLICATION testpub_default DROP TABLE testpub_tbl1, pub_test.testpub_nopk
================================

-- fail - nonexistent
ALTER PUBLICATION testpub_default DROP TABLE pub_test.testpub_nopk
================================


\d+ testpub_tbl1

-- permissions
SET ROLE regress_publication_user2
================================

CREATE PUBLICATION testpub2
================================

CREATE PUBLICATION testpub2
================================
  -- ok
CREATE PUBLICATION testpub3 FOR ALL TABLES IN SCHEMA pub_test
================================
  -- fail
CREATE PUBLICATION testpub3
================================


ALTER PUBLICATION testpub2 ADD TABLE testpub_tbl1
================================
  -- fail
ALTER PUBLICATION testpub3 ADD ALL TABLES IN SCHEMA pub_test
================================

ALTER PUBLICATION testpub2 ADD TABLE testpub_tbl1
================================
  -- ok

DROP PUBLICATION testpub2
================================

DROP PUBLICATION testpub3
================================


\dRp+ testpub_default

-- fail - must be owner of publication
SET ROLE regress_publication_user_dummy
================================

ALTER PUBLICATION testpub_default RENAME TO testpub_dummy
================================


ALTER PUBLICATION testpub_default RENAME TO testpub_foo
================================


\dRp testpub_foo

-- rename back to keep the rest simple
ALTER PUBLICATION testpub_foo RENAME TO testpub_default
================================


ALTER PUBLICATION testpub_default OWNER TO regress_publication_user2
================================


\dRp testpub_default

-- adding schemas and tables
CREATE SCHEMA pub_test1
================================

CREATE SCHEMA pub_test2
================================

CREATE SCHEMA pub_test3
================================

CREATE SCHEMA "CURRENT_SCHEMA"
================================

CREATE PUBLICATION testpub1_forschema FOR ALL TABLES IN SCHEMA pub_test1
================================

\dRp+ testpub1_forschema

CREATE PUBLICATION testpub2_forschema FOR ALL TABLES IN SCHEMA pub_test1, pub_test2, pub_test3
================================

\dRp+ testpub2_forschema

-- check create publication on CURRENT_SCHEMA
CREATE PUBLICATION testpub3_forschema FOR ALL TABLES IN SCHEMA CURRENT_SCHEMA
================================

CREATE PUBLICATION testpub4_forschema FOR ALL TABLES IN SCHEMA "CURRENT_SCHEMA"
================================

CREATE PUBLICATION testpub5_forschema FOR ALL TABLES IN SCHEMA CURRENT_SCHEMA, "CURRENT_SCHEMA"
================================

CREATE PUBLICATION testpub6_forschema FOR ALL TABLES IN SCHEMA "CURRENT_SCHEMA", CURRENT_SCHEMA
================================

CREATE PUBLICATION testpub_fortable FOR TABLE "CURRENT_SCHEMA"."CURRENT_SCHEMA"
================================


\dRp+ testpub3_forschema
\dRp+ testpub4_forschema
\dRp+ testpub5_forschema
\dRp+ testpub6_forschema
\dRp+ testpub_fortable

-- check create publication on CURRENT_SCHEMA where search_path is not set
SET SEARCH_PATH=''
================================

CREATE PUBLICATION testpub_forschema FOR ALL TABLES IN SCHEMA CURRENT_SCHEMA
================================


-- check create publication on CURRENT_SCHEMA where TABLE/ALL TABLES in SCHEMA
-- is not specified
CREATE PUBLICATION testpub_forschema1 FOR CURRENT_SCHEMA
================================


-- check create publication on CURRENT_SCHEMA along with FOR TABLE
CREATE PUBLICATION testpub_forschema1 FOR TABLE CURRENT_SCHEMA
================================


-- check create publication on a schema that does not exist
CREATE PUBLICATION testpub_forschema FOR ALL TABLES IN SCHEMA non_existent_schema
================================


-- check create publication on a system schema
CREATE PUBLICATION testpub_forschema FOR ALL TABLES IN SCHEMA pg_catalog
================================


-- check create publication on an object which is not schema
CREATE PUBLICATION testpub1_forschema1 FOR ALL TABLES IN SCHEMA testpub_view
================================


-- dropping the schema should reflect the change in publication
DROP SCHEMA pub_test3
================================

\dRp+ testpub2_forschema

-- renaming the schema should reflect the change in publication
ALTER SCHEMA pub_test1 RENAME to pub_test1_renamed
================================

\dRp+ testpub2_forschema

ALTER SCHEMA pub_test1_renamed RENAME to pub_test1
================================

\dRp+ testpub2_forschema

-- alter publication add schema
ALTER PUBLICATION testpub1_forschema ADD ALL TABLES IN SCHEMA pub_test2
================================

\dRp+ testpub1_forschema

-- add non existent schema
ALTER PUBLICATION testpub1_forschema ADD ALL TABLES IN SCHEMA non_existent_schema
================================

\dRp+ testpub1_forschema

-- add a schema which is already added to the publication
ALTER PUBLICATION testpub1_forschema ADD ALL TABLES IN SCHEMA pub_test1
================================

\dRp+ testpub1_forschema

-- alter publication drop schema
ALTER PUBLICATION testpub1_forschema DROP ALL TABLES IN SCHEMA pub_test2
================================

\dRp+ testpub1_forschema

-- drop schema that is not present in the publication
ALTER PUBLICATION testpub1_forschema DROP ALL TABLES IN SCHEMA pub_test2
================================

\dRp+ testpub1_forschema

-- drop a schema that does not exist in the system
ALTER PUBLICATION testpub1_forschema DROP ALL TABLES IN SCHEMA non_existent_schema
================================

\dRp+ testpub1_forschema

-- drop all schemas
ALTER PUBLICATION testpub1_forschema DROP ALL TABLES IN SCHEMA pub_test1
================================

\dRp+ testpub1_forschema

-- alter publication set multiple schema
ALTER PUBLICATION testpub1_forschema SET ALL TABLES IN SCHEMA pub_test1, pub_test2
================================

\dRp+ testpub1_forschema

-- alter publication set non-existent schema
ALTER PUBLICATION testpub1_forschema SET ALL TABLES IN SCHEMA non_existent_schema
================================

\dRp+ testpub1_forschema

-- alter publication set it duplicate schemas should set the schemas after
-- removing the duplicate schemas
ALTER PUBLICATION testpub1_forschema SET ALL TABLES IN SCHEMA pub_test1, pub_test1
================================

\dRp+ testpub1_forschema

-- cleanup pub_test1 schema for invalidation tests
ALTER PUBLICATION testpub2_forschema DROP ALL TABLES IN SCHEMA pub_test1
================================

DROP PUBLICATION testpub3_forschema, testpub4_forschema, testpub5_forschema, testpub6_forschema, testpub_fortable
================================

DROP SCHEMA "CURRENT_SCHEMA" CASCADE
================================

ALTER PUBLICATION testpub1_forschema DROP ALL TABLES IN SCHEMA pub_test1
================================

ALTER PUBLICATION testpub1_forschema SET ALL TABLES IN SCHEMA pub_test1
================================


-- verify invalidation of partition table having parent and child tables in
-- different schema
CREATE SCHEMA pub_testpart1
================================

CREATE SCHEMA pub_testpart2
================================

CREATE PUBLICATION testpubpart_forschema FOR ALL TABLES IN SCHEMA pub_testpart1
================================


DROP PUBLICATION testpubpart_forschema
================================

CREATE PUBLICATION testpubpart_forschema FOR ALL TABLES IN SCHEMA pub_testpart2
================================

CREATE PUBLICATION testpub3_forschema
================================

\dRp+ testpub3_forschema
ALTER PUBLICATION testpub3_forschema SET ALL TABLES IN SCHEMA pub_test1
================================

\dRp+ testpub3_forschema

-- create publication including both 'FOR TABLE' and 'FOR ALL TABLES IN SCHEMA'
SET client_min_messages = 'ERROR'
================================

CREATE PUBLICATION testpub_forschema_fortable FOR ALL TABLES IN SCHEMA pub_test1, TABLE pub_test2.tbl1
================================

CREATE PUBLICATION testpub_fortable_forschema FOR TABLE pub_test2.tbl1, ALL TABLES IN SCHEMA pub_test1
================================


\dRp+ testpub_forschema_fortable
\dRp+ testpub_fortable_forschema

-- fail specifying table without any of 'FOR ALL TABLES IN SCHEMA' or
--'FOR TABLE' or 'FOR ALL TABLES'
CREATE PUBLICATION testpub_error FOR pub_test2.tbl1
================================


DROP PUBLICATION testpub_default
================================

DROP PUBLICATION testpib_ins_trunct
================================

DROP PUBLICATION testpub_fortbl
================================

DROP PUBLICATION testpub1_forschema
================================

DROP PUBLICATION testpub2_forschema
================================

DROP PUBLICATION testpub3_forschema
================================

DROP PUBLICATION testpub_forschema_fortable
================================

DROP PUBLICATION testpub_fortable_forschema
================================

DROP PUBLICATION testpubpart_forschema
================================


DROP SCHEMA pub_test CASCADE
================================

DROP SCHEMA pub_test1 CASCADE
================================

DROP SCHEMA pub_test2 CASCADE
================================

DROP SCHEMA pub_testpart1 CASCADE
================================

DROP SCHEMA pub_testpart2 CASCADE
================================

CREATE SCHEMA sch1
================================

CREATE SCHEMA sch2
================================

-- Schema publication that does not include the schema that has the parent table
CREATE PUBLICATION pub FOR ALL TABLES IN SCHEMA sch2 WITH (PUBLISH_VIA_PARTITION_ROOT=1)
================================


DROP PUBLICATION pub
================================

-- Table publication that does not include the parent table
CREATE PUBLICATION pub FOR TABLE sch2.tbl1_part1 WITH (PUBLISH_VIA_PARTITION_ROOT=1)
================================


DROP PUBLICATION pub
================================

-- Schema publication that does not include the schema that has the parent table
CREATE PUBLICATION pub FOR ALL TABLES IN SCHEMA sch2 WITH (PUBLISH_VIA_PARTITION_ROOT=0)
================================


DROP PUBLICATION pub
================================

-- Table publication that does not include the parent table
CREATE PUBLICATION pub FOR TABLE sch2.tbl1_part1 WITH (PUBLISH_VIA_PARTITION_ROOT=0)
================================


DROP PUBLICATION pub
================================

CREATE PUBLICATION pub FOR ALL TABLES IN SCHEMA sch1 WITH (PUBLISH_VIA_PARTITION_ROOT=1)
================================

DROP PUBLICATION pub
================================

DROP SCHEMA sch1 cascade
================================

DROP SCHEMA sch2 cascade
================================


================================

SELECT txid_current() \gset
SELECT txid_current_if_assigned() IS NOT DISTINCT FROM BIGINT :'txid_current'
================================

SELECT txid_status(:rolledback) AS rolledback
================================

SELECT txid_status(:inprogress) AS inprogress
================================

  RAISE EXCEPTION 'didn''t ERROR at xid in the future as expected'
================================

EXCEPTION
  WHEN invalid_parameter_value THEN
    RAISE NOTICE 'Got expected error for xid in the future'
================================

$$
================================

SELECT test_future_xid_status(:inprogress + 10000)
================================


================================

INSERT INTO pagg_tab SELECT i % 20, i % 30, to_char(i % 12, 'FM0000'), i % 30 FROM generate_series(0, 2999) i
================================


-- When GROUP BY clause matches
================================
 full aggregation is performed for each partition.
EXPLAIN (COSTS OFF)
SELECT c, sum(a), avg(b), count(*), min(a), max(b) FROM pagg_tab GROUP BY c HAVING avg(d) < 15 ORDER BY 1, 2, 3
================================


-- When GROUP BY clause does not match
================================
 partial aggregation is performed for each partition.
EXPLAIN (COSTS OFF)
SELECT a, sum(b), avg(b), count(*), min(a), max(b) FROM pagg_tab GROUP BY a HAVING avg(d) < 15 ORDER BY 1, 2, 3
================================


-- When GROUP BY clause does not match
================================
 partial aggregation is performed for each partition.
EXPLAIN (COSTS OFF)
SELECT a, sum(b), avg(b), count(*) FROM pagg_tab GROUP BY 1 HAVING avg(d) < 15 ORDER BY 1, 2, 3
================================


-- ORDERED SET within the aggregate.
-- Full aggregation
================================
 since all the rows that belong to the same group come
-- from the same partition, having an ORDER BY within the aggregate doesn't
-- make any difference.
EXPLAIN (COSTS OFF)
SELECT c, sum(b order by a) FROM pagg_tab GROUP BY c ORDER BY 1, 2
================================

-- Since GROUP BY clause does not match with PARTITION KEY
================================
 we need to do
-- partial aggregation. However, ORDERED SET are not partial safe and thus
-- partitionwise aggregation plan is not generated.
EXPLAIN (COSTS OFF)
SELECT a, sum(b order by a) FROM pagg_tab GROUP BY a ORDER BY 1, 2
================================


INSERT INTO pagg_tab1 SELECT i % 30, i % 20 FROM generate_series(0, 299, 2) i
================================

INSERT INTO pagg_tab2 SELECT i % 20, i % 30 FROM generate_series(0, 299, 3) i
================================


-- When GROUP BY clause matches
================================
 full aggregation is performed for each partition.
EXPLAIN (COSTS OFF)
SELECT t1.x, sum(t1.y), count(*) FROM pagg_tab1 t1, pagg_tab2 t2 WHERE t1.x = t2.y GROUP BY t1.x ORDER BY 1, 2, 3
================================


-- Check with whole-row reference
================================
 partitionwise aggregation does not apply
EXPLAIN (COSTS OFF)
SELECT t1.x, sum(t1.y), count(t1) FROM pagg_tab1 t1, pagg_tab2 t2 WHERE t1.x = t2.y GROUP BY t1.x ORDER BY 1, 2, 3
================================


-- When GROUP BY clause does not match
================================
 partial aggregation is performed for each partition.
-- Also test GroupAggregate paths by disabling hash aggregates.
SET enable_hashagg TO false
================================

INSERT INTO pagg_tab_m SELECT i % 30, i % 40, i % 50 FROM generate_series(0, 2999) i
================================


INSERT INTO pagg_tab_ml SELECT i % 30, i % 10, to_char(i % 4, 'FM0000') FROM generate_series(0, 29999) i
================================


INSERT INTO pagg_tab_para SELECT i % 30, i % 20 FROM generate_series(0, 29999) i
================================


-- When GROUP BY clause matches
================================
 full aggregation is performed for each partition.
EXPLAIN (COSTS OFF)
SELECT x, sum(y), avg(y), count(*) FROM pagg_tab_para GROUP BY x HAVING avg(y) < 7 ORDER BY 1, 2, 3
================================


-- When GROUP BY clause does not match
================================
 partial aggregation is performed for each partition.
EXPLAIN (COSTS OFF)
SELECT y, sum(x), avg(x), count(*) FROM pagg_tab_para GROUP BY y HAVING avg(x) < 12 ORDER BY 1, 2, 3
================================


================================
--
-- Create access method tests
--

-- Make gist2 over gisthandler. In fact, it would be a synonym to gist.
CREATE ACCESS METHOD gist2 TYPE INDEX HANDLER gisthandler
================================


-- Verify return type checks for handlers
CREATE ACCESS METHOD bogus TYPE INDEX HANDLER int4in
================================

CREATE ACCESS METHOD bogus TYPE INDEX HANDLER heap_tableam_handler
================================


-- Make operator class for boxes using gist2
CREATE OPERATOR CLASS box_ops DEFAULT
	FOR TYPE box USING gist2 AS
	OPERATOR 1	<<,
	OPERATOR 2	&<,
	OPERATOR 3	&&,
	OPERATOR 4	&>,
	OPERATOR 5	>>,
	OPERATOR 6	~=,
	OPERATOR 7	@>,
	OPERATOR 8	<@,
	OPERATOR 9	&<|,
	OPERATOR 10	<<|,
	OPERATOR 11	|>>,
	OPERATOR 12	|&>,
	FUNCTION 1	gist_box_consistent(internal, box, smallint, oid, internal),
	FUNCTION 2	gist_box_union(internal, internal),
	-- don't need compress, decompress, or fetch functions
	FUNCTION 5	gist_box_penalty(internal, internal, internal),
	FUNCTION 6	gist_box_picksplit(internal, internal),
	FUNCTION 7	gist_box_same(box, box, internal)
================================


-- Now check the results from plain indexscan
================================
 temporarily drop existing
-- index grect2ind to ensure it doesn't capture the plan
BEGIN
================================


EXPLAIN (COSTS OFF)
SELECT * FROM fast_emp4000
    WHERE home_base <@ '(200,200),(2000,1000)'::box
    ORDER BY (home_base[0])[0]
================================

SELECT * FROM fast_emp4000
    WHERE home_base <@ '(200,200),(2000,1000)'::box
    ORDER BY (home_base[0])[0]
================================


EXPLAIN (COSTS OFF)
SELECT count(*) FROM fast_emp4000 WHERE home_base && '(1000,1000,0,0)'::box
================================

SELECT count(*) FROM fast_emp4000 WHERE home_base && '(1000,1000,0,0)'::box
================================


-- Try to drop access method: fail because of dependent objects
DROP ACCESS METHOD gist2
================================

LOCK TABLE fast_emp4000
================================

DROP ACCESS METHOD gist2 CASCADE
================================



-- Create a heap2 table am handler with heapam handler
CREATE ACCESS METHOD heap2 TYPE TABLE HANDLER heap_tableam_handler
================================


-- Verify return type checks for handlers
CREATE ACCESS METHOD bogus TYPE TABLE HANDLER int4in
================================

CREATE ACCESS METHOD bogus TYPE TABLE HANDLER bthandler
================================


-- CREATE TABLE AS
CREATE TABLE tableam_tblas_heap2 USING heap2 AS SELECT * FROM tableam_tbl_heap2
================================


-- CREATE VIEW doesn't support USING
CREATE VIEW tableam_view_heap2 USING heap2 AS SELECT * FROM tableam_tbl_heap2
================================


-- CREATE MATERIALIZED VIEW does support USING
CREATE MATERIALIZED VIEW tableam_tblmv_heap2 USING heap2 AS SELECT * FROM tableam_tbl_heap2
================================


-- ALTER TABLE SET ACCESS METHOD
CREATE TABLE heaptable USING heap AS
  SELECT a, repeat(a::text, 100) FROM generate_series(1,9) AS a
================================

ALTER TABLE heaptable SET ACCESS METHOD heap2
================================

-- No support for multiple subcommands
ALTER TABLE heaptable SET ACCESS METHOD heap, SET ACCESS METHOD heap2
================================

ALTER TABLE am_partitioned SET ACCESS METHOD heap2
================================

CREATE TABLE tableam_tblas_heapx AS SELECT * FROM tableam_tbl_heapx
================================

CREATE MATERIALIZED VIEW tableam_tblmv_heapx USING heap2 AS SELECT * FROM tableam_tbl_heapx
================================

CREATE FOREIGN DATA WRAPPER fdw_heap2 VALIDATOR postgresql_fdw_validator
================================

CREATE SERVER fs_heap2 FOREIGN DATA WRAPPER fdw_heap2 
================================

CREATE FOREIGN table tableam_fdw_heapx () SERVER fs_heap2
================================


-- Third, check that we can neither create a table using a nonexistent
-- AM, nor using an index AM
CREATE TABLE i_am_a_failure() USING ""
================================


-- Drop table access method, which fails as objects depends on it
DROP ACCESS METHOD heap2
================================


-- we intentionally leave the objects created above alive, to verify pg_dump support

================================


================================
--
-- CREATE_CAST
--

-- Create some types to test with
CREATE TYPE casttesttype
================================


CREATE TYPE casttesttype (
   internallength = variable,
   input = casttesttype_in,
   output = casttesttype_out,
   alignment = int4
)
================================
 $$
================================
 -- fails, as there's no cast

-- Try binary coercion cast
CREATE CAST (text AS casttesttype) WITHOUT FUNCTION
================================
 -- should work
DROP CAST (text AS casttesttype)
================================
 -- cleanup

-- Try IMPLICIT binary coercion cast
CREATE CAST (text AS casttesttype) WITHOUT FUNCTION AS IMPLICIT
================================
 -- No cast yet, should fail

CREATE CAST (int4 AS casttesttype) WITH INOUT
================================
 -- Should work now

DROP CAST (int4 AS casttesttype)
================================
 $$
================================


CREATE CAST (int4 AS casttesttype) WITH FUNCTION int4_casttesttype(int4) AS IMPLICIT
================================
 -- Should work now

================================

            INSERT INTO clean_aborted_self SELECT g.i, 'rolling back in a sec' FROM generate_series(1, 100) g(i)
================================

	    -- just some error that's not normally thrown
	    RAISE reading_sql_data_not_permitted USING MESSAGE = 'round and round again'
================================

	EXCEPTION WHEN reading_sql_data_not_permitted THEN END
================================
$$
================================


================================


-- Subplans and initplans in the RETURNING list

INSERT INTO foo SELECT f1+10, f2, f3+99 FROM foo
  RETURNING *, f1+112 IN (SELECT q1 FROM int8_tbl) AS subplan,
    EXISTS(SELECT * FROM int4_tbl) AS initplan
================================


CREATE RULE voo_i AS ON INSERT TO voo DO INSTEAD
  INSERT INTO foo VALUES(new.*, 57)
================================


-- fails, incompatible list:
CREATE OR REPLACE RULE voo_i AS ON INSERT TO voo DO INSTEAD
  INSERT INTO foo VALUES(new.*, 57) RETURNING *
================================


CREATE OR REPLACE RULE voo_i AS ON INSERT TO voo DO INSTEAD
  INSERT INTO foo VALUES(new.*, 57) RETURNING f1, f2
================================


-- Check aliased target relation
INSERT INTO foo AS bar DEFAULT VALUES RETURNING *
================================
 -- ok
INSERT INTO foo AS bar DEFAULT VALUES RETURNING foo.*
================================
 -- fails, wrong name
INSERT INTO foo AS bar DEFAULT VALUES RETURNING bar.*
================================
 -- ok
INSERT INTO foo AS bar DEFAULT VALUES RETURNING bar.f3
================================
 -- ok

================================


INSERT INTO bmscantest
  SELECT (r%53), (r%59), 'foooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo'
  FROM generate_series(1,70000) r
================================


================================


-- prosrc should never be null
================================
 it can be empty only if prosqlbody isn't null
SELECT p1.oid, p1.proname
FROM pg_proc as p1
WHERE prosrc IS NULL
================================


-- currently, no built-in functions should be SECURITY DEFINER
================================


-- Considering only built-in procs (prolang = 12), look for multiple uses
-- of the same internal function (ie, matching prosrc fields).  It's OK to
-- have several entries with different pronames for the same internal function,
-- but conflicts in the number of arguments and other critical items should
-- be complained of.  (We don't check data types here
================================
 see next query.)
-- Note: ignore aggregate functions here, since they all point to the same
-- dummy built-in function.

SELECT p1.oid, p1.proname, p2.oid, p2.proname
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid < p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    (p1.prokind != 'a' OR p2.prokind != 'a') AND
    (p1.prolang != p2.prolang OR
     p1.prokind != p2.prokind OR
     p1.prosecdef != p2.prosecdef OR
     p1.proleakproof != p2.proleakproof OR
     p1.proisstrict != p2.proisstrict OR
     p1.proretset != p2.proretset OR
     p1.provolatile != p2.provolatile OR
     p1.pronargs != p2.pronargs)
================================


SELECT DISTINCT p1.proargtypes[0]::regtype, p2.proargtypes[0]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    p1.prosrc NOT LIKE E'range\\_constructor_' AND
    p2.prosrc NOT LIKE E'range\\_constructor_' AND
    p1.prosrc NOT LIKE E'multirange\\_constructor_' AND
    p2.prosrc NOT LIKE E'multirange\\_constructor_' AND
    (p1.proargtypes[0] < p2.proargtypes[0])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[1]::regtype, p2.proargtypes[1]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    p1.prosrc NOT LIKE E'range\\_constructor_' AND
    p2.prosrc NOT LIKE E'range\\_constructor_' AND
    p1.prosrc NOT LIKE E'multirange\\_constructor_' AND
    p2.prosrc NOT LIKE E'multirange\\_constructor_' AND
    (p1.proargtypes[1] < p2.proargtypes[1])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[2]::regtype, p2.proargtypes[2]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    (p1.proargtypes[2] < p2.proargtypes[2])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[3]::regtype, p2.proargtypes[3]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    (p1.proargtypes[3] < p2.proargtypes[3])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[4]::regtype, p2.proargtypes[4]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    (p1.proargtypes[4] < p2.proargtypes[4])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[5]::regtype, p2.proargtypes[5]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    (p1.proargtypes[5] < p2.proargtypes[5])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[6]::regtype, p2.proargtypes[6]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    (p1.proargtypes[6] < p2.proargtypes[6])
ORDER BY 1, 2
================================


SELECT DISTINCT p1.proargtypes[7]::regtype, p2.proargtypes[7]::regtype
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid != p2.oid AND
    p1.prosrc = p2.prosrc AND
    p1.prolang = 12 AND p2.prolang = 12 AND
    p1.prokind != 'a' AND p2.prokind != 'a' AND
    (p1.proargtypes[7] < p2.proargtypes[7])
ORDER BY 1, 2
================================



-- Look for functions that accept cstring and are neither datatype input
-- functions nor encoding conversion functions.  It's almost never a good
-- idea to use cstring input for a function meant to be called from SQL
================================


-- Check that proallargtypes matches proargtypes
SELECT p1.oid, p1.proname, p1.proargtypes, p1.proallargtypes, p1.proargmodes
FROM pg_proc as p1
WHERE proallargtypes IS NOT NULL AND
  ARRAY(SELECT unnest(proargtypes)) <>
  ARRAY(SELECT proallargtypes[i]
        FROM generate_series(1, array_length(proallargtypes, 1)) g(i)
        WHERE proargmodes IS NULL OR proargmodes[i] IN ('i', 'b', 'v'))
================================


-- Check for prosupport functions with the wrong signature
SELECT p1.oid, p1.proname, p2.oid, p2.proname
FROM pg_proc AS p1, pg_proc AS p2
WHERE p2.oid = p1.prosupport AND
    (p2.prorettype != 'internal'::regtype OR p2.proretset OR p2.pronargs != 1
     OR p2.proargtypes[0] != 'internal'::regtype)
================================


-- List of built-in leakproof functions
--
-- Leakproof functions should only be added after carefully
-- scrutinizing all possibly executed codepaths for possible
-- information leaks. Don't add functions here unless you know what a
-- leakproof function is. If unsure, don't mark it as such.

-- temporarily disable fancy output, so catalog changes create less diff noise
\a\t

SELECT p1.oid::regprocedure
FROM pg_proc p1 JOIN pg_namespace pn
     ON pronamespace = pn.oid
WHERE nspname = 'pg_catalog' AND proleakproof
ORDER BY 1
================================


-- restore normal output mode
\a\t

-- List of functions used by libpq's fe-lobj.c
--
-- If the output of this query changes, you probably broke libpq.
-- lo_initialize() assumes that there will be at most one match for
-- each listed name.
select proname, oid from pg_catalog.pg_proc
where proname in (
  'lo_open',
  'lo_close',
  'lo_creat',
  'lo_create',
  'lo_unlink',
  'lo_lseek',
  'lo_lseek64',
  'lo_tell',
  'lo_tell64',
  'lo_truncate',
  'lo_truncate64',
  'loread',
  'lowrite')
and pronamespace = (select oid from pg_catalog.pg_namespace
                    where nspname = 'pg_catalog')
order by 1
================================


-- Look for cast functions that don't have the right signature.  The
-- argument and result types in pg_proc must be the same as, or binary
-- compatible with, what it says in pg_cast.
-- As a special case, we allow casts from CHAR(n) that use functions
-- declared to take TEXT.  This does not pass the binary-coercibility test
-- because CHAR(n)-to-TEXT normally invokes rtrim().  However, the results
-- are the same, so long as the function is one that ignores trailing blanks.

SELECT c.*
FROM pg_cast c, pg_proc p
WHERE c.castfunc = p.oid AND
    (p.pronargs < 1 OR p.pronargs > 3
     OR NOT (binary_coercible(c.castsource, p.proargtypes[0])
             OR (c.castsource = 'character'::regtype AND
                 p.proargtypes[0] = 'text'::regtype))
     OR NOT binary_coercible(p.prorettype, c.casttarget))
================================


SELECT c.*
FROM pg_cast c, pg_proc p
WHERE c.castfunc = p.oid AND
    ((p.pronargs > 1 AND p.proargtypes[1] != 'int4'::regtype) OR
     (p.pronargs > 2 AND p.proargtypes[2] != 'bool'::regtype))
================================


-- Look for conprocs that don't have the expected signature.

SELECT p.oid, p.proname, c.oid, c.conname
FROM pg_proc p, pg_conversion c
WHERE p.oid = c.conproc AND
    (p.prorettype != 'int4'::regtype OR p.proretset OR
     p.pronargs != 6 OR
     p.proargtypes[0] != 'int4'::regtype OR
     p.proargtypes[1] != 'int4'::regtype OR
     p.proargtypes[2] != 'cstring'::regtype OR
     p.proargtypes[3] != 'internal'::regtype OR
     p.proargtypes[4] != 'int4'::regtype OR
     p.proargtypes[5] != 'bool'::regtype)
================================


-- Check for conprocs that don't perform the specific conversion that
-- pg_conversion alleges they do, by trying to invoke each conversion
-- on some simple ASCII data.  (The conproc should throw an error if
-- it doesn't accept the encodings that are passed to it.)
-- Unfortunately, we can't test non-default conprocs this way, because
-- there is no way to ask convert() to invoke them, and we cannot call
-- them directly from SQL.  But there are no non-default built-in
-- conversions anyway.
-- (Similarly, this doesn't cope with any search path issues.)

SELECT p1.oid, p1.conname
FROM pg_conversion as p1
WHERE condefault AND
    convert('ABC'::bytea, pg_encoding_to_char(conforencoding),
            pg_encoding_to_char(contoencoding)) != 'ABC'
================================


-- Look for commutative operators that don't commute.
-- DEFINITIONAL NOTE: If A.oprcom = B, then x A y has the same result as y B x.
-- We expect that B will always say that B.oprcom = A as well
================================
 that's not
-- inherently essential, but it would be inefficient not to mark it so.

SELECT p1.oid, p1.oprcode, p2.oid, p2.oprcode
FROM pg_operator AS p1, pg_operator AS p2
WHERE p1.oprcom = p2.oid AND
    (p1.oprkind != 'b' OR
     p1.oprleft != p2.oprright OR
     p1.oprright != p2.oprleft OR
     p1.oprresult != p2.oprresult OR
     p1.oid != p2.oprcom)
================================


-- Look for negatory operators that don't agree.
-- DEFINITIONAL NOTE: If A.oprnegate = B, then both A and B must yield
-- boolean results, and (x A y) == ! (x B y), or the equivalent for
-- single-operand operators.
-- We expect that B will always say that B.oprnegate = A as well
================================
 that's not
-- inherently essential, but it would be inefficient not to mark it so.
-- Also, A and B had better not be the same operator.

SELECT p1.oid, p1.oprcode, p2.oid, p2.oprcode
FROM pg_operator AS p1, pg_operator AS p2
WHERE p1.oprnegate = p2.oid AND
    (p1.oprkind != p2.oprkind OR
     p1.oprleft != p2.oprleft OR
     p1.oprright != p2.oprright OR
     p1.oprresult != 'bool'::regtype OR
     p2.oprresult != 'bool'::regtype OR
     p1.oid != p2.oprnegate OR
     p1.oid = p2.oid)
================================


-- Check that each operator defined in pg_operator matches its oprcode entry
-- in pg_proc.  Easiest to do this separately for each oprkind.

SELECT p1.oid, p1.oprname, p2.oid, p2.proname
FROM pg_operator AS p1, pg_proc AS p2
WHERE p1.oprcode = p2.oid AND
    p1.oprkind = 'b' AND
    (p2.pronargs != 2
     OR NOT binary_coercible(p2.prorettype, p1.oprresult)
     OR NOT binary_coercible(p1.oprleft, p2.proargtypes[0])
     OR NOT binary_coercible(p1.oprright, p2.proargtypes[1]))
================================


SELECT p1.oid, p1.oprname, p2.oid, p2.proname
FROM pg_operator AS p1, pg_proc AS p2
WHERE p1.oprcode = p2.oid AND
    p1.oprkind = 'l' AND
    (p2.pronargs != 1
     OR NOT binary_coercible(p2.prorettype, p1.oprresult)
     OR NOT binary_coercible(p1.oprright, p2.proargtypes[0])
     OR p1.oprleft != 0)
================================


-- If oprrest is set, the operator must return boolean,
-- and it must link to a proc with the right signature
-- to be a restriction selectivity estimator.
-- The proc signature we want is: float8 proc(internal, oid, internal, int4)

SELECT p1.oid, p1.oprname, p2.oid, p2.proname
FROM pg_operator AS p1, pg_proc AS p2
WHERE p1.oprrest = p2.oid AND
    (p1.oprresult != 'bool'::regtype OR
     p2.prorettype != 'float8'::regtype OR p2.proretset OR
     p2.pronargs != 4 OR
     p2.proargtypes[0] != 'internal'::regtype OR
     p2.proargtypes[1] != 'oid'::regtype OR
     p2.proargtypes[2] != 'internal'::regtype OR
     p2.proargtypes[3] != 'int4'::regtype)
================================


-- If oprjoin is set, the operator must be a binary boolean op,
-- and it must link to a proc with the right signature
-- to be a join selectivity estimator.
-- The proc signature we want is: float8 proc(internal, oid, internal, int2, internal)
-- (Note: the old signature with only 4 args is still allowed, but no core
-- estimator should be using it.)

SELECT p1.oid, p1.oprname, p2.oid, p2.proname
FROM pg_operator AS p1, pg_proc AS p2
WHERE p1.oprjoin = p2.oid AND
    (p1.oprkind != 'b' OR p1.oprresult != 'bool'::regtype OR
     p2.prorettype != 'float8'::regtype OR p2.proretset OR
     p2.pronargs != 5 OR
     p2.proargtypes[0] != 'internal'::regtype OR
     p2.proargtypes[1] != 'oid'::regtype OR
     p2.proargtypes[2] != 'internal'::regtype OR
     p2.proargtypes[3] != 'int2'::regtype OR
     p2.proargtypes[4] != 'internal'::regtype)
================================


-- Check that operators' underlying functions have suitable comments,
-- namely 'implementation of XXX operator'.  (Note: it's not necessary to
-- put such comments into pg_proc.dat
================================
 initdb will generate them as needed.)
-- In some cases involving legacy names for operators, there are multiple
-- operators referencing the same pg_proc entry, so ignore operators whose
-- comments say they are deprecated.
-- We also have a few functions that are both operator support and meant to
-- be called directly
================================
 those should have comments matching their operator.
WITH funcdescs AS (
  SELECT p.oid as p_oid, proname, o.oid as o_oid,
    pd.description as prodesc,
    'implementation of ' || oprname || ' operator' as expecteddesc,
    od.description as oprdesc
  FROM pg_proc p JOIN pg_operator o ON oprcode = p.oid
       LEFT JOIN pg_description pd ON
         (pd.objoid = p.oid and pd.classoid = p.tableoid and pd.objsubid = 0)
       LEFT JOIN pg_description od ON
         (od.objoid = o.oid and od.classoid = o.tableoid and od.objsubid = 0)
  WHERE o.oid <= 9999
)
SELECT * FROM funcdescs
  WHERE prodesc IS DISTINCT FROM expecteddesc
    AND oprdesc NOT LIKE 'deprecated%'
    AND prodesc IS DISTINCT FROM oprdesc
================================


-- Show all the operator-implementation functions that have their own
-- comments.  This should happen only in cases where the function and
-- operator syntaxes are both documented at the user level.
-- This should be a pretty short list
================================
 it's mostly legacy cases.
WITH funcdescs AS (
  SELECT p.oid as p_oid, proname, o.oid as o_oid,
    pd.description as prodesc,
    'implementation of ' || oprname || ' operator' as expecteddesc,
    od.description as oprdesc
  FROM pg_proc p JOIN pg_operator o ON oprcode = p.oid
       LEFT JOIN pg_description pd ON
         (pd.objoid = p.oid and pd.classoid = p.tableoid and pd.objsubid = 0)
       LEFT JOIN pg_description od ON
         (od.objoid = o.oid and od.classoid = o.tableoid and od.objsubid = 0)
  WHERE o.oid <= 9999
)
SELECT p_oid, proname, prodesc FROM funcdescs
  WHERE prodesc IS DISTINCT FROM expecteddesc
    AND oprdesc NOT LIKE 'deprecated%'
ORDER BY 1
================================


-- Cross-check transfn against its entry in pg_proc.
SELECT a.aggfnoid::oid, p.proname, ptr.oid, ptr.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS ptr
WHERE a.aggfnoid = p.oid AND
    a.aggtransfn = ptr.oid AND
    (ptr.proretset
     OR NOT (ptr.pronargs =
             CASE WHEN a.aggkind = 'n' THEN p.pronargs + 1
             ELSE greatest(p.pronargs - a.aggnumdirectargs, 1) + 1 END)
     OR NOT binary_coercible(ptr.prorettype, a.aggtranstype)
     OR NOT binary_coercible(a.aggtranstype, ptr.proargtypes[0])
     OR (p.pronargs > 0 AND
         NOT binary_coercible(p.proargtypes[0], ptr.proargtypes[1]))
     OR (p.pronargs > 1 AND
         NOT binary_coercible(p.proargtypes[1], ptr.proargtypes[2]))
     OR (p.pronargs > 2 AND
         NOT binary_coercible(p.proargtypes[2], ptr.proargtypes[3]))
     -- we could carry the check further, but 3 args is enough for now
     OR (p.pronargs > 3)
    )
================================


-- Cross-check finalfn (if present) against its entry in pg_proc.

SELECT a.aggfnoid::oid, p.proname, pfn.oid, pfn.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS pfn
WHERE a.aggfnoid = p.oid AND
    a.aggfinalfn = pfn.oid AND
    (pfn.proretset OR
     NOT binary_coercible(pfn.prorettype, p.prorettype) OR
     NOT binary_coercible(a.aggtranstype, pfn.proargtypes[0]) OR
     CASE WHEN a.aggfinalextra THEN pfn.pronargs != p.pronargs + 1
          ELSE pfn.pronargs != a.aggnumdirectargs + 1 END
     OR (pfn.pronargs > 1 AND
         NOT binary_coercible(p.proargtypes[0], pfn.proargtypes[1]))
     OR (pfn.pronargs > 2 AND
         NOT binary_coercible(p.proargtypes[1], pfn.proargtypes[2]))
     OR (pfn.pronargs > 3 AND
         NOT binary_coercible(p.proargtypes[2], pfn.proargtypes[3]))
     -- we could carry the check further, but 4 args is enough for now
     OR (pfn.pronargs > 4)
    )
================================


-- If transfn is strict then either initval should be non-NULL, or
-- input type should match transtype so that the first non-null input
-- can be assigned as the state value.

SELECT a.aggfnoid::oid, p.proname, ptr.oid, ptr.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS ptr
WHERE a.aggfnoid = p.oid AND
    a.aggtransfn = ptr.oid AND ptr.proisstrict AND
    a.agginitval IS NULL AND
    NOT binary_coercible(p.proargtypes[0], a.aggtranstype)
================================


-- Cross-check mtransfn (if present) against its entry in pg_proc.
SELECT a.aggfnoid::oid, p.proname, ptr.oid, ptr.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS ptr
WHERE a.aggfnoid = p.oid AND
    a.aggmtransfn = ptr.oid AND
    (ptr.proretset
     OR NOT (ptr.pronargs =
             CASE WHEN a.aggkind = 'n' THEN p.pronargs + 1
             ELSE greatest(p.pronargs - a.aggnumdirectargs, 1) + 1 END)
     OR NOT binary_coercible(ptr.prorettype, a.aggmtranstype)
     OR NOT binary_coercible(a.aggmtranstype, ptr.proargtypes[0])
     OR (p.pronargs > 0 AND
         NOT binary_coercible(p.proargtypes[0], ptr.proargtypes[1]))
     OR (p.pronargs > 1 AND
         NOT binary_coercible(p.proargtypes[1], ptr.proargtypes[2]))
     OR (p.pronargs > 2 AND
         NOT binary_coercible(p.proargtypes[2], ptr.proargtypes[3]))
     -- we could carry the check further, but 3 args is enough for now
     OR (p.pronargs > 3)
    )
================================


-- Cross-check minvtransfn (if present) against its entry in pg_proc.
SELECT a.aggfnoid::oid, p.proname, ptr.oid, ptr.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS ptr
WHERE a.aggfnoid = p.oid AND
    a.aggminvtransfn = ptr.oid AND
    (ptr.proretset
     OR NOT (ptr.pronargs =
             CASE WHEN a.aggkind = 'n' THEN p.pronargs + 1
             ELSE greatest(p.pronargs - a.aggnumdirectargs, 1) + 1 END)
     OR NOT binary_coercible(ptr.prorettype, a.aggmtranstype)
     OR NOT binary_coercible(a.aggmtranstype, ptr.proargtypes[0])
     OR (p.pronargs > 0 AND
         NOT binary_coercible(p.proargtypes[0], ptr.proargtypes[1]))
     OR (p.pronargs > 1 AND
         NOT binary_coercible(p.proargtypes[1], ptr.proargtypes[2]))
     OR (p.pronargs > 2 AND
         NOT binary_coercible(p.proargtypes[2], ptr.proargtypes[3]))
     -- we could carry the check further, but 3 args is enough for now
     OR (p.pronargs > 3)
    )
================================


-- Cross-check mfinalfn (if present) against its entry in pg_proc.

SELECT a.aggfnoid::oid, p.proname, pfn.oid, pfn.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS pfn
WHERE a.aggfnoid = p.oid AND
    a.aggmfinalfn = pfn.oid AND
    (pfn.proretset OR
     NOT binary_coercible(pfn.prorettype, p.prorettype) OR
     NOT binary_coercible(a.aggmtranstype, pfn.proargtypes[0]) OR
     CASE WHEN a.aggmfinalextra THEN pfn.pronargs != p.pronargs + 1
          ELSE pfn.pronargs != a.aggnumdirectargs + 1 END
     OR (pfn.pronargs > 1 AND
         NOT binary_coercible(p.proargtypes[0], pfn.proargtypes[1]))
     OR (pfn.pronargs > 2 AND
         NOT binary_coercible(p.proargtypes[1], pfn.proargtypes[2]))
     OR (pfn.pronargs > 3 AND
         NOT binary_coercible(p.proargtypes[2], pfn.proargtypes[3]))
     -- we could carry the check further, but 4 args is enough for now
     OR (pfn.pronargs > 4)
    )
================================


-- If mtransfn is strict then either minitval should be non-NULL, or
-- input type should match mtranstype so that the first non-null input
-- can be assigned as the state value.

SELECT a.aggfnoid::oid, p.proname, ptr.oid, ptr.proname
FROM pg_aggregate AS a, pg_proc AS p, pg_proc AS ptr
WHERE a.aggfnoid = p.oid AND
    a.aggmtransfn = ptr.oid AND ptr.proisstrict AND
    a.aggminitval IS NULL AND
    NOT binary_coercible(p.proargtypes[0], a.aggmtranstype)
================================


-- Check that all combine functions have signature
-- combine(transtype, transtype) returns transtype

SELECT a.aggfnoid, p.proname
FROM pg_aggregate as a, pg_proc as p
WHERE a.aggcombinefn = p.oid AND
    (p.pronargs != 2 OR
     p.prorettype != p.proargtypes[0] OR
     p.prorettype != p.proargtypes[1] OR
     NOT binary_coercible(a.aggtranstype, p.proargtypes[0]))
================================


-- Check that all serialization functions have signature
-- serialize(internal) returns bytea
-- Also insist that they be strict
================================
 it's wasteful to run them on NULLs.

SELECT a.aggfnoid, p.proname
FROM pg_aggregate as a, pg_proc as p
WHERE a.aggserialfn = p.oid AND
    (p.prorettype != 'bytea'::regtype OR p.pronargs != 1 OR
     p.proargtypes[0] != 'internal'::regtype OR
     NOT p.proisstrict)
================================


-- Check that all deserialization functions have signature
-- deserialize(bytea, internal) returns internal
-- Also insist that they be strict
================================
 it's wasteful to run them on NULLs.

SELECT a.aggfnoid, p.proname
FROM pg_aggregate as a, pg_proc as p
WHERE a.aggdeserialfn = p.oid AND
    (p.prorettype != 'internal'::regtype OR p.pronargs != 2 OR
     p.proargtypes[0] != 'bytea'::regtype OR
     p.proargtypes[1] != 'internal'::regtype OR
     NOT p.proisstrict)
================================


-- Check datatypes match

SELECT a.aggfnoid::oid, o.oid
FROM pg_operator AS o, pg_aggregate AS a, pg_proc AS p
WHERE a.aggfnoid = p.oid AND a.aggsortop = o.oid AND
    (oprkind != 'b' OR oprresult != 'boolean'::regtype
     OR oprleft != p.proargtypes[0] OR oprright != p.proargtypes[0])
================================


-- Check that there are not aggregates with the same name and different
-- numbers of arguments.  While not technically wrong, we have a project policy
-- to avoid this because it opens the door for confusion in connection with
-- ORDER BY: novices frequently put the ORDER BY in the wrong place.
-- See the fate of the single-argument form of string_agg() for history.
-- (Note: we don't forbid users from creating such aggregates
================================
 the policy is
-- just to think twice before creating built-in aggregates like this.)
-- The only aggregates that should show up here are count(x) and count(*).

SELECT p1.oid::regprocedure, p2.oid::regprocedure
FROM pg_proc AS p1, pg_proc AS p2
WHERE p1.oid < p2.oid AND p1.proname = p2.proname AND
    p1.prokind = 'a' AND p2.prokind = 'a' AND
    array_dims(p1.proargtypes) != array_dims(p2.proargtypes)
ORDER BY 1
================================


-- Check for index amhandler functions with the wrong signature

SELECT p1.oid, p1.amname, p2.oid, p2.proname
FROM pg_am AS p1, pg_proc AS p2
WHERE p2.oid = p1.amhandler AND p1.amtype = 'i' AND
    (p2.prorettype != 'index_am_handler'::regtype
     OR p2.proretset
     OR p2.pronargs != 1
     OR p2.proargtypes[0] != 'internal'::regtype)
================================


-- Check for table amhandler functions with the wrong signature

SELECT p1.oid, p1.amname, p2.oid, p2.proname
FROM pg_am AS p1, pg_proc AS p2
WHERE p2.oid = p1.amhandler AND p1.amtype = 's' AND
    (p2.prorettype != 'table_am_handler'::regtype
     OR p2.proretset
     OR p2.pronargs != 1
     OR p2.proargtypes[0] != 'internal'::regtype)
================================


================================


================================
--
-- Check that system tables can be reindexed.
--
-- Note that this test currently is not included in the default
-- schedules, as currently reindexing catalog tables can cause
-- deadlocks:
--
-- * The lock upgrade between the ShareLock acquired for the reindex
--   and RowExclusiveLock needed for pg_class/pg_index locks can
--   trigger deadlocks.
--
-- * The uniqueness checks performed when reindexing a unique/primary
--   key index possibly need to wait for the transaction of a
--   about-to-deleted row in pg_class to commit. That can cause
--   deadlocks because, in contrast to user tables, locks on catalog
--   tables are routinely released before commit - therefore the lock
--   held for reindexing doesn't guarantee that no running transaction
--   performed modifications in the table underlying the index.
--
--   This is particularly problematic as such conflicts can be
--   triggered even when run in isolation, as a previous session's
--   temporary table cleanup might still be running (even when the
--   session ended from a client perspective).


-- Check reindexing of whole tables
REINDEX TABLE pg_class
================================
 -- mapped, non-shared, critical
REINDEX TABLE pg_index
================================
 -- non-mapped, non-shared, critical
REINDEX TABLE pg_operator
================================
 -- non-mapped, non-shared, critical
REINDEX TABLE pg_database
================================
 -- mapped, shared, critical
REINDEX TABLE pg_shdescription
================================
 -- mapped, shared non-critical

-- Check that individual system indexes can be reindexed. That's a bit
-- different from the entire-table case because reindex_relation
-- treats e.g. pg_class special.
REINDEX INDEX pg_class_oid_index
================================
 -- mapped, non-shared, critical
REINDEX INDEX pg_class_relname_nsp_index
================================
 -- mapped, non-shared, non-critical
REINDEX INDEX pg_index_indexrelid_index
================================
 -- non-mapped, non-shared, critical
REINDEX INDEX pg_index_indrelid_index
================================
 -- non-mapped, non-shared, non-critical
REINDEX INDEX pg_database_oid_index
================================
 -- mapped, shared, critical
REINDEX INDEX pg_shdescription_o_c_index
================================

REINDEX INDEX pg_class_oid_index
================================
 -- mapped, non-shared, critical
REINDEX INDEX pg_class_relname_nsp_index
================================
 -- mapped, non-shared, non-critical
REINDEX INDEX pg_index_indexrelid_index
================================
 -- non-mapped, non-shared, critical
REINDEX INDEX pg_index_indrelid_index
================================
 -- non-mapped, non-shared, non-critical
REINDEX INDEX pg_database_oid_index
================================
 -- mapped, shared, critical
REINDEX INDEX pg_shdescription_o_c_index
================================


================================

EXECUTE q1
================================

EXECUTE q1
================================

CREATE TEMPORARY TABLE q5_prep_results AS EXECUTE q5(200, 'DTAAAA')
================================

CREATE TEMPORARY TABLE q5_prep_nodata AS EXECUTE q5(200, 'DTAAAA')
    WITH NO DATA
================================


-- test DEALLOCATE ALL
================================


================================

CREATE SCHEMA fast_default
================================

$$ LANGUAGE 'plpgsql'
================================

$$ LANGUAGE 'plpgsql'
================================

    if this_schema = 'fast_default'
    then
        RAISE NOTICE 'rewriting table % for reason %',
          pg_event_trigger_table_rewrite_oid()::regclass,
          pg_event_trigger_table_rewrite_reason()
================================

$func$
================================


CREATE TABLE has_volatile AS
SELECT * FROM generate_series(1,10) id
================================



CREATE EVENT TRIGGER has_volatile_rewrite
                  ON table_rewrite
   EXECUTE PROCEDURE log_rewrite()
================================

        i INT
================================

  WHILE (i < a) LOOP
    res := res || chr(ascii('a') + i)
================================

    i := i + 1
================================

  RETURN res
================================
 $$ LANGUAGE PLPGSQL STABLE
================================


INSERT INTO T SELECT * FROM generate_series(1, 10) a
================================


INSERT INTO T SELECT b, b - 10 FROM generate_series(11, 20) a(b)
================================


INSERT INTO T SELECT b, b - 10, (b + 10)::text FROM generate_series(21, 30) a(b)
================================


-- query to exercise expand_tuple function
CREATE TABLE t1 AS
SELECT 1::int AS a , 2::int AS b
FROM generate_series(1,20) q
================================

    if TG_OP = 'DELETE'
    then
       return OLD
================================

    else
       return NEW
================================


$$
================================

INSERT INTO t DEFAULT VALUES
================================


-- verify that a default set on a non-plain table doesn't set a missing
-- value on the attribute
CREATE FOREIGN DATA WRAPPER dummy
================================

CREATE SERVER s0 FOREIGN DATA WRAPPER dummy
================================

CREATE FOREIGN TABLE ft1 (c1 integer NOT NULL) SERVER s0
================================

ALTER FOREIGN TABLE ft1 ADD COLUMN c8 integer DEFAULT 0
================================

ALTER FOREIGN TABLE ft1 ALTER COLUMN c8 TYPE char(10)
================================


-- cleanup
DROP FOREIGN TABLE ft1
================================

DROP SERVER s0
================================

DROP FOREIGN DATA WRAPPER dummy
================================

DROP EVENT TRIGGER has_volatile_rewrite
================================

DROP SCHEMA fast_default
================================


================================


================================


-- left of
SELECT p.* FROM POINT_TBL p WHERE p.f1 << '(0.0, 0.0)'
================================


-- right of
SELECT p.* FROM POINT_TBL p WHERE '(0.0,0.0)' >> p.f1
================================


-- above
SELECT p.* FROM POINT_TBL p WHERE '(0.0,0.0)' |>> p.f1
================================


-- below
SELECT p.* FROM POINT_TBL p WHERE p.f1 <<| '(0.0, 0.0)'
================================


-- equal
SELECT p.* FROM POINT_TBL p WHERE p.f1 ~= '(5.1, 34.5)'
================================


SELECT p.* FROM POINT_TBL p
   WHERE box '(0,0,100,100)' @> p.f1
================================


SELECT p.* FROM POINT_TBL p
   WHERE not box '(0,0,100,100)' @> p.f1
================================


SELECT p.f1, p.f1 <-> point '(0,0)' AS dist
   FROM POINT_TBL p
   ORDER BY dist
================================


SELECT p1.f1 AS point1, p2.f1 AS point2, p1.f1 <-> p2.f1 AS dist
   FROM POINT_TBL p1, POINT_TBL p2
   ORDER BY dist, p1.f1[0], p2.f1[0]
================================


-- put distance result into output to allow sorting with GEQ optimizer - tgl 97/05/10
SELECT p1.f1 AS point1, p2.f1 AS point2, (p1.f1 <-> p2.f1) AS distance
   FROM POINT_TBL p1, POINT_TBL p2
   WHERE (p1.f1 <-> p2.f1) > 3 and p1.f1 << p2.f1
   ORDER BY distance, p1.f1[0], p2.f1[0]
================================


-- put distance result into output to allow sorting with GEQ optimizer - tgl 97/05/10
SELECT p1.f1 AS point1, p2.f1 AS point2, (p1.f1 <-> p2.f1) AS distance
   FROM POINT_TBL p1, POINT_TBL p2
   WHERE (p1.f1 <-> p2.f1) > 3 and p1.f1 << p2.f1 and p1.f1 |>> p2.f1
   ORDER BY distance
================================

INSERT INTO point_gist_tbl SELECT '(0,0)' FROM generate_series(0,1000)
================================

SELECT COUNT(*) FROM point_gist_tbl WHERE f1 ~= '(0.0000009,0.0000009)'::point
================================

SELECT COUNT(*) FROM point_gist_tbl WHERE f1 <@ '(0.0000009,0.0000009),(0.0000009,0.0000009)'::box
================================

SELECT COUNT(*) FROM point_gist_tbl WHERE f1 ~= '(0.0000018,0.0000018)'::point
================================

SELECT COUNT(*) FROM point_gist_tbl WHERE f1 ~= '(0.0000009,0.0000009)'::point
================================

SELECT COUNT(*) FROM point_gist_tbl WHERE f1 <@ '(0.0000009,0.0000009),(0.0000009,0.0000009)'::box
================================

SELECT COUNT(*) FROM point_gist_tbl WHERE f1 ~= '(0.0000018,0.0000018)'::point
================================


================================